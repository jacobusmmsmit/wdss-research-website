<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Money for Nothing: An Arbitrage Paradox</title>
      <link href="/money-for-nothing/"/>
      <url>/money-for-nothing/</url>
      
        <content type="html"><![CDATA[<p>Utilising the concept of arbitrage can be a lucrative opportunity when done correctly. Yet somehow, when done incorrectly, the results can be even more propitious (at least for one of the parties involved). In this post, we will walk through a paradox related to arbitrage in which the seeming obvious fair price for an asset results in a strategy for obtaining infinite wealth.</p><h2 id="the-financial-jargon"><a class="markdownIt-Anchor" href="#the-financial-jargon"></a> The Financial Jargon</h2><div class="note warning"><p>If you are already familiar with the notion of options and arbitrage, you may wish to skip to the <a href="#the-paradox">next section</a>.</p></div><p>To make this post inclusive to all, we will begin by offering a brief, requisite understanding of the financial vocabulary used in this post, before attempting to describe the paradox itself.</p><p>In the financial world, it is hard to avoid the words <em>stocks</em> and <em>shares</em>, so we will begin by defining these. A stock is a type of security that represents the ownership of a fraction of a company. This entitles the owner of the stock to a proportion of the corporation’s assets and profits based on how much stock they own. A share is simply the smallest denomination of a company’s stock and acts as a unit for measuring one’s stake in a company.</p><p>Another key financial concept is that of the <em>option</em>. These are financial instruments based on the value underlying securities (such as stocks). Since the value of an option is dependent on the value of another financial asset (or assets), it is known as a derivative. An options contract offers the buyer the opportunity to buy or sell the underlying asset at within some fixed timeframe for a predetermined price. The two most common types of options are calls and puts; a call being that the holder can buy the asset and a put being that the holder can sell the asset. It is important to note, that an options contract puts the option owner under no obligation to buy/sell the asset within the agreed upon timeframe (unlike a futures contract). Instead, they are free to exercise only when it benefits them.</p><p>There are a several components of an options contract, such as the premium, strike price, expiration date, and contract size, but we will focus only on the premium and strike price in this post. The premium is the down payment that the holder of the contract pays for having this right to exercise an options trade. This is paid regardless of whether the holder exercises the option. The strike price is the price at which the holder of the option can buy/sell the underlying security if they decide to exercise it. Note, that this is a fixed price and so does not change, even though the value of the underlying security might.</p><p>This brings us to <em>arbitrage</em>. Arbitrage is the act of simultaneously buying and selling assets or commodities in different markets or in derivative forms, to take advantage of different prices of the same asset. It exploits the price difference that arises as a result of market inefficiencies and is usually considered useful to markets, as it helps promote market efficiency. However, this opportunity is usually captured by automated trading software, making it nearly impossible for anyone to take advantage.</p><h2 id="the-paradox"><a class="markdownIt-Anchor" href="#the-paradox"></a> The Paradox</h2><h3 id="setup"><a class="markdownIt-Anchor" href="#setup"></a> Setup</h3><p>Say one share of stock is currently worth <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>£</mi><mn>100</mn></mrow><annotation encoding="application/x-tex">£100</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">1</span><span class="mord">0</span><span class="mord">0</span></span></span></span>. On top of this, by some sort of divine wisdom, we know that the share price has a <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mn>2</mn><mn>3</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{2}{3}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-.345em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.845108em"><span style="top:-2.6550000000000002em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> chance of rising to <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>£</mi><mn>120</mn></mrow><annotation encoding="application/x-tex">£120</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">1</span><span class="mord">2</span><span class="mord">0</span></span></span></span> by the next day and a <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mn>1</mn><mn>3</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{3}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-.345em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.845108em"><span style="top:-2.6550000000000002em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> chance of dropping to <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>£</mi><mn>90</mn></mrow><annotation encoding="application/x-tex">£90</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">9</span><span class="mord">0</span></span></span></span>. In reality, the behaviour of the share price would be governed by a statistical distribution. In this case, this post would still be valid, but the numbers wouldn’t be quite as simple to handle.<br><img src="/" class="lazyload" data-src="/images/money-for-nothing/share_price.png"  alt="Illustration of the possible share price changes (Credit: Freepik.com)"><br>A options contract for the same asset has a strike price of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>£</mi><mn>105</mn></mrow><annotation encoding="application/x-tex">£105</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">1</span><span class="mord">0</span><span class="mord">5</span></span></span></span>. This implies that there is a <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mn>2</mn><mn>3</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{2}{3}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-.345em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.845108em"><span style="top:-2.6550000000000002em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> chance of making a profit of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>£</mi><mn>15</mn></mrow><annotation encoding="application/x-tex">£15</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">1</span><span class="mord">5</span></span></span></span> (the difference between the asset selling price and the strike price) and a <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mn>1</mn><mn>3</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{3}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-.345em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.845108em"><span style="top:-2.6550000000000002em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> of neither losing nor gaining money (since the holder wouldn’t exercise the option in this case, as this would lead to losing money).<br><img src="/" class="lazyload" data-src="/images/money-for-nothing/option_value.png"  alt="Illustration of the options contract value (Credit: Freepik.com)"><br>This leaves us to ask what the fair price for such an option would be. The answer to this seems intuitive; if we have a <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mn>2</mn><mn>3</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{2}{3}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-.345em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.845108em"><span style="top:-2.6550000000000002em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> chance of making <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>£</mi><mn>15</mn></mrow><annotation encoding="application/x-tex">£15</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">1</span><span class="mord">5</span></span></span></span>, and a <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mn>1</mn><mn>3</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{3}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-.345em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.845108em"><span style="top:-2.6550000000000002em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> chance gaining nothing, then on average, we would expect to gain <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mn>2</mn><mn>3</mn></mfrac><mo>×</mo><mi>£</mi><mn>15</mn><mo>+</mo><mfrac><mn>1</mn><mn>3</mn></mfrac><mo>×</mo><mi>£</mi><mn>0</mn><mo>=</mo><mi>£</mi><mn>10</mn></mrow><annotation encoding="application/x-tex">\frac{2}{3} \times £15 + \frac{1}{3} \times £0 = £10</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-.345em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.845108em"><span style="top:-2.6550000000000002em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.77777em;vertical-align:-.08333em"></span><span class="mord mathit">£</span><span class="mord">1</span><span class="mord">5</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-.345em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.845108em"><span style="top:-2.6550000000000002em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">0</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">1</span><span class="mord">0</span></span></span></span>. Therefore, pricing the option at <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>£</mi><mn>10</mn></mrow><annotation encoding="application/x-tex">£10</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">1</span><span class="mord">0</span></span></span></span> seems to be the only logical option.</p><p>This is seemed like the obvious answer when I first say this problem, yet we will see that leads to a paradox. Furthermore, this isn’t some sort of trivial, technical paradox, but rather a way for a bank to exploit us to generate an endless stream of cash!</p><h3 id="breaking-down-the-balance-sheet"><a class="markdownIt-Anchor" href="#breaking-down-the-balance-sheet"></a> Breaking Down the Balance Sheet</h3><p>The process for such exploitation is devilishly simple. The bank starts by selling an option for the price of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>£</mi><mn>10</mn></mrow><annotation encoding="application/x-tex">£10</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">1</span><span class="mord">0</span></span></span></span> that we agreed upon above. They follow this by buying a half stock for <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>£</mi><mn>50</mn></mrow><annotation encoding="application/x-tex">£50</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">5</span><span class="mord">0</span></span></span></span>. We will insist that, by the end of the day, the bank must have a net zero balance sheet, so they also take out a loan. For simplicity, we will assume that the loan is interest-free (e.g. they supplied the loan to themselves through savings) but the paradox would hold regardless.<br><img src="/" class="lazyload" data-src="/images/money-for-nothing/day_zero.png"  alt="The bank's day zero strategy"><br>By the following day, the share price will have either increased to <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>£</mi><mn>120</mn></mrow><annotation encoding="application/x-tex">£120</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">1</span><span class="mord">2</span><span class="mord">0</span></span></span></span> or decreased to <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>£</mi><mn>90</mn></mrow><annotation encoding="application/x-tex">£90</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">9</span><span class="mord">0</span></span></span></span>. Let’s first consider the former possibility.</p><p>In this case, the buyer will exercise the option, so the bank will need to pay out <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>£</mi><mn>15</mn></mrow><annotation encoding="application/x-tex">£15</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">1</span><span class="mord">5</span></span></span></span>. On top of this, the bank has to pay back the loan of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>£</mi><mn>40</mn></mrow><annotation encoding="application/x-tex">£40</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">4</span><span class="mord">0</span></span></span></span>. It’s not all gloom however, as the stock price has now increased, so their half-share is now worth <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>£</mi><mn>60</mn></mrow><annotation encoding="application/x-tex">£60</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">6</span><span class="mord">0</span></span></span></span>. When we tally up the wins and losses, we see that the bank to have a tidy profit of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>£</mi><mn>5</mn></mrow><annotation encoding="application/x-tex">£5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">5</span></span></span></span>.<br><img src="/" class="lazyload" data-src="/images/money-for-nothing/day_one_up.png"  alt="The bank's day one strategy for share price increase"><br>In the case where the price drops to <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>£</mi><mn>90</mn></mrow><annotation encoding="application/x-tex">£90</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">9</span><span class="mord">0</span></span></span></span>, the result is equally auspicious. The buyer won’t exercise the option, so no money is lost their, though the bank still has to pay back the <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>£</mi><mn>40</mn></mrow><annotation encoding="application/x-tex">£40</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">4</span><span class="mord">0</span></span></span></span> loan. The stock price may have dropped, but this still leaves the bank with a half-share worth <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>£</mi><mn>45</mn></mrow><annotation encoding="application/x-tex">£45</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">4</span><span class="mord">5</span></span></span></span>. After crunching the numbers, we should be amazed to see that the bank has again made a profit of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>£</mi><mn>5</mn></mrow><annotation encoding="application/x-tex">£5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">5</span></span></span></span>.<br><img src="/" class="lazyload" data-src="/images/money-for-nothing/day_one_down.png"  alt="The bank's day one strategy for share price increase"><br>There’s no funny-business or mathematical slight of hand here; by using this strategy, the bank is <strong>guaranteed</strong> to make a profit (at our expense, no less). Additionally, by repeating this process or scaling up the numbers involved, the bank is left with what is essentially a monetary printing press.</p><p>Thankfully (or sadly, if banking is your thing), this is not how this problem goes in the real world. The chink is this paradox’s armour was our initial assumption of what a fair price for the option would be. Now that we’ve seen the result of such a decision, it is clear that what may have seemed obviously true, was actually incorrect.</p><p>What we failed to account for was the risk associated with the option. Rather than using the raw expected value of the option as our baseline, we should instead use a risk-adjusted version. When we follow this procedure, we see that the correct premium should have been <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>£</mi><mn>5</mn></mrow><annotation encoding="application/x-tex">£5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">5</span></span></span></span>, not <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>£</mi><mn>10</mn></mrow><annotation encoding="application/x-tex">£10</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">1</span><span class="mord">0</span></span></span></span>. We can confirm this by modifying the balance sheets to use this figure, at which point, the bank’s easy profit evaporates.<br><img src="/" class="lazyload" data-src="/images/money-for-nothing/day_zero_adj.png"  alt="Day zero after risk adjustment"><br>We see in this case, that the reduced option price results in the bank requiring a slightly larger loan to balance the sheet.<br><img src="/" class="lazyload" data-src="/images/money-for-nothing/day_one_adj.png"  alt="Day one after risk adjustment"><br>This additional loan repayment balances the sheet on the next day so that no profit is made in either case.</p><h2 id="closing-remarks"><a class="markdownIt-Anchor" href="#closing-remarks"></a> Closing Remarks</h2><p>This paradox has been overly simplified, and in reality, the forces of supply and demand cut off most opportunity for arbitrage of this form. Nevertheless, this idea of arbitrage is used to price options. In 1973, Fisher Black and Myron Scholes used the method of repeated portfolio replication as well as an arbitrage argument to determine the price of an option. They were awarded for their work with a Nobel Prize, but unfortunately Fischer Black passed away before the award was given.</p><p>I hope that this post has brought to light an interesting application of arbritage and inspired the reader to look further into the topic, especially with regrad to its practical applications.</p>]]></content>
      
      
      <categories>
          
          <category> Economics </category>
          
          <category> Finance </category>
          
      </categories>
      
      
        <tags>
            
            <tag> lesson </tag>
            
            <tag> puzzle </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Word2Vec: Arithmetic with Words</title>
      <link href="/word2vec/"/>
      <url>/word2vec/</url>
      
        <content type="html"><![CDATA[<div class="note info"><p><strong>Accessing Post Source</strong><br>We are still working on getting this site set up, so source code for this post is not yet available. Check back soon and you’ll be able to find it linked here.</p></div><p>Unsurprisingly, human language seem intuitive to humans, but that is not the case for a computer. On the other hand, though we can easily determine whether two words share a similar meaning, if asked to give a quantitative reason as to why that is the case, we will may struggle to reach a satisfying answer. Thankfully, our shortcoming is where computers thrive, as they are remarkably good with numbers and tabular data. All this leaves us to do is to translate our problem into such a numeric system. Word embedding is a technique to take language and transform it into this computer-friendly format so we can take advantage of computing power for solving natural language processing tasks.</p><h2 id="background"><a class="markdownIt-Anchor" href="#background"></a> Background</h2><h3 id="the-need-for-word2vec"><a class="markdownIt-Anchor" href="#the-need-for-word2vec"></a> The Need for Word2vec</h3><p>Initial attempts at solving this problem before word embeddings included one-hot encoding. The essence of this process is to represent our text as a sequence of vectors in a space with number of dimensions equal to the number of words in our corpus. We assign each word its own dimension so that each word vector is orthogonal to the others. Technical details aside, this isn’t practical to model anything on the scale of human languages, as the vector dimensions would be the size of the language (and as a results, storage and computational requirements would be astronomical). More importantly, however, this doesn’t capture any sort of similarity between two words (since ever vector is mutually orthogonal to the others). As far as this method is concerned, the words ‘dolphin’ and ‘neoliberal’ are equally similar to ‘shark’. Word2vec aims to solve this problem by providing word embedding which take into account relations between words. In essence, word2vec provides a canvas (albeit a strange multi-dimensional one) where any possible word in the language could lie, and plots points on this canvas for each word in our corpus. How close any two points on this canvas lie (captured mathematically by the cosine distance) should therefore correspond to how likely humans are to describe the respresented words as “similar”.</p><p><img src="/" class="lazyload" data-src="/images/word2vec/word2vec_6_0.png"  alt=""></p><h3 id="deriving-the-word-embeddings"><a class="markdownIt-Anchor" href="#deriving-the-word-embeddings"></a> Deriving the Word Embeddings</h3><p>To derive each word embedding, the word2vec model is usually trained using a method called Skipgram with Negative Sampling (SGNS). Essentially, a large corpus (typically billions of words) is fed to the model, and an <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathdefault">n</span></span></span></span>-sized sliding window is used to capture the words that lie either side of each word in the corpus to determine each word’s context. The context for each word is then used to determine the word’s embedding vector, with the negative sampling process controlling the rate at which these weights are updated to reduce computation time and produce a more robust result. Because words with a similar context usually have closely-linked meanings, such words will end up having similar embedding vectors too.</p><p><img src="/" class="lazyload" data-src="/images/word2vec/sliding_window.png"  alt="The sliding window used in word2vec training"><br>Take the above diagram as an example. On iteration <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.85396em;vertical-align:-.19444em"></span><span class="mord mathdefault" style="margin-right:.05724em">j</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathdefault" style="margin-right:.03148em">k</span></span></span></span>, ‘fox’ and ‘bear’ have similar contexts, so will end up with relatively close embedding vectors. After many adjustments each time they are found in the corpus, their vectors will provide an increasingly accurate represention of the “fox” and “bear” relation—types of animals.</p><h2 id="application"><a class="markdownIt-Anchor" href="#application"></a> Application</h2><div class="note info"><p>The following examples are derived from a word2vec model trained on the <a href="https://code.google.com/archive/p/word2vec/" target="_blank" rel="noopener">Google News dataset</a>, featuring over 100 billion words taken from various news articles. The trained model is stored an object called <code>model</code> which we can query for results.</p></div><p>Once we have trained a word embedding using word2vec, we can apply it in many different ways to extract the relationships between words in the corpus.</p><p>For example, we can find the similarity between words based on their cosine distance in the vector space.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.similarity(<span class="string">'queen'</span>, <span class="string">'throne'</span>)</span><br></pre></td></tr></table></figure><pre><code>0.45448625</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.similarity(<span class="string">'queen'</span>, <span class="string">'forklift'</span>)</span><br></pre></td></tr></table></figure><pre><code>-0.030027825</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.similarity(<span class="string">'Queen'</span>, <span class="string">'Bowie'</span>)</span><br></pre></td></tr></table></figure><pre><code>0.20833209</code></pre><div class="note warning"><p>Since we are measuring similarity using the cosine distance, values will range from -1 to 1. Words with a similarity near 1 are likely to be extremely similar, words with a similarity of 0 have little in common, and words with similarity near -1 <em>should</em> be opposites (though we’ll later see that this doesn’t always work)</p></div><p>As expected, the word ‘forklift’ is relatively distinct from ‘queen’, especially when compared to ‘throne’. What’s fascinating, however, is that multiple facets of the word ‘queen’ are captured; we see that ‘Bowie’ is also relatively close to ‘Queen’ due to the word’s relation to the iconic rock band.</p><p>Naturally, with vectors come mathematical operations, and the real power of word2vec starts to emerge. Vector differences are the crux behind <em>analogies</em>, a concept best explained through examples…</p><h3 id="analogies"><a class="markdownIt-Anchor" href="#analogies"></a> Analogies</h3><h4 id="starting-with-a-classic"><a class="markdownIt-Anchor" href="#starting-with-a-classic"></a> Starting With a Classic</h4><p>The most infamous example of the use of word2vec is answering the question, “Man is to woman, as king is to…what?”. As we can see, word2vec takes this puzzle in it’s stride.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># king + (woman - man) = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'king'</span>, <span class="string">'woman'</span>], negative=[<span class="string">'man'</span>], topn=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>queen (0.712)</code></pre><p>This example is rather intuitive; the female version of the male title ‘king’ is ‘queen’ and so this is the natural choice to complete the analogy. To get word2vec to return this result, we have to phrase the question in the language of arthimetic; that is, <code>king + (woman - man)</code>. In other words, we are taking the word ‘king’ and asking what the corresponding word would be if added the difference between ‘woman’ and ‘man’. This may seem unituitive—why can’t we just add ‘woman’ to ‘king’? The reason for this is that the word ‘king’ already has ‘man’ as a component of its vector representation. Therefore, if we simply added ‘woman’ without first subtracting ‘man’ we end up with components of both ‘woman’ and ‘man’ which confuses the model, leaving us with nonsensical results.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Invalid approach: king + woman = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'king'</span>, <span class="string">'woman'</span>], topn=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>man (0.663)</code></pre><h3 id="plurals"><a class="markdownIt-Anchor" href="#plurals"></a> Plurals</h3><p>We can use this system of analogy solving for finding the singular and plural forms of words. With a rather mundane example such as <code>gloves + (bike - bikes)</code>, it’s not unsurprising the model returns ‘glove’; it could simply be obtained from deciphering that the pattern is removing the trailing ‘s’—hardly groundbreaking. But when talking about irregular plurals, the required task to output the derived word shifts from spotting a simple pattern to seemingly needing a human-like understanding of the structure and complexities of the English language. Never-the-less, word2vec is up for the challenge.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># foot + (cacti - cactus) = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'foot'</span>, <span class="string">'cacti'</span>], negative=[<span class="string">'cactus'</span>], topn=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>feet (0.568)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># child + (sheep - sheep) = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'child'</span>, <span class="string">'sheep'</span>], negative=[<span class="string">'sheep'</span>], topn=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>children (0.726)</code></pre><p>Here, ‘sheep’ is both the singular and the plural, meaning the result of the word arithmetic is still ‘child’. But since ‘child’ is such a similar word to ‘children’, word2vec still manages to come out with the correct answer.</p><h3 id="geographical-analogies"><a class="markdownIt-Anchor" href="#geographical-analogies"></a> Geographical analogies</h3><p>We can use analogies to find cities.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Portugal + (Moscow - Russia) = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'Portugal'</span>, <span class="string">'Moscow'</span>], negative=[<span class="string">'Russia'</span>], topn=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>Lisbon (0.655)</code></pre><p>Or we can flip things around to find what country a city resides in.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Delhi + (Spain - Barcelona) = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'Delhi'</span>, <span class="string">'Spain'</span>], negative=[<span class="string">'Barcelona'</span>], topn=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>India (0.626)</code></pre><p>Finally, we can go one step up and find the geographic regions of countries.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Cambodia + (Africa - Egypt) = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'Cambodia'</span>, <span class="string">'Africa'</span>], negative=[<span class="string">'Egypt'</span>], topn=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>Southeast_Asia (0.566)</code></pre><p>The geographic intelligence of word2vec isn’t limited to the form of analogy. Here we see an example in which we perform straight addition.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Iran + war = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'Iran'</span>, <span class="string">'war'</span>],topn=<span class="number">3</span>))</span><br></pre></td></tr></table></figure><pre><code>Iraq (0.683)Islamic_republic (0.671)Syria (0.653)</code></pre><p>This example shows how much geographic and political complexity is captured in the model. ‘Iraq’ and ‘Islamic_republic’ are most likely referencing the Iran-Iraq war. On the other hand Iraq and Syria, are both war-stricken countries near Iran, which could easily explain this relation.</p><h3 id="opposites"><a class="markdownIt-Anchor" href="#opposites"></a> Opposites</h3><p>When a word has a clear opposite, we can use analogy to find it.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># high + (big - small)</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'high'</span>, <span class="string">'big'</span>], negative=[<span class="string">'small'</span>], topn=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>low (0.448)</code></pre><p>Note however that we can’t just negate a word to find its opposite or we obtain gibberish in return.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -high</span></span><br><span class="line">print_similar(model.most_similar(negative=[<span class="string">'high'</span>], topn=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>----------_-----------------------------------------------_GS## (0.321)</code></pre><p>The reason for this is that an opposite word in a vector space has to be opposite in every way. Even though we would say that ‘high’ and ‘low’ are opposites, they do in fact have components in common, such as how they both represent heights. For that reason the model stubbornly ignores words that are opposites in the way we intend, and instead, tries to find the word that is most dissimilar to ‘high’, resulting in some strange garble of characters.</p><h3 id="vector-sums-and-differences"><a class="markdownIt-Anchor" href="#vector-sums-and-differences"></a> Vector Sums and Differences</h3><p>As hinted at before, word2vec can solve problems far more general than analogies. Here we look at some examples of generic vector sums and differences.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># death + water = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'death'</span>, <span class="string">'water'</span>], topn=<span class="number">3</span>))</span><br></pre></td></tr></table></figure><pre><code>drowning (0.556)drowing (0.545)scalding_bath (0.526)</code></pre><div class="note info"><p>It appears that the second most similar term has picked up on a common typo of ‘drowning’. The joys of real world data…</p></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># death + knife = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'death'</span>, <span class="string">'knife'</span>], topn=<span class="number">3</span>))</span><br></pre></td></tr></table></figure><pre><code>kitchen_knife (0.644)stabbing (0.637)murder (0.634)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># girlfriend - love = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'girlfriend'</span>], negative=[<span class="string">'love'</span>], topn=<span class="number">3</span>))</span><br></pre></td></tr></table></figure><pre><code>ex_girlfriend (0.517)fiancee (0.479)estranged_wife (0.476)</code></pre><p>The second result is rather strange. If you have a theory of where this relation might have come from, make sure to comment below.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># colleague + love = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'colleage'</span>, <span class="string">'love'</span>], topn=<span class="number">3</span>))</span><br></pre></td></tr></table></figure><pre><code>loved (0.580)friend (0.551)pal (0.542)</code></pre><p>With the last example we can see a shortcoming of the word2vec model. It appears that ‘love’ has a much stronger vector representation than ‘collegue’; that is, the term captures more complexity, which makes sense. For this reason, the term ‘love’ can overpower the sum so that a word similar to ‘love’, ‘loved’ can be returned as highly similar even though it doesn’t relate much to the word ‘collegue’. Despite this, the other two preditions are strong.</p><h3 id="miscellaneous"><a class="markdownIt-Anchor" href="#miscellaneous"></a> Miscellaneous</h3><p>To wrap up our examples, we will look at some miscellaneous analogies involving people and places.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Obama + (Russia - USA) = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'Obama'</span>, <span class="string">'Russia'</span>], negative=[<span class="string">'USA'</span>], topn=<span class="number">3</span>))</span><br></pre></td></tr></table></figure><pre><code>Medvedev (0.674)Putin (0.647)Kremlin (0.617)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UK + (Hitler - Germany) = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'UK'</span>, <span class="string">'Hitler'</span>], negative=[<span class="string">'Germany'</span>], topn=<span class="number">3</span>))</span><br></pre></td></tr></table></figure><pre><code>Tony_Blair (0.522)Oliver_Cromwell (0.509)Maggie_Thatcher (0.506)</code></pre><p>Make of the above what you will…</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Apple + (Gates - Microsoft) = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'Apple'</span>, <span class="string">'Gates'</span>], negative=[<span class="string">'Microsoft'</span>], topn=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>Steve_Jobs (0.523)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Victoria Beckham + (Barack Obama - Michelle) = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'Victoria_Beckham'</span>, <span class="string">'Barack_Obama'</span>], negative=[<span class="string">'Michelle'</span>], topn=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>David_Beckham (0.528)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Manchester + (Anfield - Liverpool) = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'Manchester'</span>, <span class="string">'Anfield'</span>], negative=[<span class="string">'Liverpool'</span>], topn=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>Old_Trafford (0.765)</code></pre><h2 id="word2vec-in-the-wild"><a class="markdownIt-Anchor" href="#word2vec-in-the-wild"></a> Word2Vec in the Wild</h2><p>Above, we have seen some fairly isolated applications of the word2vec model, but that is not to say that there are not wider reaching use cases. For example, word2vec is often a key step in the production of sentiment analysis models (See: <a href="https://youtu.be/l40-JFn6F9M?t=1845" target="_blank" rel="noopener">a WDSS virtual talk on the use of sentiment analysis for predicting presidential approval</a>), recommender systems, and chat bots. Aside from these ecommerce-centric examples, word2vec has also flurished in scientific applications such as BioNLP, which have utilised word embeddings for advancements in knowledge.</p><p>Hopefully, through these examples, the potential power of Word2Vec has been made clear. Thank you reading.</p>]]></content>
      
      
      <categories>
          
          <category> Computer Science </category>
          
          <category> Natural Language Processing </category>
          
      </categories>
      
      
        <tags>
            
            <tag> lesson </tag>
            
            <tag> word-embedding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Higher or Lower: Reinventing a Classic Card Game</title>
      <link href="/higher-or-lower/"/>
      <url>/higher-or-lower/</url>
      
        <content type="html"><![CDATA[<div class="note info"><p>This post is the corresponding write-up for a WDSS project in which a small team of society members collaborated to produce a web-toy that plays a game of Higher or Lower using the Twitter follower counts of celebrities. You can play this game at <a href="https://shiny.warwickdatascience.com/higher-or-lower/" target="_blank" rel="noopener">this link</a>.</p></div><h2 id="motivation"><a class="markdownIt-Anchor" href="#motivation"></a> Motivation</h2><p>Sometimes, simplicity is beautiful. Higher or Lower is a game embodying this philosophy. Played solo with a standard deck of cards, play consists of revealing these one at a time after first guessing whether the next card will have a higher or lower value. In recent years, this game has been re-envisioned as a <a href="http://www.higherlowergame.com/" target="_blank" rel="noopener">popular web toy</a>, in which card values are replaced by the number of global monthly Google searches for various topics. Not wishing to limit ourselves to search results, we decided to implement our own online version of the classic game based on the follower counts of Twitter celebrities. The final app can be found at the link above, and we will spend the rest of this post looking into the techniques behind our approach as well as reviewing the lessons this project can teach us about collaborative data science at WDSS.</p><p>For our purposes, this project is an ideal medium to practise web scraping and creating sharable products for others to enjoy. Web scraping is a way of extracting data from websites, leveraging automation to gather information efficiently and without unnecessary repetition. In all, three members of WDSS worked on this project, combining their specific skills to develop the final product. <a href="https://www.linkedin.com/in/tim-hargreaves/" target="_blank" rel="noopener">Tim Hargreaves</a>, focused on the backbone of the app, <a href="https://www.linkedin.com/in/mhbardsley/" target="_blank" rel="noopener">Matthew Bardsley</a>, the visuals of the game, and I (<a href="https://www.linkedin.com/in/parthdevalia/" target="_blank" rel="noopener">Parth Devalia</a>) have responsibility for the communication of results.</p><h2 id="implemenation"><a class="markdownIt-Anchor" href="#implemenation"></a> Implemenation</h2><div class="note info"><p>The source code for the web app and scraping scripts have been open-sourced in <a href="https://github.com/warwickdatascience/higher-or-lower" target="_blank" rel="noopener">this repository</a>.</p></div><p>With 330 million monthly users, Twitter has become an indispensable medium for instant news and opinions from politicians, brands, and of course, celebrities. Manually defining celebrityhood and iterating through matching accounts would be a difficult task, so we decided to look at what existing resources we could take advantage of. We eventually settled on a website called <a href="http://profilerehab.com/twitter-help/celebrity_twitter_list" target="_blank" rel="noopener">ProfileRehab</a>. On this site, links to celebrities’ Twitter accounts are sorted into categories. By scraping this information we were able to collect Twitter profile URLs for around five hundred celebrities, matched to their names. We then interfaced with the Twitter API to read their respective follower counts and download their profile pictures. This entire scraping process was performed using Python, to take advantage of the rich ecosystem of web scraping packages the language has.</p><p>You may well be asking, “What is an API?”, and so I will take a moment to introduce this term. Application Programming Interfaces (APIs) simply allow applications to communicate with each other and are responsible for much of the connectivity we rely upon. They act as messengers, taking your request, telling the target application what you want to do, and then returning the response. Rather than accessing the application server directly, APIs offer us a dedicated access point, improving security and reliability. A common analogy is that of a restaurant—the API is the waiter, the interface between your table and the kitchen, taking your request and returning the response (the food).</p><p>With regard to our project, we decided to access the data we wanted through an API as Twitter have made recent obfuscations to their website code to make direct scraping more difficult. Use of Twitter’s API, as is often the case, is subject to terms and conditions regarding the usage of the data obtained. Additionally, the company have implemented a rate limit; that is, a maximum number of requests they can handle in a given timespan (just like with a restaurant waiter). Careful examination of these limitations needs to be considered when using an API, but fortunately we found them to be adequate for our needs.</p><p>The application is made using Shiny, a package for the R programming language that allows you to build interactive web applications. The framework allows for the development of powerful and flexible web applications with no need for HTML, CSS or JavaScript knowledge. For this reason, Shiny stands out for its unrivalled speed of development.</p><p>Despite its benefits, the raw product of Shiny development is not always the prettiest and can lack strong mobile support. To overcome this, Matthew implemented additional styling using CSS to improve the aesthetics of the final application.</p><h2 id="takeaways"><a class="markdownIt-Anchor" href="#takeaways"></a> Takeaways</h2><p>This project allowed us three WDSS members to work together in creating something that we wouldn’t have done individually. Further, if not for the society, we would not have had the opportunity to work together. This highlights the role of WDSS, bringing people from different backgrounds together to solve challenging problems.</p><p>Projects are extremely important in growing your skills and are critical for developing a strong portfolio. Through collaboration, we can see problems from new perspectives, build our professional networks, and gain experience of working in a team. This is in comparison to university work, that is often done alone without using real world data, and usually without a solid final product.</p><p>This project leverages infrastructure offered by WDSS, such as our blogging platform and Shiny server. For this reason, alongside the support<br>offered by experienced students, working with WDSS to complete research makes it easier to get projects off the ground and showcase what you can do.</p><p>Thank you for reading and we hope you enjoy playing our implementation of Higher or Lower.</p>]]></content>
      
      
      <categories>
          
          <category> Computer Science </category>
          
          <category> Web </category>
          
      </categories>
      
      
        <tags>
            
            <tag> shiny </tag>
            
            <tag> web-scraping </tag>
            
            <tag> game </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Urban Cities: A History Told By Data</title>
      <link href="/urban-cities/"/>
      <url>/urban-cities/</url>
      
        <content type="html"><![CDATA[<div class="note info"><p><strong>Accessing Post Source</strong></p><p>We are still working on getting this site set up, so source code for this post is not yet available. Check back soon and you’ll be able to find it linked here.</p></div><h2 id="introduction"><a class="markdownIt-Anchor" href="#introduction"></a> Introduction</h2><p><img src="/" class="lazyload" data-src="/images/urban-cities/barcelona.jpg"  alt="The skyline of Barcelona"><br>An aerial observer would be forgiven for mistaking Barcelona’s octagonal blocks and diagonal streets as some red-brick reimagining of Legoland. Indeed, Spain’s 2000-year-old capital is a strict grid-like design, engineered to tackle overpopulation while maximizing airflow for its inhabitants. It’s an ancient city with all the efficiency of contemporary urban structures like New York.</p><p>Barcelona is a fascinating example, but its grid-like patterns are obvious to the human eye. I wondered about other cities with more complex features. What subtle quirks lie in the road/street structures of Bristol, Newcastle or Coventry?</p><p>For that question I wanted a scientific answer, so I set about applying data science to learn more about the intricacies of the UK’s densely populated cities.</p><h3 id="methodology"><a class="markdownIt-Anchor" href="#methodology"></a> Methodology</h3><p>This project was made possible by <a href="https://www.openstreetmap.org/" target="_blank" rel="noopener">OpenStreetMap</a> (OSM). OSM is a world-wide database of roads, trails and streets, verified by field maps and aerial imagery and maintained by an active community of engineers and GIS professionals. Its open API enables raw geodata to be sourced on any city in the world.</p><p>OSM describes geodata elements in a number of ways. Linear features, like roads or rivers, are modelled using a “way”: nodes (between 2 and 2000) connected into a simple chain of line segments, a polyline. Most whole cities with solid polygon geometries (Manchester and Birmingham to name a few) are bounded by a single “closed way”, with the same start and end nodes. These are generally those with a well-defined natural border in the real world. Others, that have no obvious boundary, are rather represented as a single point marker, denoting the center of the city. We will be focusing on the former type as the more complex definition of the geometry allows us to draw far more insight.</p><p>Of the UK cities with these ‘nice’ geometries, the twelve with the largest populations were considered and OSM API was used to fetch full network graphs, from which the bearings of streets were determined. A summary of this data is shown below.</p><table><thead><tr><th></th><th>geometry</th><th>place_name</th><th>bbox_north</th><th>bbox_south</th><th>bbox_east</th><th>bbox_west</th></tr></thead><tbody><tr><th>0</th><td>POLYGON ((-2.03365 52.40231, -2.03322 52.40217...</td><td>Birmingham, West Midlands Combined Authority, ...</td><td>52.608706</td><td>52.381053</td><td>-1.728858</td><td>-2.033649</td></tr><tr><th>1</th><td>POLYGON ((-1.80042 53.88595, -1.80041 53.88594...</td><td>Leeds, Yorkshire and the Humber, England, Unit...</td><td>53.945872</td><td>53.698968</td><td>-1.290352</td><td>-1.800421</td></tr><tr><th>2</th><td>POLYGON ((-1.80147 53.48098, -1.80049 53.48027...</td><td>Sheffield, Yorkshire and the Humber, England, ...</td><td>53.503104</td><td>53.304512</td><td>-1.324669</td><td>-1.801471</td></tr><tr><th>3</th><td>POLYGON ((-2.06125 53.82562, -2.06017 53.82504...</td><td>Bradford, Yorkshire and the Humber, England, U...</td><td>53.963151</td><td>53.724341</td><td>-1.640330</td><td>-2.061248</td></tr><tr><th>4</th><td>POLYGON ((-2.31992 53.41161, -2.31847 53.40999...</td><td>Manchester, Greater Manchester, North West Eng...</td><td>53.544592</td><td>53.340104</td><td>-2.146829</td><td>-2.319918</td></tr><tr><th>5</th><td>POLYGON ((-3.01917 53.43616, -3.01806 53.43323...</td><td>Liverpool, North West England, England, United...</td><td>53.474967</td><td>53.311543</td><td>-2.818000</td><td>-3.019173</td></tr><tr><th>6</th><td>POLYGON ((-2.71837 51.50617, -2.71837 51.50616...</td><td>Bristol, City of Bristol, South West England, ...</td><td>51.544432</td><td>51.397284</td><td>-2.510419</td><td>-2.718370</td></tr><tr><th>7</th><td>POLYGON ((-1.62490 53.65363, -1.62488 53.65358...</td><td>Wakefield, Yorkshire and the Humber, England, ...</td><td>53.741811</td><td>53.575349</td><td>-1.198814</td><td>-1.624898</td></tr><tr><th>8</th><td>POLYGON ((-1.61446 52.42795, -1.61412 52.42774...</td><td>Coventry, West Midlands Combined Authority, We...</td><td>52.464772</td><td>52.363885</td><td>-1.423957</td><td>-1.614459</td></tr><tr><th>9</th><td>POLYGON ((-1.24696 52.95344, -1.24689 52.95317...</td><td>City of Nottingham, East Midlands, England, Un...</td><td>53.018672</td><td>52.889008</td><td>-1.086119</td><td>-1.246956</td></tr><tr><th>10</th><td>POLYGON ((-1.77567 54.98962, -1.77566 54.98955...</td><td>Newcastle upon Tyne, Tyne and Wear, North East...</td><td>55.079382</td><td>54.959032</td><td>-1.529200</td><td>-1.775672</td></tr><tr><th>11</th><td>POLYGON ((-1.56888 54.92462, -1.56824 54.92409...</td><td>Sunderland, Tyne and Wear, North East England,...</td><td>54.944170</td><td>54.799042</td><td>-1.345665</td><td>-1.568879</td></tr></tbody></table><h2 id="visualizations"><a class="markdownIt-Anchor" href="#visualizations"></a> Visualizations</h2><p>For each of these 12 cities, the returned bearings were weighted by street length to produce the following visualisations using simple polar projection plots. It is important to note two key points about this approach:</p><ol><li>The bearings of streets were derived from only their start and finish nodes, ignoring paths in-between. This was to ensure reasonable computation times as an alternative to using the full polylines.</li><li>Naturally, bearings are rotationally symmetric as we do not account for the direction of one-way streets.</li></ol><h3 id="birmingham"><a class="markdownIt-Anchor" href="#birmingham"></a> Birmingham</h3><p><img src="/" class="lazyload" data-src="/images/urban-cities/urban-cities_11_0.png"  alt=""></p><p>Birmingham’s visualization is unmistakably circular. Many older cities lack a grid structure, with impromptu-built streets going off in many different directions. The place now called “Birmingham” has been around for more than 1,400 years. It was believed to have been established by a Saxon tribe, before expanding over the centuries into the city of 8500 streets we know today.</p><p><img src="/" class="lazyload" data-src="/images/urban-cities/prospect_of_birmingham.jpg"  alt="William Westley's 1732 Prospect of Birmingham"></p><h3 id="manchester-newcastle"><a class="markdownIt-Anchor" href="#manchester-newcastle"></a> Manchester &amp; Newcastle</h3><p><img src="/" class="lazyload" data-src="/images/urban-cities/urban-cities_15_0.png"  alt=""></p><p>Manchester displays quite a clear cross-like visualization. This city has an interesting grid structure with a strong emphasis on moving north-to-south. The east-to-west flow is perhaps due to Manchester sitting almost directly between Liverpool and Sheffield, and the reduced SE/NW activity might result from its position just in the upper left of the Peak District.</p><p>Newcastle is similar but for a more obvious reason; its central ring-road system, which is vaguely hexagonal, is presumably responsible for the high degree of symmetry we observe.</p><h3 id="coventry"><a class="markdownIt-Anchor" href="#coventry"></a> Coventry</h3><p><img src="/" class="lazyload" data-src="/images/urban-cities/urban-cities_18_0.png"  alt=""></p><p>Not unlike Manchester, Coventry’s visualization hints at a grid-like design. Famously, this ancient city—once a hotspot of trade for cloth and textiles—was obliterated in 1940 by a series of bombing raids, now called the Coventry Blitz. In the decades following, the city’s remains were rebuilt into a modern grid structure.</p><h3 id="bristol"><a class="markdownIt-Anchor" href="#bristol"></a> Bristol</h3><p><img src="/" class="lazyload" data-src="/images/urban-cities/urban-cities_21_0.png"  alt=""></p><p>For a city founded on the turn of the second-last millennia, we wouldn’t expect to see much beyond a uniformly distributed set of bearings. Yet Bristol’s otherwise almost-circular plot is cut along the NE/SW line. Modern Bristol is dominated by the M5, as well as the River Avon. It’s interesting that a 1960s motorway construction can have such an impact on the bearings of an ancient city.</p><h2 id="final-thoughts"><a class="markdownIt-Anchor" href="#final-thoughts"></a> Final Thoughts</h2><p>As well as analyzing the street orientations of individual cities, it is beneficial to to showcase their visualizations side-by-side to draw comparisons and appreciate relative differences. To extend, I have collated the visualizations for the twelve cities I considered into one final image.</p><p><img src="/" class="lazyload" data-src="/images/urban-cities/urban-cities_25_0.png"  alt=""></p><h3 id="the-importance-of-data-science"><a class="markdownIt-Anchor" href="#the-importance-of-data-science"></a> The Importance of Data Science</h3><p>I think it important to observe how data science, once a niche and theoretical discipline, can offer such rich insights into the history of the UK’s major cities. From Birmingham’s pre-industrial beginnings, to the devastating campaigns of the German Luftwaffe, these simple visualisations tell stories of our past. They compress a millennia-long archive of the achievements and failures that gave rise to modern Britain.</p><p>To that extent, if you are a data scientist wondering how you can apply your technical skills to real-world projects, or a student with domain expertise, keen to build up the technical skills to answer the questions you care about, make sure you follow Warwick Data Science Society <a href="https://www.facebook.com/warwickdatascience" target="_blank" rel="noopener">on social media</a> to keep up-to-date with relevant opportunties. These include academic talks, data science news, workshops, and beginners programming courses, so there is certainly something for everyone. Thank you for reading this piece.</p>]]></content>
      
      
      <categories>
          
          <category> Humanities </category>
          
          <category> Geography </category>
          
          <category> History </category>
          
      </categories>
      
      
        <tags>
            
            <tag> visualization </tag>
            
            <tag> data-analysis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>What Factors Actually Affect Your Grades?</title>
      <link href="/school-success/"/>
      <url>/school-success/</url>
      
        <content type="html"><![CDATA[<div class="note info"><p><strong>Accessing Post Source</strong><br>We are still working on getting this site set up, so source code for this post is not yet available. Check back soon and you’ll be able to find it linked here.</p></div><p>With exam period approaching fast, every student is wondering how to score the best possible grade. Some factors—like how much sleep you’re getting or how healthy you are—seem to have an obvious correlation with your final grade. What about your relationship status? How much should you be studying to achieve the grade you want? Does the subject you’re studying influence your final grade? In this article, we will use two datasets containing student math and Portuguese language performance in two different Portuguese schools and see which factors affected student performance the most.</p><h2 id="exploratory-data-analysis"><a class="markdownIt-Anchor" href="#exploratory-data-analysis"></a> Exploratory Data Analysis</h2><h3 id="dataset-overview"><a class="markdownIt-Anchor" href="#dataset-overview"></a> Dataset Overview</h3><p>The variables are the same for the two datasets:</p><table><thead><tr><th style="text-align:center">Variable</th><th style="text-align:center">Description</th><th style="text-align:center">Type</th><th style="text-align:center">Possible Values</th></tr></thead><tbody><tr><td style="text-align:center">school</td><td style="text-align:center">School</td><td style="text-align:center">binary</td><td style="text-align:center">GP—Gabriel Pereira; MS—Mousinho da Silveira</td></tr><tr><td style="text-align:center">sex</td><td style="text-align:center">Sex</td><td style="text-align:center">binary</td><td style="text-align:center">F—female; M—male</td></tr><tr><td style="text-align:center">age</td><td style="text-align:center">Age</td><td style="text-align:center">numeric</td><td style="text-align:center">15–22, inclusive</td></tr><tr><td style="text-align:center">address</td><td style="text-align:center">Address type</td><td style="text-align:center">binary</td><td style="text-align:center">U—Urban; R—Rural</td></tr><tr><td style="text-align:center">famsize</td><td style="text-align:center">Family Size</td><td style="text-align:center">binary</td><td style="text-align:center">LE3—less than or equal to 3; GE3—greater than 3</td></tr><tr><td style="text-align:center">Pstatus</td><td style="text-align:center">Parent’s cohabitation status</td><td style="text-align:center">binary</td><td style="text-align:center">T—living together; A—living apart</td></tr><tr><td style="text-align:center">Medu</td><td style="text-align:center">Mother’s Education</td><td style="text-align:center">ordinal</td><td style="text-align:center">0—none; 1—up to 4th grade; 2—5th–9th grade; 3—secondary; 4—higher</td></tr><tr><td style="text-align:center">Fedu</td><td style="text-align:center">Father’s Education</td><td style="text-align:center">ordinal</td><td style="text-align:center">0—none; 1—up to 4th grade; 2—5th–9th grade; 3—secondary; 4—higher</td></tr><tr><td style="text-align:center">Mjob</td><td style="text-align:center">Mother’s Job</td><td style="text-align:center">nominal</td><td style="text-align:center">teacher; health(-care related); (civil )services; at home; other</td></tr><tr><td style="text-align:center">Fjob</td><td style="text-align:center">Father’s Job</td><td style="text-align:center">nominal</td><td style="text-align:center">teacher; health(-care related); (civil )services; at home; other</td></tr><tr><td style="text-align:center">reason</td><td style="text-align:center">Reason for choosing school</td><td style="text-align:center">nominal</td><td style="text-align:center">(close to )home; (school )reputation; course(preference); other</td></tr><tr><td style="text-align:center">guardian</td><td style="text-align:center">Student’s guardian</td><td style="text-align:center">nominal</td><td style="text-align:center">mother; father; other</td></tr><tr><td style="text-align:center">traveltime</td><td style="text-align:center">Travel time to school</td><td style="text-align:center">ordinal</td><td style="text-align:center">1—&lt;15 min.; 2—15–30 min.; 3—30 min.–1 hour; 4—&gt;1 hour</td></tr><tr><td style="text-align:center">studytime</td><td style="text-align:center">Weekly study time</td><td style="text-align:center">ordinal</td><td style="text-align:center">1—&lt;2 hours; 2—2–5 hours; 3—5–10 hours; 4—&gt;10 hours</td></tr><tr><td style="text-align:center">failures</td><td style="text-align:center">Past class failures</td><td style="text-align:center">numeric</td><td style="text-align:center">0–3, else 4</td></tr><tr><td style="text-align:center">schoolsup</td><td style="text-align:center">Extra educational support</td><td style="text-align:center">binary</td><td style="text-align:center">yes; no</td></tr><tr><td style="text-align:center">famsup</td><td style="text-align:center">Family educational support</td><td style="text-align:center">binary</td><td style="text-align:center">yes; no</td></tr><tr><td style="text-align:center">paid</td><td style="text-align:center">Extra paid classes</td><td style="text-align:center">binary</td><td style="text-align:center">yes; no</td></tr><tr><td style="text-align:center">activities</td><td style="text-align:center">Extra-curricular activities</td><td style="text-align:center">binary</td><td style="text-align:center">yes; no</td></tr><tr><td style="text-align:center">nursery</td><td style="text-align:center">Attend nursery</td><td style="text-align:center">binary</td><td style="text-align:center">yes; no</td></tr><tr><td style="text-align:center">higher</td><td style="text-align:center">Wants to take higher education</td><td style="text-align:center">binary</td><td style="text-align:center">yes; no</td></tr><tr><td style="text-align:center">internet</td><td style="text-align:center">Home internet access</td><td style="text-align:center">binary</td><td style="text-align:center">yes; no</td></tr><tr><td style="text-align:center">romantic</td><td style="text-align:center">In a romantic relationship</td><td style="text-align:center">binary</td><td style="text-align:center">yes; no</td></tr><tr><td style="text-align:center">famrel</td><td style="text-align:center">Quality of family relationships</td><td style="text-align:center">ordinal</td><td style="text-align:center">1—very bad to 5—very good</td></tr><tr><td style="text-align:center">freetime</td><td style="text-align:center">Free time after school</td><td style="text-align:center">ordinal</td><td style="text-align:center">1—very low to 5—very high</td></tr><tr><td style="text-align:center">goout</td><td style="text-align:center">Going out with friends</td><td style="text-align:center">ordinal</td><td style="text-align:center">1—very low to 5—very high</td></tr><tr><td style="text-align:center">Dalc</td><td style="text-align:center">Workday alcohol consumption</td><td style="text-align:center">ordinal</td><td style="text-align:center">1—very low to 5—very high</td></tr><tr><td style="text-align:center">Walc</td><td style="text-align:center">Weekend alcohol consumption</td><td style="text-align:center">ordinal</td><td style="text-align:center">1—very low to 5—very high</td></tr><tr><td style="text-align:center">health</td><td style="text-align:center">Current health status</td><td style="text-align:center">ordinal</td><td style="text-align:center">1—very bad to 5—very good</td></tr><tr><td style="text-align:center">absences</td><td style="text-align:center">Number absences</td><td style="text-align:center">numeric</td><td style="text-align:center">0–93</td></tr><tr><td style="text-align:center">G1</td><td style="text-align:center">First Period Grade</td><td style="text-align:center">numeric</td><td style="text-align:center">0–20</td></tr><tr><td style="text-align:center">G2</td><td style="text-align:center">Second Period Grade</td><td style="text-align:center">numeric</td><td style="text-align:center">0–20</td></tr><tr><td style="text-align:center">G3</td><td style="text-align:center">Final Grade</td><td style="text-align:center">numeric</td><td style="text-align:center">0–20</td></tr></tbody></table><p>I will be conducting a basic analysis of the dataset followed by visualizations of the correlations between different factors. Finally, I will build a linear regression model for each subject to predict the students’ final grades.</p><p>We will start by importing all the necessary packages and load the datasets into a pandas dataframe.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import necessary packages</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd  </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np  </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> statistics <span class="keyword">as</span> stats</span><br><span class="line"><span class="keyword">import</span> statsmodels.api <span class="keyword">as</span> sm</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the dataset from the csv file using pandas</span></span><br><span class="line">data_m = pd.read_csv(<span class="string">r'data/student-mat.csv'</span>, sep=<span class="string">';'</span>)</span><br><span class="line">data_p = pd.read_csv(<span class="string">r'data/student-por.csv'</span>, sep=<span class="string">';'</span>)</span><br></pre></td></tr></table></figure><p>We can start by taking a look at the first few rows of each dataset.</p><pre><code>First 5 lines of the math performance dataset:</code></pre><table><thead><tr><th></th><th>school</th><th>sex</th><th>age</th><th>address</th><th>famsize</th><th>Pstatus</th><th>Medu</th><th>Fedu</th><th>Mjob</th><th>Fjob</th><th>reason</th><th>guardian</th><th>traveltime</th><th>studytime</th><th>failures</th><th>schoolsup</th><th>famsup</th><th>paid</th><th>activities</th><th>nursery</th><th>higher</th><th>internet</th><th>romantic</th><th>famrel</th><th>freetime</th><th>goout</th><th>Dalc</th><th>Walc</th><th>health</th><th>absences</th><th>G1</th><th>G2</th><th>G3</th></tr></thead><tbody><tr><th>0</th><td>GP</td><td>F</td><td>18</td><td>U</td><td>GT3</td><td>A</td><td>4</td><td>4</td><td>at_home</td><td>teacher</td><td>course</td><td>mother</td><td>2</td><td>2</td><td>0</td><td>yes</td><td>no</td><td>no</td><td>no</td><td>yes</td><td>yes</td><td>no</td><td>no</td><td>4</td><td>3</td><td>4</td><td>1</td><td>1</td><td>3</td><td>6</td><td>5</td><td>6</td><td>6</td></tr><tr><th>1</th><td>GP</td><td>F</td><td>17</td><td>U</td><td>GT3</td><td>T</td><td>1</td><td>1</td><td>at_home</td><td>other</td><td>course</td><td>father</td><td>1</td><td>2</td><td>0</td><td>no</td><td>yes</td><td>no</td><td>no</td><td>no</td><td>yes</td><td>yes</td><td>no</td><td>5</td><td>3</td><td>3</td><td>1</td><td>1</td><td>3</td><td>4</td><td>5</td><td>5</td><td>6</td></tr><tr><th>2</th><td>GP</td><td>F</td><td>15</td><td>U</td><td>LE3</td><td>T</td><td>1</td><td>1</td><td>at_home</td><td>other</td><td>other</td><td>mother</td><td>1</td><td>2</td><td>3</td><td>yes</td><td>no</td><td>yes</td><td>no</td><td>yes</td><td>yes</td><td>yes</td><td>no</td><td>4</td><td>3</td><td>2</td><td>2</td><td>3</td><td>3</td><td>10</td><td>7</td><td>8</td><td>10</td></tr><tr><th>3</th><td>GP</td><td>F</td><td>15</td><td>U</td><td>GT3</td><td>T</td><td>4</td><td>2</td><td>health</td><td>services</td><td>home</td><td>mother</td><td>1</td><td>3</td><td>0</td><td>no</td><td>yes</td><td>yes</td><td>yes</td><td>yes</td><td>yes</td><td>yes</td><td>yes</td><td>3</td><td>2</td><td>2</td><td>1</td><td>1</td><td>5</td><td>2</td><td>15</td><td>14</td><td>15</td></tr><tr><th>4</th><td>GP</td><td>F</td><td>16</td><td>U</td><td>GT3</td><td>T</td><td>3</td><td>3</td><td>other</td><td>other</td><td>home</td><td>father</td><td>1</td><td>2</td><td>0</td><td>no</td><td>yes</td><td>yes</td><td>no</td><td>yes</td><td>yes</td><td>no</td><td>no</td><td>4</td><td>3</td><td>2</td><td>1</td><td>2</td><td>5</td><td>4</td><td>6</td><td>10</td><td>10</td></tr></tbody></table><pre><code>First 5 lines of the Portuguese performance dataset:</code></pre><table><thead><tr><th></th><th>school</th><th>sex</th><th>age</th><th>address</th><th>famsize</th><th>Pstatus</th><th>Medu</th><th>Fedu</th><th>Mjob</th><th>Fjob</th><th>reason</th><th>guardian</th><th>traveltime</th><th>studytime</th><th>failures</th><th>schoolsup</th><th>famsup</th><th>paid</th><th>activities</th><th>nursery</th><th>higher</th><th>internet</th><th>romantic</th><th>famrel</th><th>freetime</th><th>goout</th><th>Dalc</th><th>Walc</th><th>health</th><th>absences</th><th>G1</th><th>G2</th><th>G3</th></tr></thead><tbody><tr><th>0</th><td>GP</td><td>F</td><td>18</td><td>U</td><td>GT3</td><td>A</td><td>4</td><td>4</td><td>at_home</td><td>teacher</td><td>course</td><td>mother</td><td>2</td><td>2</td><td>0</td><td>yes</td><td>no</td><td>no</td><td>no</td><td>yes</td><td>yes</td><td>no</td><td>no</td><td>4</td><td>3</td><td>4</td><td>1</td><td>1</td><td>3</td><td>4</td><td>0</td><td>11</td><td>11</td></tr><tr><th>1</th><td>GP</td><td>F</td><td>17</td><td>U</td><td>GT3</td><td>T</td><td>1</td><td>1</td><td>at_home</td><td>other</td><td>course</td><td>father</td><td>1</td><td>2</td><td>0</td><td>no</td><td>yes</td><td>no</td><td>no</td><td>no</td><td>yes</td><td>yes</td><td>no</td><td>5</td><td>3</td><td>3</td><td>1</td><td>1</td><td>3</td><td>2</td><td>9</td><td>11</td><td>11</td></tr><tr><th>2</th><td>GP</td><td>F</td><td>15</td><td>U</td><td>LE3</td><td>T</td><td>1</td><td>1</td><td>at_home</td><td>other</td><td>other</td><td>mother</td><td>1</td><td>2</td><td>0</td><td>yes</td><td>no</td><td>no</td><td>no</td><td>yes</td><td>yes</td><td>yes</td><td>no</td><td>4</td><td>3</td><td>2</td><td>2</td><td>3</td><td>3</td><td>6</td><td>12</td><td>13</td><td>12</td></tr><tr><th>3</th><td>GP</td><td>F</td><td>15</td><td>U</td><td>GT3</td><td>T</td><td>4</td><td>2</td><td>health</td><td>services</td><td>home</td><td>mother</td><td>1</td><td>3</td><td>0</td><td>no</td><td>yes</td><td>no</td><td>yes</td><td>yes</td><td>yes</td><td>yes</td><td>yes</td><td>3</td><td>2</td><td>2</td><td>1</td><td>1</td><td>5</td><td>0</td><td>14</td><td>14</td><td>14</td></tr><tr><th>4</th><td>GP</td><td>F</td><td>16</td><td>U</td><td>GT3</td><td>T</td><td>3</td><td>3</td><td>other</td><td>other</td><td>home</td><td>father</td><td>1</td><td>2</td><td>0</td><td>no</td><td>yes</td><td>no</td><td>no</td><td>yes</td><td>yes</td><td>no</td><td>no</td><td>4</td><td>3</td><td>2</td><td>1</td><td>2</td><td>5</td><td>0</td><td>11</td><td>13</td><td>13</td></tr></tbody></table><p>An important detail to note is that there are 395 high school students in the math dataset and 649 in the Portuguese dataset. The grades of the student are from 0 to 20. Furthermore, there are 16 numerical variables out of 33; the rest of the variables will need to be one-hot encoded when we will analyze correlations and build the regression model.</p><p>Now let’s visualize the final grades distributions for both subjects.</p><p><img src="/" class="lazyload" data-src="/images/school-success/school-success_13_0.png"  alt="Distribution of student grades for math and Portuguese"></p><p>We can also calculate that the average final grades for math and Portuguese students are 10.42 and 11.91, respectively. This suggests that Portuguese students score higher on average than math students although this comparison could easily have been skewed by the large number of math students scoring zero.</p><h3 id="finding-and-visualizing-correlations-for-numerical-variables"><a class="markdownIt-Anchor" href="#finding-and-visualizing-correlations-for-numerical-variables"></a> Finding and Visualizing Correlations for Numerical Variables</h3><p>We are now going to automatically find the variables with the strongest correlation to the final grades for both datasets. Finding correlations between non-numeric features and the outcome can get a bit messy, so we will focus on testing only the existing numerical values of the datasets at first. To better visualize the insights, we will also use correlation bar plots and heat maps for both datasets.</p><p><img src="/" class="lazyload" data-src="/images/school-success/school-success_17_0.png"  alt="Correlations between numeric predictors and the response for math"></p><p>To interpret correlation bar plots and heat map:</p><ul><li>The darker the bar/square, the stronger the correlation is.</li><li>Brown represents negative correlations, whereas purple represents positive correlations.</li></ul><p><img src="/" class="lazyload" data-src="/images/school-success/school-success_19_0.png"  alt="Correlations between numeric predictors and the response for Portuguese"></p><p>Insights:</p><ul><li>For both datasets, the number of past classes <code>failures</code> has a strong negative correlation with <code>G3</code>.</li><li>Other common variables with a negative correlation are <code>age</code>, frequency of going out with friends (<code>goout</code>), <code>traveltime</code>, <code>freetime</code> and <code>health</code>.</li><li><code>G1</code> and <code>G2</code> have very strong positive correlation coefficients for both datasets because student performance usually remains constant throughout the year; we will therefore ignore them.</li><li>Other common variables with a positive correlation are: <code>studytime</code>, education of parents (<code>Fedu</code> and <code>Medu</code>) and family relationship (<code>famrel</code>).</li></ul><h3 id="one-hot-encoding"><a class="markdownIt-Anchor" href="#one-hot-encoding"></a> One-Hot Encoding</h3><p>In order to get more insight from these datasets, we need to be able to use the categorical variables as well. An example of categorical variable is the <code>school</code> variable (the student is either at Gabriel Pereira or Mousinho da Silveira) as there are multiple possible values with no intrinsic ordering. We will use a technique called one-hot encoding, which assigns binary value to each category level indicating whether or not that level was the value of the original predictor. Here is an example of how it would look like for the variable father’s job (<code>Fjob</code>).</p><table><thead><tr><th>Father’s Job</th><th style="text-align:center">Occupation_teacher</th><th style="text-align:center">Occupation_health</th><th style="text-align:center">Occupation_services</th><th style="text-align:center">Occupation_at_home</th><th style="text-align:center">Occupation_other</th></tr></thead><tbody><tr><td>teacher</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td>health</td><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td>services</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td>at_home</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">0</td></tr><tr><td>other</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">1</td></tr></tbody></table><div class="note warning"><p><strong>Technical Detail</strong><br>One-hot encoding is actually slightly more subtle than the description given above. The missing detail is that we often drop the first of the resulting binary columns. The reason we do this is that knowing the value of the other columns is enough to be certain of the value of the first. Indeed, if all of the other columns are zero, then the first column must be one, and vice-versa. Removing the first column is important as the algebra behind linear regression fails we duplicate predictor information.</p></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">one_hot_encode</span><span class="params">(df)</span>:</span></span><br><span class="line">    <span class="comment"># Select only categorical variables</span></span><br><span class="line">    cat_df = df.select_dtypes(include=[<span class="string">'object'</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># One-hot encode variables</span></span><br><span class="line">    dummy_df = pd.get_dummies(cat_df, drop_first=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Add the response back and return</span></span><br><span class="line">    dummy_df[<span class="string">'G3'</span>] = df[<span class="string">'G3'</span>]</span><br><span class="line">    <span class="keyword">return</span> dummy_df</span><br><span class="line"></span><br><span class="line"><span class="comment"># One-hot encode both datasets</span></span><br><span class="line">dummy_dfm = one_hot_encode(data_m)</span><br><span class="line">dummy_dfp = one_hot_encode(data_p)</span><br></pre></td></tr></table></figure><h3 id="finding-and-visualizing-correlations-for-encoded-categorical-variables"><a class="markdownIt-Anchor" href="#finding-and-visualizing-correlations-for-encoded-categorical-variables"></a> Finding and Visualizing Correlations for Encoded Categorical Variables</h3><p>We can now analyze the correlation coefficients for the final grades of all the variables for both datasets.</p><p><img src="/" class="lazyload" data-src="/images/school-success/school-success_28_0.png"  alt="Correlations between categorical predictors and the response for math"></p><p><img src="/" class="lazyload" data-src="/images/school-success/school-success_29_0.png"  alt="Correlations between categorical predictors and the response for Portuguese"></p><p>Insights:</p><ul><li>Variables that impact negatively final grades in both datasets: in a romantic relationship (<code>romantic_yes</code>), does not want to go to higher education (<code>higher_no</code>), lives in a rural area (<code>address_R</code>) and has no access to internet (<code>internet_no</code>).</li><li>Variables that impact positively final grades in both datasets: not in a romantic relationship (<code>romantic_no</code>), wants to go to higher education (<code>higher_yes</code>), lives in a urban area (<code>address_U</code>), has access to internet (<code>internet_no</code>).</li><li>In Portuguese performance dataset, the <code>school</code> variable has a very high impact on the final grade (negatively impacted if goes to MS and positively impacted if goes to GP).</li><li>Males seem to score higher in math whereas females score higher in portuguese.</li></ul><h2 id="visualizing-key-trends"><a class="markdownIt-Anchor" href="#visualizing-key-trends"></a> Visualizing Key Trends</h2><p>Some of the results are quite unexpected so let’s visualize them.</p><h3 id="effect-of-address-type-on-grades"><a class="markdownIt-Anchor" href="#effect-of-address-type-on-grades"></a> Effect of Address Type on Grades</h3><p><img src="/" class="lazyload" data-src="/images/school-success/school-success_34_0.png"  alt="Impact of address type on student performance"></p><p>Insights:</p><ul><li>For math performance, there is not too much difference between urban and rural students. However, urban students tend to score slightly more.</li><li>For portuguese performance, we can see that urban students score higher more often than rural students.</li></ul><h3 id="effect-of-relationship-status-on-grades"><a class="markdownIt-Anchor" href="#effect-of-relationship-status-on-grades"></a> Effect of Relationship Status on Grades</h3><p><img src="/" class="lazyload" data-src="/images/school-success/school-success_37_0.png"  alt="Impact of relationship status on student performance"></p><p>Note, that of the 395 math students, 132 (33.4%) were in a relationship. Likewise 239 (36.8%) or of the 649 Portuguese students were in a relationship</p><p>Insights:</p><ul><li>In both datasets, there are more single students than in a relationship (only 33% in math dataset and 36% in portuguese dataset). This might skew results as there is less data to analyze for students in a relationship. We can see that in the Portuguese dataset where there are more values to analyze, the scatter plot shapes tend to look more similar.</li><li>Not enough data to say if relationship has true impact on math performance.</li></ul><h3 id="effect-of-sex-on-grades"><a class="markdownIt-Anchor" href="#effect-of-sex-on-grades"></a> Effect of Sex on Grades</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">fig, axs = plt.subplots(<span class="number">2</span>, <span class="number">1</span>, figsize=(<span class="number">12</span>,<span class="number">10</span>))</span><br><span class="line">plt.subplots_adjust(hspace=<span class="number">.25</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sex_plot</span><span class="params">(data, ax, subject)</span>:</span></span><br><span class="line">    ax.set_xlim(<span class="number">0</span>, <span class="number">20</span>)</span><br><span class="line">    sns.kdeplot(data.loc[data[<span class="string">'sex'</span>] == <span class="string">'F'</span>, <span class="string">'G3'</span>], label=<span class="string">'Female'</span>, shade=<span class="literal">True</span>, ax=ax)</span><br><span class="line">    sns.kdeplot(data.loc[data[<span class="string">'sex'</span>] == <span class="string">'M'</span>, <span class="string">'G3'</span>], label=<span class="string">'Male'</span>, shade=<span class="literal">True</span>, ax=ax)</span><br><span class="line">    ax.set_title(<span class="string">f'Female vs Male Students <span class="subst">&#123;subject&#125;</span> Performance'</span>)</span><br><span class="line">    ax.set_xlabel(<span class="string">'Grade'</span>)</span><br><span class="line">    ax.set_ylabel(<span class="string">'Density'</span>)</span><br><span class="line"></span><br><span class="line">sex_plot(data_m, axs[<span class="number">0</span>], subject=<span class="string">'Math'</span>)</span><br><span class="line">sex_plot(data_p, axs[<span class="number">1</span>], subject=<span class="string">'Portuguese'</span>)</span><br></pre></td></tr></table></figure><p><img src="/" class="lazyload" data-src="/images/school-success/school-success_41_0.png"  alt="Impact of sex on student performance"></p><h3 id="effect-of-school-choice-on-grades"><a class="markdownIt-Anchor" href="#effect-of-school-choice-on-grades"></a> Effect of School Choice on Grades</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Analyzing impact of choice of school on Portuguese performance</span></span><br><span class="line">plt.subplots(figsize=(<span class="number">12</span>,<span class="number">8</span>))</span><br><span class="line">b = sns.swarmplot(x=<span class="string">'school'</span>, y=<span class="string">'G3'</span>, data=data_p)</span><br><span class="line">b.axes.set_title(<span class="string">'School Choice vs Final Grade Portuguese'</span>)</span><br><span class="line">b.set_xlabel(<span class="string">'School'</span>)</span><br><span class="line">b.set_ylabel(<span class="string">'Final Grade'</span>);</span><br></pre></td></tr></table></figure><p><img src="/" class="lazyload" data-src="/images/school-success/school-success_43_0.png"  alt="Impact of school choice on student performance"></p><p>Insights:</p><ul><li>From the available data, MS students (<code>school_MS</code>) tend to score less than GP students (<code>school_GP</code>) in Portuguese. Maybe GP is specialized in Portuguese and students have access to higher-quality resources.</li><li>However as for the relationship analysis, there are less students going to MS so it might affect results.</li></ul><h2 id="model-fitting"><a class="markdownIt-Anchor" href="#model-fitting"></a> Model-fitting</h2><p>We are now going to build a multi-linear regression model for both datasets. To avoid the impact of correlated variables, we only use the top twelve most influential predictors. We start with the math scores.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit_regression_model</span><span class="params">(df, dummy_df)</span>:</span></span><br><span class="line">    num_df = df.select_dtypes(exclude=[<span class="string">'object'</span>])</span><br><span class="line">    full_df = pd.concat([num_df, dummy_df.drop(<span class="string">'G3'</span>, axis=<span class="number">1</span>)], axis=<span class="number">1</span>)</span><br><span class="line">    full_df.drop([<span class="string">'G1'</span>, <span class="string">'G2'</span>], axis=<span class="number">1</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">    most_inf = np.abs(full_df.corr()[<span class="string">'G3'</span>]).sort_values()[<span class="number">-13</span>:].index</span><br><span class="line">    red_df = full_df.loc[:, most_inf]</span><br><span class="line"></span><br><span class="line">    X = np.array(red_df.drop(<span class="string">'G3'</span>, axis=<span class="number">1</span>))</span><br><span class="line">    y = np.array(red_df[<span class="string">'G3'</span>])</span><br><span class="line"></span><br><span class="line">    Z = sm.add_constant(X)</span><br><span class="line">    mod = sm.OLS(y, Z).fit()</span><br><span class="line">    </span><br><span class="line">    results_as_html = mod.summary().tables[<span class="number">1</span>].as_html()</span><br><span class="line">    coeffs = pd.read_html(results_as_html, header=<span class="number">0</span>, index_col=<span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line">    coeffs = coeffs.set_index(pd.Index([<span class="string">'intercept'</span>]).append(red_df.drop(<span class="string">'G3'</span>, axis=<span class="number">1</span>).columns))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> mod.rsquared, coeffs</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">r2_m, coeffs_m = fit_regression_model(data_m, dummy_dfm)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"Model R^2: <span class="subst">&#123;r2_m:<span class="number">.02</span>f&#125;</span>"</span>)</span><br><span class="line">display(coeffs_m)</span><br></pre></td></tr></table></figure><pre><code>Model R^2: 0.20</code></pre><table><thead><tr><th></th><th>coef</th><th>std err</th><th>t</th><th>P&gt;|t|</th><th>[0.025</th><th>0.975]</th></tr></thead><tbody><tr><th>intercept</th><td>10.2932</td><td>3.454</td><td>2.981</td><td>0.003</td><td>3.503</td><td>17.083</td></tr><tr><th>paid_yes</th><td>0.2347</td><td>0.439</td><td>0.534</td><td>0.593</td><td>-0.629</td><td>1.099</td></tr><tr><th>sex_M</th><td>1.1617</td><td>0.436</td><td>2.665</td><td>0.008</td><td>0.305</td><td>2.019</td></tr><tr><th>address_U</th><td>0.5889</td><td>0.543</td><td>1.084</td><td>0.279</td><td>-0.479</td><td>1.657</td></tr><tr><th>Mjob_health</th><td>1.1831</td><td>0.779</td><td>1.519</td><td>0.130</td><td>-0.349</td><td>2.715</td></tr><tr><th>traveltime</th><td>-0.2877</td><td>0.324</td><td>-0.887</td><td>0.376</td><td>-0.926</td><td>0.350</td></tr><tr><th>romantic_yes</th><td>-0.8371</td><td>0.458</td><td>-1.829</td><td>0.068</td><td>-1.737</td><td>0.063</td></tr><tr><th>goout</th><td>-0.4753</td><td>0.194</td><td>-2.450</td><td>0.015</td><td>-0.857</td><td>-0.094</td></tr><tr><th>Fedu</th><td>-0.1004</td><td>0.251</td><td>-0.400</td><td>0.689</td><td>-0.594</td><td>0.393</td></tr><tr><th>age</th><td>-0.0467</td><td>0.178</td><td>-0.263</td><td>0.793</td><td>-0.396</td><td>0.302</td></tr><tr><th>higher_yes</th><td>1.4442</td><td>1.044</td><td>1.383</td><td>0.167</td><td>-0.608</td><td>3.497</td></tr><tr><th>Medu</th><td>0.4811</td><td>0.259</td><td>1.861</td><td>0.064</td><td>-0.027</td><td>0.989</td></tr><tr><th>failures</th><td>-1.7399</td><td>0.314</td><td>-5.547</td><td>0.000</td><td>-2.357</td><td>-1.123</td></tr></tbody></table><p>Insights for math data set linear regression model:</p><ul><li>Our model explains explains 20% of the inputs into the final grade (<code>G3</code>), however it could still be improve if the goal of this article would be pure accuracy.</li><li>We can see that the willingness of the student to go into higher education (<code>higher_yes</code>) is a variable with one of the largest absolute coefficients. If the student is willing to go into higher education, their score will increase, on average, by 1.44 points.</li><li>There are other statistically significant coefficients such as <code>failures</code>, <code>sex_M</code>, and <code>goout</code>.</li><li>For example, <code>failures</code> plays a decisive role in student performance: for each class the student has failed in the past, they can roughly except a decrease of 1.74 in their final score.</li></ul><p>And now for the Portuguese scores.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">r2_p, coeffs_p = fit_regression_model(data_p, dummy_dfp)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"Model R^2: <span class="subst">&#123;r2_p:<span class="number">.03</span>f&#125;</span>"</span>)</span><br><span class="line">display(coeffs_p)</span><br></pre></td></tr></table></figure><pre><code>Model R^2: 0.305</code></pre><table><thead><tr><th></th><th>coef</th><th>std err</th><th>t</th><th>P&gt;|t|</th><th>[0.025</th><th>0.975]</th></tr></thead><tbody><tr><th>intercept</th><td>9.8593</td><td>0.612</td><td>16.114</td><td>0.000</td><td>8.658</td><td>11.061</td></tr><tr><th>Mjob_teacher</th><td>0.2976</td><td>0.384</td><td>0.776</td><td>0.438</td><td>-0.456</td><td>1.051</td></tr><tr><th>internet_yes</th><td>0.3248</td><td>0.269</td><td>1.207</td><td>0.228</td><td>-0.204</td><td>0.853</td></tr><tr><th>address_U</th><td>0.3521</td><td>0.252</td><td>1.397</td><td>0.163</td><td>-0.143</td><td>0.847</td></tr><tr><th>reason_reputation</th><td>0.4602</td><td>0.270</td><td>1.704</td><td>0.089</td><td>-0.070</td><td>0.990</td></tr><tr><th>Walc</th><td>-0.1570</td><td>0.108</td><td>-1.455</td><td>0.146</td><td>-0.369</td><td>0.055</td></tr><tr><th>Dalc</th><td>-0.3119</td><td>0.149</td><td>-2.098</td><td>0.036</td><td>-0.604</td><td>-0.020</td></tr><tr><th>Fedu</th><td>0.1503</td><td>0.129</td><td>1.169</td><td>0.243</td><td>-0.102</td><td>0.403</td></tr><tr><th>Medu</th><td>0.0980</td><td>0.137</td><td>0.718</td><td>0.473</td><td>-0.170</td><td>0.366</td></tr><tr><th>studytime</th><td>0.4366</td><td>0.137</td><td>3.192</td><td>0.001</td><td>0.168</td><td>0.705</td></tr><tr><th>school_MS</th><td>-1.0299</td><td>0.252</td><td>-4.087</td><td>0.000</td><td>-1.525</td><td>-0.535</td></tr><tr><th>higher_yes</th><td>1.6627</td><td>0.376</td><td>4.421</td><td>0.000</td><td>0.924</td><td>2.401</td></tr><tr><th>failures</th><td>-1.4374</td><td>0.193</td><td>-7.449</td><td>0.000</td><td>-1.816</td><td>-1.058</td></tr></tbody></table><p>Insights for the portuguese data set linear regression model:</p><ul><li>Our model explains explains 30.5% of the inputs into the final grade (<code>G3</code>), better than the math model but still leaving room for improvement.</li><li>We again see that the desire to go into higher education and the number of previous failures are highly influential when determining a student’s final grade.</li><li>There are other statistically significant coefficients such as <code>school_MS</code>, <code>failures</code> and <code>studytime</code>.</li><li>In fact, the influence of going to Mousinho da Silveira is strong, with an expected decrease in one mark in a student’s Portuguese grade</li></ul><h2 id="conclusion"><a class="markdownIt-Anchor" href="#conclusion"></a> Conclusion</h2><p>We have seen that many factors can influence your final grades, the strongest of which typically being socio-economic characteristics (address, parent’s education, family relationship, etc.) that cannot be changed. Those factors can also depend on the potential biases of the dataset. For example, maybe the mother’s unemployment status has a bigger cultural impact on Portuguese student than on UK students. However, some variables that are controllable by the student such as <code>studytime</code>, going out (<code>goout</code>), consumption of alcohol (<code>Dalc</code> and <code>Walc</code>) and potentially relationship status (<code>romantic</code>) have been proved to have an impact on the final grade (<code>G3</code>) of students in these datasets.</p><p>Although valuable insights have been gleaned from this dataset it is clear from our poorly fitting regression model that linear interactions alone are insufficient for capturing a system as complicated as a student’s school performance. If a purely performative model is what we desired, then moving towards a tree-based model or including carefully chosen interaction terms would be advised.</p>]]></content>
      
      
      <categories>
          
          <category> Social Sciences </category>
          
          <category> Education </category>
          
      </categories>
      
      
        <tags>
            
            <tag> visualization </tag>
            
            <tag> data-analysis </tag>
            
            <tag> regression </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Gibrat&#39;s Law: The Central Limit Theorem&#39;s Forgotten Twin</title>
      <link href="/gibrats-law/"/>
      <url>/gibrats-law/</url>
      
        <content type="html"><![CDATA[<div class="note info"><p><strong>Accessing Post Source</strong><br>We are still working on getting this site set up, so source code for this post is not yet available. Check back soon and you’ll be able to find it linked here.</p></div><p>Testing, testing. One, two, three. Can everyone hear me alright?</p><p>Wonderful. Hello and welcome to the Warwick Data Science Society Research Blog. It’s so nice to see you.</p><h2 id="introduction"><a class="markdownIt-Anchor" href="#introduction"></a> Introduction</h2><h3 id="testing-and-tediosity"><a class="markdownIt-Anchor" href="#testing-and-tediosity"></a> Testing and Tediosity</h3><p>Testing is boring. Everyone knows that they <em>should</em> test the solutions they produce, yet reality often fails to live up to this ideal. We check a few obvious things, run through the logic in our head, and convince ourselves that nothing could ever go wrong…and then it does just that.</p><p>Sometimes this is okay. For small, individual projects, it’s not the end of the world if your code base comes collapsing down on you; a few hours of bodging and everything should be stable again (all be it with code now so messy that you’d want to consider cryongenics). For larger projects, and especially those of collaborative nature, this just won’t do.</p><p>That is a long way to say, this post is a test, but hopefully not a boring one. To ensure that the testing of this site is performed thoroughly, I decided to write this post to ensure that everything is behaving as expected. I hope it can also act as a template for other keen researchers—whether they are currently a member of WDSS or merely interested in getting involved—to contribute their own work.</p><p>The philosophy of this post is to write about a topic that is actually (well, hopefully) of some interest to people, and do so in a natural way. None of this testing through enumerating corner-cases; let’s use the site for what it’s made for in practice and share some interesting insights along the way.</p><p>With that in mind, I wish to introduce to you: Gibrat’s law. Don’t be disheartened if you haven’t come across this term before,—most haven’t. This is likely because the rule is overshadowed by it’s far more infamous twin, the central limit theorem. If you’ve not come across this second term either, don’t fret. You may want to watch <a href="https://www.youtube.com/watch?v=JNm3M9cqWyc" target="_blank" rel="noopener">this short video</a> by <a href="https://www.khanacademy.org/" target="_blank" rel="noopener">Khan Academy</a> as a preliminary, but we’ll introduce both of these ideas either way.</p><div class="note info"><p>As a matter of fact, the <a href="https://en.wikipedia.org/wiki/Gibrat%27s_law" target="_blank" rel="noopener">Wikipedia article for Gibrat’s Law</a> is only made up of a few paragraphs with the link to the law’s creator, Robert Gibrat, directing you to a yet non-existant page. We seem to be diving head first into the rabbit hole today.</p></div><h2 id="background"><a class="markdownIt-Anchor" href="#background"></a> Background</h2><div class="note warning"><p>If you are already familiar with central limit theorem, you may wish to skip to the <a href="#from-sums-to-products">next section</a>.</p></div><h3 id="the-central-limit-theorem"><a class="markdownIt-Anchor" href="#the-central-limit-theorem"></a> The Central Limit Theorem</h3><p>We’ll start with the central limit theorem. There are many ways of explaining or defining this phenomenon, each striking a subtle balance between statistical rigor and clarity of explanation. We will favor the latter, ensuring to remember that we should keep it too ourselves if we wish to avoid the disapproving gaze of the pure probability theorists.</p><p>This simplified description goes as follows:</p><ul><li>Consider a sequence of independent random variables</li><li>Take the sum of these values</li><li>Regardless of whether the original variables were normally distributed, the distribution of this sum will tend towards a <a href="https://www.mathsisfun.com/data/standard-normal-distribution.html" target="_blank" rel="noopener">normal distribution</a> as the number of variables increases</li></ul><p>To clarify this notion, let’s look an example. One of the simplest would be an experiment involving tossing multiple fair coins. We consider each coin flip to be an independent random variable taking value zero if the result is tails, and likewise one for heads. We can then say that the total number of heads is the sum of these random variables. It would follow from the central limit theorem that the total number of heads should therefore tend to a normal distribution as the number of total coin tosses gets large. Let’s simulate this experiment for different numbers of coins and see this process in action.</p><p><img src="/" class="lazyload" data-src="/images/gibrats-law/gibrats-law_18_0.png"  alt=""></p><p>Just as we expected, the more coins we toss, the closer the resulting distribution of heads resembles the classic bell shape of the normal distribution. Indeed, once we use around one hundred or more tosses, we can almost model the number of heads as a continuous variable.</p><h3 id="the-importance-of-the-clt"><a class="markdownIt-Anchor" href="#the-importance-of-the-clt"></a> The Importance of the CLT</h3><p>It’s all well and good that the theory works, but why should we care?</p><p>The real power of the central limit theorem presents itself when we want to perform statistical analysis or tests on unfriendly distributions. The normal distribution has some incredibly delightful properties and has been studied in great extent, so working with it is often straightforward. On the other hand, there are many distributions which are not so cooperative, and that’s even assuming we know the distribution of whatever we are trying to model. Thankfully, the central limit theorem allows us to circumvent these issues by assuring us that as long as our process can be modeled as the sum of independent random variables (which is a surprisingly common property), we can approximate its distribution as normal and apply all of the standard techniques we know and love.</p><p>For example, the number of visitors to a website in a given time period is unlikely to have an underlying normal distribution governing the process. This could make it difficult to analyze data related to this, however the central limit theorem offers a workaround. The rough conceptual notion behind this approach is to divide our time period up into smaller and smaller intervals and consider how many people visit the website in each of those. We can assume that these are reasonably independent and so our total visitor count becomes the sum of many independent random variables and so the central limit theorem holds. A small caveat of this specific result is that we require the number of visitors to the site in the time period to be reasonably large (&gt;20 visitors is typically fine), but as long as this holds, we can perform analysis just as if our data was normally distributed.</p><div class="note success"><p>We can even go one step further. Since translating or scaling a normal distribution does not change its normality, it is possible to approximate many situations using the standard normal distrubtion (mean 0, variance 1) by applying appropriate transformations. This makes our lives even simpler and offers some elegance in the process.</p></div><h2 id="from-sums-to-products"><a class="markdownIt-Anchor" href="#from-sums-to-products"></a> From Sums to Products</h2><p>This leads us nicely onto Gibrat’s law. In our rough definition of the central limit theorem above, we described taking numerous independent random variables and computing their sum. Gibrat’s law begins in a similar vein but goes on to consider the product of these values instead. Because of this alteration, we can no longer expect the aggregation to approach a normal distribution as the number of variables grows large. Instead we tend towards a related distribution—the log-normal distribution.</p><p>As the name eludes to, a random variable following log-normal distribution can be defined by its logarithm being normally distributed. The converse of this framing is that whenever we take a normal random variable and take its exponential, the result will follow a log-normal distribution. Because of this, a log-normal random variable only takes strictly positive values with a density curve along the lines of the following.</p><p><img src="/" class="lazyload" data-src="/images/gibrats-law/gibrats-law_28_0.png"  alt=""></p><p>Just as with our coin-tossing experiment for the central limit theorem, we can validate our belief in Gibrat’s law using another simulation. We’ll introduce an explicit example of Gibrat’s law in the final section of the post and for now look an a more esoteric example. In particular, we will consider numerous independent random variables, uniformly-distributed on the interval <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">(</mo><mn>0.5</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(0.5, 1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">(</span><span class="mord">0</span><span class="mord">.</span><span class="mord">5</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span>. We then take the product of the first <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathdefault">n</span></span></span></span> these variables and look at the probability distribution of this value. Using Gibrat’s law we would expect this product to approach a log-normal as <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathdefault">n</span></span></span></span> becomes large, with a density tending in shape towards the curves shown above. A few simulations shows exactly that.</p><p><img src="/" class="lazyload" data-src="/images/gibrats-law/gibrats-law_30_0.png"  alt=""></p><div class="note success"><p>The example shown here is, admittedly, rather abstract. Despite this, it is still simple to frame it in terms of a real world problem (though perhaps not one of upmost importance). For example, we could imagine the product of independent uniform random variables modeling the process of repeatedly cutting a piece of string at a randomly chosen point along its length and retaining the longer part.</p></div><p>It is worth noting that just as there is no one normal distribution, but rather many of various shapes and sizes, parameterized by their mean (<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>μ</mi></mrow><annotation encoding="application/x-tex">\mu</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathdefault">μ</span></span></span></span>) and variance (<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>σ</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\sigma^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8141079999999999em;vertical-align:0"></span><span class="mord"><span class="mord mathdefault" style="margin-right:.03588em">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8141079999999999em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>), the same is true for the log-normal distribution. In fact, we parameterize a log-normal distribution not by its own mean and variance, but by the mean and variance of the resulting normal distribution we obtain when taking logarithms.</p><p>This relationship between the two distributions offers us much of the same power the central limit theorem displays for Gibrat’s law too. Yes, taking the product of the independent random variables in its definition doesn’t give us the normal distribution we so desire directly, but with a simple logarithmic transformation we can get there. Further, because taking the logarithm of a random variable is a simple transformation with some desirable properties, we are able to utilize many of the tools we have developed for manipulating and analyzing the normal distribution with log-normal distribution. This in turn makes Gibrat’s law an incredibly powerful tool for simplifying statistical analysis. One question remains though: when does it hold? What sort of scenarios can be modeled as a product of independent random variables? This brings us neatly to the final section of this post in which we will look into just that, and confirm Gibrat’s law in action.</p><h2 id="verifying-the-law"><a class="markdownIt-Anchor" href="#verifying-the-law"></a> Verifying the Law</h2><p>So far we’ve discussed some interesting ideas, but I think it’s time to pull things back from the land of statistical theory into the real world. Where would we expect to find Gibrat’s law in our lives?</p><p>The key insight needed to answer this question is to understand the underlying mechanism of Gibrat’s law; it’s all above the multiplication of random variables. Specifically, we take the previous product and multiply it by some new random amount. Another way of looking at this is that these random variables represent the growth rate of some quantity. Now, this is not a fixed growth rate as you would have say with interest rates or nuclear decay, but rather a stochastic growth rate.</p><p>To simplify this notion, all we are looking for is systems that evolve by a growth rate proportional to their current size with some added variation captured by the random nature of the variables. An example of such a system (and incidentally the original inspiration for the law) is that of modeling the growth of a firm. In a simple model, we can ignore the impacts of overarching market changes and catastrophic economic events and instead suggest that the growth of a company is roughly proportional to its current size with some added noise. Obviously, this model is primitive and its assumptions do not hold exactly, but it is close enough to believe that the distribution of the size of firms would indeed be log-normally distributed.</p><p>Another example could be the distribution of the population sizes for various countries or cities. There are clear troubles with modeling such a system using Gibrat’s law, but overall it is not unreasonable to suggest that the population growth of a city is largely dependent on its current size, with some added stochasticity. Don’t take my word for it though! Instead, let’s quickly gather some data to verify this result for ourselves.</p><h3 id="sourcing-data"><a class="markdownIt-Anchor" href="#sourcing-data"></a> Sourcing Data</h3><p>After some searching, I decided that the best data source to investigate the validity of Gibrat’s law for use with population data is <a href="https://en.wikipedia.org/wiki/List_of_cities_in_the_United_Kingdom" target="_blank" rel="noopener">this Wikipedia article</a> containing the populations of all sovereign states and dependencies. I then scraped relevant columns from the main table of this page, resulting in a dataset for which ten randomly chosen rows are shown below.</p><table><thead><tr><th></th><th>City</th><th>Population</th></tr></thead><tbody><tr><th>5</th><td>Manchester</td><td>503127</td></tr><tr><th>24</th><td>Southampton</td><td>236882</td></tr><tr><th>28</th><td>York</td><td>198051</td></tr><tr><th>31</th><td>Chelmsford</td><td>168310</td></tr><tr><th>32</th><td>Dundee</td><td>153990</td></tr><tr><th>51</th><td>Bath</td><td>88859</td></tr><tr><th>53</th><td>Hereford</td><td>58896</td></tr><tr><th>56</th><td>Stirling</td><td>34790</td></tr><tr><th>57</th><td>Lichfield</td><td>32219</td></tr><tr><th>66</th><td>London</td><td>7375</td></tr></tbody></table><h3 id="visualizing-the-results"><a class="markdownIt-Anchor" href="#visualizing-the-results"></a> Visualizing the Results</h3><p>We can now plot a histogram of the populations for these cities. On top of this, we overlay a log-normal distribution with parameters chosen to best fit (using maximum likelihood estimation) to see how well the densities match.</p><p><img src="/" class="lazyload" data-src="/images/gibrats-law/gibrats-law_44_0.png"  alt=""></p><p>Now, before you say anything, I know. It’s not perfect. But, I hope we can agree that there is definitely <em>something</em> there. It’s also quite easy to explain why the match isn’t exact. For a start, we only had data for 69 cities so it’s no surprise that the histogram is so jagged. On top of this, city populations is one of the more difficult applications of Gibrat’s law due to the many factors that influence their size that aim to violate the assumptions of the rule.</p><p>Despite these shortcomings, it is clear that Gibrat’s law has value to it either as a conceptual or statistical tool. It should certainly be the case that the validity of the law in any particular scenario should be tested thoroughly before resting too heavy on the results, but as a tool to guide you in the right direction, it is is invaluable.</p><p>As eluded to above, the example of population data is more difficult than most applications of Gibrat’s law due to the numerous and influential externalities. I didn’t want to shy away from this case, especially when it is an example that has clear real-world implications for modeling. It therefore follows that in many more simplistic and controlled cases, Gibrat’s law shines even brighter. For example, it has been of great benefit in my work at AstraZeneca in forming suitable priors for energy distributions in statistical models. Without knowledge of this law it may have taken me more time to discover that the log-normal distribution was a natural and accurate model for these quanta.</p><p>If anything, this post does not offer anything of immediate practical use. That is not to say however that there isn’t an important message. That is, remember Gibrat’s law—it’s there more than you think, and your awareness of it is vital for optimum efficiency in your statistical work. I hope you found this post of some insight, and I look forward to sharing more ideas and research as this blog developments.</p>]]></content>
      
      
      <categories>
          
          <category> Meta </category>
          
          <category> Mathematics </category>
          
          <category> Statistics </category>
          
      </categories>
      
      
        <tags>
            
            <tag> lesson </tag>
            
            <tag> normal-distribution </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
