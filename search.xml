<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Quantifying Anonymity: The Science of Staying Private</title>
      <link href="/quantifying-anonymity/"/>
      <url>/quantifying-anonymity/</url>
      
        <content type="html"><![CDATA[<div class="note info"><p><strong>Accessing Post Source</strong><br>We are still working on getting this site set up, so source code for this post is not yet available. Check back soon and you’ll be able to find it linked here.</p></div><p>Understanding and respecting data privacy is an increasingly important responsibility, which all data controllers and processors must take when handling personal information. But what do we mean when we talk about data privacy? And how can we go about measuring something as abstract and conceptual as anonymity?</p><p>In this article, we investigate a recently proposed statistical metric used to measure the privacy of trajectory datasets. We consider the capabilities and limitations of the method and illustrate the process of measuring privacy using a publicly available dataset of mobile spatio-temporal data.</p><div class="note info"><p>Trajectory data records the locations of moving objects at specific times and is a critical resource for studying human behaviour, environmental changes, and traffic systems.</p></div><h2 id="what-is-privacy"><a class="markdownIt-Anchor" href="#what-is-privacy"></a> What is Privacy?</h2><p>Before we go about measuring it, it’s a good idea to have a firm understanding of exactly what we mean when we talk about privacy in a data scientific context. We usually consider privacy to be the extent to which an individual is able to limit or control access to their personal information. However, data privacy is a little bit more general; it refers to the protections or safeguards against the potential misuse of an individual’s data with reference to their rights and consent.</p><h3 id="privacy-vs-security"><a class="markdownIt-Anchor" href="#privacy-vs-security"></a> Privacy vs Security</h3><p>Whereas data security places its focus on protecting an individual’s data by implementing techniques such as authentication, encryption and access control, data privacy is more concerned with mitigating against the risk that the data you provide to others, either knowingly or unknowingly, is used for nefarious purposes. This risk can manifest anywhere from tracking your location in real-time to showing you targeted adverts without your consent; both can represent a breach of your individual privacy.</p><p>Privacy is not necessarily the same as security; in fact, it is often the case that people are asked to decrease their standards for individual privacy to safeguard a society’s safety and security in the face of crime and terrorism. Similarly, privacy campaigners and activists argue that a decrease in security is an acceptable risk to maintain every individual’s right to privacy and civil liberties.</p><div class="note warning"><p>The line between privacy and security is often blurred and is frequently contested by academics and industry professionals alike. Whatever your opinion on the matter, it is universally acknowledged that an understanding of both concepts is a critical asset to anybody working in data science.</p></div><h3 id="hiding-in-plain-sight"><a class="markdownIt-Anchor" href="#hiding-in-plain-sight"></a> Hiding in Plain Sight</h3><p>Similarly to how data security experts have access to a wide range of tools such as encryption and authentication, data privacy experts have their own arsenal of techniques on hand to help mitigate privacy risks. One of the most common techniques used to enhance data privacy is anonymisation, a process by which an individual’s data can be published in a dataset, without revealing their identity or other sensitive information.</p><p>Let’s consider a data schema that might reasonably be collated by the MyWarwick application. It contains personal information about each student (their name, date of birth and university ID), the location of the student collected through the application (remember, MyWarwick requests location permissions to improve its campus map) and each student’s mark for a recent Tabula assignment.</p><table><thead><tr><th style="text-align:center">Full Name</th><th style="text-align:center">Date of Birth</th><th style="text-align:center">University ID</th><th style="text-align:center">Location</th><th style="text-align:center">Mark</th></tr></thead><tbody><tr><td style="text-align:center">Hugh Heath</td><td style="text-align:center">1998-02-21</td><td style="text-align:center"><code>u1602937</code></td><td style="text-align:center"><code>52.379925, -1.557104</code></td><td style="text-align:center">54%</td></tr><tr><td style="text-align:center">Nyla Idris</td><td style="text-align:center">1998-06-17</td><td style="text-align:center"><code>u1672635</code></td><td style="text-align:center"><code>52.282188, -1.531144</code></td><td style="text-align:center">95%</td></tr><tr><td style="text-align:center">Rosa Coates</td><td style="text-align:center">1998-03-15</td><td style="text-align:center"><code>u1646945</code></td><td style="text-align:center"><code>52.384243, -1.559346</code></td><td style="text-align:center">68%</td></tr><tr><td style="text-align:center"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">⋮</mi><mpadded height="+0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mrow><annotation encoding="application/x-tex">\vdots</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.53em;vertical-align:-.03em"></span><span class="mord"><span class="mord">⋮</span><span class="mord rule" style="border-right-width:0;border-top-width:1.5em;bottom:0"></span></span></span></span></span></td><td style="text-align:center"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">⋮</mi><mpadded height="+0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mrow><annotation encoding="application/x-tex">\vdots</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.53em;vertical-align:-.03em"></span><span class="mord"><span class="mord">⋮</span><span class="mord rule" style="border-right-width:0;border-top-width:1.5em;bottom:0"></span></span></span></span></span></td><td style="text-align:center"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">⋮</mi><mpadded height="+0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mrow><annotation encoding="application/x-tex">\vdots</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.53em;vertical-align:-.03em"></span><span class="mord"><span class="mord">⋮</span><span class="mord rule" style="border-right-width:0;border-top-width:1.5em;bottom:0"></span></span></span></span></span></td><td style="text-align:center"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">⋮</mi><mpadded height="+0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mrow><annotation encoding="application/x-tex">\vdots</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.53em;vertical-align:-.03em"></span><span class="mord"><span class="mord">⋮</span><span class="mord rule" style="border-right-width:0;border-top-width:1.5em;bottom:0"></span></span></span></span></span></td><td style="text-align:center"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">⋮</mi><mpadded height="+0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mrow><annotation encoding="application/x-tex">\vdots</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.53em;vertical-align:-.03em"></span><span class="mord"><span class="mord">⋮</span><span class="mord rule" style="border-right-width:0;border-top-width:1.5em;bottom:0"></span></span></span></span></span></td></tr></tbody></table><p>MyWarwick wants to outsource this information to a fictional third party data processor, Warwick Analytica, with the brief of identifying whether there is a correlation between student location and the mark they achieved in the assignment. However, before they can release the data, they are required to anonymise it.</p><p>The simplest form of anonymisation is the redaction of personally identifiable information—data which is very likely, if not guaranteed, to be unique to a single individual. Examples might include your full name and date of birth, your National Insurance number or university ID. The inclusion of personally identifiable information in a dataset significantly increases the likelihood and severity of a potential privacy attack, so it is often completely removed (data sanitisation) or replaced with an artificial identifying value (data pseudonymisation) instead.</p><table><thead><tr><th style="text-align:center">Pseudonym</th><th style="text-align:center">Location</th><th style="text-align:center"><strong>Mark</strong></th></tr></thead><tbody><tr><td style="text-align:center"><code>1d36cc</code></td><td style="text-align:center"><code>52.379925, -1.557104</code></td><td style="text-align:center">54%</td></tr><tr><td style="text-align:center"><code>e1f999</code></td><td style="text-align:center"><code>52.282188, -1.531144</code></td><td style="text-align:center">95%</td></tr><tr><td style="text-align:center"><code>c60764</code></td><td style="text-align:center"><code>52.384243, -1.559346</code></td><td style="text-align:center">68%</td></tr><tr><td style="text-align:center"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">⋮</mi><mpadded height="+0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mrow><annotation encoding="application/x-tex">\vdots</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.53em;vertical-align:-.03em"></span><span class="mord"><span class="mord">⋮</span><span class="mord rule" style="border-right-width:0;border-top-width:1.5em;bottom:0"></span></span></span></span></span></td><td style="text-align:center"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">⋮</mi><mpadded height="+0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mrow><annotation encoding="application/x-tex">\vdots</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.53em;vertical-align:-.03em"></span><span class="mord"><span class="mord">⋮</span><span class="mord rule" style="border-right-width:0;border-top-width:1.5em;bottom:0"></span></span></span></span></span></td><td style="text-align:center"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">⋮</mi><mpadded height="+0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mrow><annotation encoding="application/x-tex">\vdots</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.53em;vertical-align:-.03em"></span><span class="mord"><span class="mord">⋮</span><span class="mord rule" style="border-right-width:0;border-top-width:1.5em;bottom:0"></span></span></span></span></span></td></tr></tbody></table><p>This way, it’s a lot harder for an observer to learn sensitive information about a target individual from the data—but it’s not impossible.</p><h3 id="background-knowledge"><a class="markdownIt-Anchor" href="#background-knowledge"></a> Background Knowledge</h3><p>Although this process successfully guarantees anonymity in isolation (i.e. when we have no extra information outside of the data), with additional background knowledge, it’s still possible to perform a privacy attack on users in the dataset.</p><p>Let’s say you know that your friend Hugh, who completed the assignment, is currently at home in Jack Martin 2 (<code>52.379925, -1.557104</code>). By linking this information with MyWarwick’s ‘anonymised’ data, we can isolate Hugh’s mark of 54% as well as his pseudonym (which could be used to identify him in other datasets)—all without his consent.</p><p>Similarly, if Nyla was the student in the dataset who achieved a mark of 95% and later posted about her success on a public LinkedIn or Instagram page, this data could still be used to isolate her current location (<code>52.282188, -1.531144</code>—most likely a well deserved celebratory coffee at Procaffeinate) and her pseudonym.</p><p>These example privacy breaches are extreme and arguably unlikely; however, it demonstrates that anonymised data rarely exists in isolation. Background knowledge for a dataset will always exist in some form, and each piece of background information constitutes a privacy risk to the anonymised dataset.</p><p>For that reason, in order to limit the effectiveness of reidentification attacks, we often employ more rigorous methods of guaranteeing the privacy and anonymity of data than just data sanitisation or pseudonymisation.</p><div class="note success"><p>More information about these techniques, including <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathdefault" style="margin-right:.03148em">k</span></span></span></span>-anonymity, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi></mrow><annotation encoding="application/x-tex">l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathdefault" style="margin-right:.01968em">l</span></span></span></span>-diversity and differential privacy, can be found <a href="https://arx.deidentifier.org/overview/privacy-criteria/" target="_blank" rel="noopener">here</a>.</p></div><h2 id="how-is-privacy-measured"><a class="markdownIt-Anchor" href="#how-is-privacy-measured"></a> How is Privacy Measured?</h2><p>Privacy is a difficult quality to measure for a dataset, principally because it’s difficult to formally define. Unlike other statistical properties of data, there is no universally agreed formula or procedure to measure privacy. As a result, it’s difficult for data collectors to communicate the extent to which user data is anonymised or handled in a privacy-preserving manner.</p><p>Before we can explore a recently proposed metric used to measure the risk of reidentification in a dataset, we need to make a few tweaks to a well-known children’s board game…</p><h3 id="guess-who"><a class="markdownIt-Anchor" href="#guess-who"></a> Guess Who?</h3><p>In the popular board game <em><a href="https://en.wikipedia.org/wiki/Guess_Who%3F" target="_blank" rel="noopener">Guess Who?</a></em>, players take turns requesting and revealing information about a face on their respective, identical boards. Each player selects, or is dealt at random, an identity relating to a face on the board; to win, they must deduce their opponent’s identity before their opponent can do the same.</p><p>Each turn, the requester asks the revealer to disclose a feature about their selected face, to which the revealer responds honestly. From this information, the requester may be able to reduce their pool of potential secret identities the revealer may have adopted. The roles of requester and revealer then alternate until a player has successfully managed to isolate a single face belonging to their opponent (or is confident enough to make a guess at random), at which point the game ends.</p><p>Although the game works perfectly well with its original ruleset, for this data privacy scenario we’re going to make a few tweaks to make the game more interesting from a statistical perspective:</p><ul><li>The roles of requester and revealer are fixed, and only the revealer has a secret face and identity.</li><li>The revealer’s face is selected at random with a uniform probability distribution.</li><li>The requester cannot guess at random. They must completely isolate a single face to win.</li><li>The number of questions the requester can ask is limited by a number <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">p &gt; 0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.7335400000000001em;vertical-align:-.19444em"></span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">0</span></span></span></span>. The requester wins if they are able to deduce the revealer’s face and identity within this limit; otherwise, the revealer wins.</li><li>The requester is not restricted to yes/no questions, but rather uses questions that completely identify a feature, such as, “What eye colour does your face have?”.</li></ul><p>Now that we’ve defined the rules, we can do what data scientists do best: analyse the game so that we can win more often!</p><h3 id="running-the-numbers"><a class="markdownIt-Anchor" href="#running-the-numbers"></a> Running the Numbers</h3><p>The difficulty of the game from the requester’s perspective is effectively determined by the number of questions available, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathdefault">p</span></span></span></span>, negotiated by the players before the start of the game. Therefore, we want to be able to calculate the probability of us winning (as the requester) with respect to a value of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathdefault">p</span></span></span></span>:</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>Win </mtext><mi mathvariant="normal">∣</mi><mtext> </mtext><mi>p</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(\text{Win}\ |\ p)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathdefault" style="margin-right:.13889em">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">Win</span></span><span class="mspace"> </span><span class="mord">∣</span><span class="mspace"> </span><span class="mord mathdefault">p</span><span class="mclose">)</span></span></span></span></span></p><p>We can start by breaking our calculation down into a smaller subproblem: given an arbitrary face, what is the probability of winning with <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathdefault">p</span></span></span></span> questions if this is the correct face? To calculate this probability, we need to consider the features of the face that we could confirm through a question to the revealer and the number of other faces on the board that share those same features.</p><table><thead><tr><th style="text-align:center">Identity</th><th style="text-align:center">Hair Colour</th><th style="text-align:center">Eye Colour</th><th style="text-align:center">Has Hat</th></tr></thead><tbody><tr><td style="text-align:center">Alice</td><td style="text-align:center">Black</td><td style="text-align:center">Green</td><td style="text-align:center">False</td></tr><tr><td style="text-align:center">Bob</td><td style="text-align:center">Brown</td><td style="text-align:center">Blue</td><td style="text-align:center">False</td></tr><tr><td style="text-align:center">Charlie</td><td style="text-align:center">Black</td><td style="text-align:center">Brown</td><td style="text-align:center">True</td></tr><tr><td style="text-align:center"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">⋮</mi><mpadded height="+0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mrow><annotation encoding="application/x-tex">\vdots</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.53em;vertical-align:-.03em"></span><span class="mord"><span class="mord">⋮</span><span class="mord rule" style="border-right-width:0;border-top-width:1.5em;bottom:0"></span></span></span></span></span></td><td style="text-align:center"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">⋮</mi><mpadded height="+0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mrow><annotation encoding="application/x-tex">\vdots</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.53em;vertical-align:-.03em"></span><span class="mord"><span class="mord">⋮</span><span class="mord rule" style="border-right-width:0;border-top-width:1.5em;bottom:0"></span></span></span></span></span></td><td style="text-align:center"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">⋮</mi><mpadded height="+0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mrow><annotation encoding="application/x-tex">\vdots</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.53em;vertical-align:-.03em"></span><span class="mord"><span class="mord">⋮</span><span class="mord rule" style="border-right-width:0;border-top-width:1.5em;bottom:0"></span></span></span></span></span></td><td style="text-align:center"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">⋮</mi><mpadded height="+0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mrow><annotation encoding="application/x-tex">\vdots</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.53em;vertical-align:-.03em"></span><span class="mord"><span class="mord">⋮</span><span class="mord rule" style="border-right-width:0;border-top-width:1.5em;bottom:0"></span></span></span></span></span></td></tr></tbody></table><p>For example, let’s say we select Alice as our arbitrary face and we only have one question that we can ask (<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">p = 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span></span></span></span>). Alice has black hair, green eyes and isn’t wearing a hat. Given that we can only confirm one of these features, we need to consider the number of faces on the board that match with Alice’s feature:</p><table><thead><tr><th style="text-align:center">Feature</th><th style="text-align:center">Alice</th><th style="text-align:center">Number of matching faces</th></tr></thead><tbody><tr><td style="text-align:center">Hair Colour</td><td style="text-align:center">Black</td><td style="text-align:center">4</td></tr><tr><td style="text-align:center">Eye Colour</td><td style="text-align:center">Green</td><td style="text-align:center">2</td></tr><tr><td style="text-align:center">Has Hat</td><td style="text-align:center">False</td><td style="text-align:center">10</td></tr></tbody></table><p>Unfortunately, this information suggests that it would be impossible to win if Alice is the secret face. Even if we asked about her most distinguishing feature, her eye colour, we would still be left with one other matching face and would lose the game. Therefore, our chance of winning is <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">0</span></span></span></span> under these parameters.</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>Win </mtext><mi mathvariant="normal">∣</mi><mtext> </mtext><mi>p</mi><mo>=</mo><mn>1</mn><mo separator="true">,</mo><mtext> Secret Face is Alice</mtext><mo stretchy="false">)</mo><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">P(\text{Win}\ |\ p = 1,\ \text{Secret Face is Alice}) = 0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathdefault" style="margin-right:.13889em">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">Win</span></span><span class="mspace"> </span><span class="mord">∣</span><span class="mspace"> </span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mspace"> </span><span class="mord text"><span class="mord">Secret Face is Alice</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">0</span></span></span></span></span></p><p>However, if we’re allowed two questions instead (<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">p = 2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">2</span></span></span></span>), the game changes completely. Now we can ask about any two of Alice’s features, which we can take from the possible subsets of features of size <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>2</mn></mrow><annotation encoding="application/x-tex">2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">2</span></span></span></span>.</p><table><thead><tr><th style="text-align:center">Subset of features</th><th style="text-align:center">Alice</th><th style="text-align:center">Number of matching faces</th></tr></thead><tbody><tr><td style="text-align:center">{Hair Colour, Eye Colour}</td><td style="text-align:center">{Black, Green}</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">{Hair Colour, Has Hat}</td><td style="text-align:center">{Black, False}</td><td style="text-align:center">3</td></tr><tr><td style="text-align:center">{Eye Colour, Has Hat}</td><td style="text-align:center">{Green, False}</td><td style="text-align:center">1</td></tr></tbody></table><p>This information suggests that our chances of winning with <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">p = 2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">2</span></span></span></span> are a lot higher than for <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">p = 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span></span></span></span>, because Alice is the only face remaining on the board if we filter faces by her hair colour and eye colour or eye colour and the presence of a hat, irrespective of order. Assuming that we choose our features to reveal at random from a uniform distribution, we can calculate our chance of winning for <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">p = 2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">2</span></span></span></span> as <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mn>2</mn><mn>3</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{2}{3}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-.345em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.845108em"><span style="top:-2.6550000000000002em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> from the table above, if Alice is the secret face. Those odds are pretty good and are certainly a lot better than our <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>0</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">0\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.80556em;vertical-align:-.05556em"></span><span class="mord">0</span><span class="mord">%</span></span></span></span> chance of winning for <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">p = 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span></span></span></span>.</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>Win </mtext><mi mathvariant="normal">∣</mi><mtext> </mtext><mi>p</mi><mo>=</mo><mn>2</mn><mo separator="true">,</mo><mtext> Secret Face is Alice</mtext><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>2</mn><mn>3</mn></mfrac></mrow><annotation encoding="application/x-tex">P(\text{Win}\ |\ p = 2,\ \text{Secret Face is Alice}) = \frac{2}{3}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathdefault" style="margin-right:.13889em">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">Win</span></span><span class="mspace"> </span><span class="mord">∣</span><span class="mspace"> </span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mspace"> </span><span class="mord text"><span class="mord">Secret Face is Alice</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:2.00744em;vertical-align:-.686em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">3</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.686em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p><p>Clearly, we can’t take this probability to be our overall chance of winning because it relies on the assumption that Alice is the revealer’s secret face. However, we can use the fact that the secret face is chosen randomly using a uniform distribution to find an average chance of winning across all possible faces.</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>Win </mtext><mi mathvariant="normal">∣</mi><mtext> </mtext><mi>p</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><munder><mo>∑</mo><mrow><mi>f</mi><mo>∈</mo><mtext>Faces</mtext></mrow></munder><mi>P</mi><mo stretchy="false">(</mo><mtext>Win </mtext><mi mathvariant="normal">∣</mi><mtext> </mtext><mi>p</mi><mo separator="true">,</mo><mtext> Secret Face is </mtext><mi>f</mi><mo stretchy="false">)</mo></mrow><mtext>Number of Faces</mtext></mfrac></mrow><annotation encoding="application/x-tex">P(\text{Win}\ |\ p) = \frac{\sum_{f \in \text{Faces}} P(\text{Win}\ |\ p,\ \text{Secret Face is }f)}{\text{Number of Faces}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathdefault" style="margin-right:.13889em">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">Win</span></span><span class="mspace"> </span><span class="mord">∣</span><span class="mspace"> </span><span class="mord mathdefault">p</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:2.261818em;vertical-align:-.686em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.575818em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord text"><span class="mord">Number of Faces</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.825818em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-.0000050000000000050004em">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.18639799999999984em"><span style="top:-2.40029em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:.10764em">f</span><span class="mrel mtight">∈</span><span class="mord text mtight"><span class="mord mtight">Faces</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.43581800000000004em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathdefault" style="margin-right:.13889em">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">Win</span></span><span class="mspace"> </span><span class="mord">∣</span><span class="mspace"> </span><span class="mord mathdefault">p</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mspace"> </span><span class="mord text"><span class="mord">Secret Face is </span></span><span class="mord mathdefault" style="margin-right:.10764em">f</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.686em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p><p>From this, we can reasonably estimate our chance of winning a modified game of <em>Guess Who?</em> given a value of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathdefault">p</span></span></span></span>. But what happens if we abstract this concept further? Can we apply this approach of quantifying the reidentifiability of faces in a board game to more general data sets? The answer is yes, and it’s already being explored through a privacy measure called <em>unicity</em>.</p><h3 id="understanding-unicity"><a class="markdownIt-Anchor" href="#understanding-unicity"></a> Understanding Unicity</h3><p>Unicity is a privacy metric that measures the risk that individuals in a dataset will be reidentified following the disclosure of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathdefault">p</span></span></span></span> data points. In other words, it quantifies the risk of reidentification for users in a dataset, assuming that an adversary (the person who wants to breach an individual’s privacy) already has access to <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathdefault">p</span></span></span></span> data points of background knowledge. It was first defined in 2013 as a method of quantifying the reidentifiability of individuals based on mobile phone location data and uses a similar approach to our previous analysis of the modified <em>Guess Who?</em> ruleset.</p><p>Consider a dataset <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathdefault" style="margin-right:.02778em">D</span></span></span></span> containing the location history of mobile phone users over time. This data, when grouped by user, forms a number of <em>trajectories</em> across a time interval. That is to say, each user has a single trajectory in the dataset, composed of (likely) numerous location-time pairs. By choosing a selection of these pairs, we can form a <em>subtrajectory</em>. We can formalise this as so:</p><ol><li>Given one such trajectory <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathdefault" style="margin-right:.13889em">T</span></span></span></span>, we isolate <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>I</mi><mi>p</mi></msub><mo>⊆</mo><mi>T</mi></mrow><annotation encoding="application/x-tex">I_p \subseteq T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.969438em;vertical-align:-.286108em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:.07847em">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.15139200000000003em"><span style="top:-2.5500000000000003em;margin-left:-.07847em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">⊆</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathdefault" style="margin-right:.13889em">T</span></span></span></span>, a subtrajectory of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathdefault">p</span></span></span></span> spatio-temporal points randomly selected from <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathdefault" style="margin-right:.13889em">T</span></span></span></span>.</li></ol><p>It will often be the case that trajectories overlap (i.e. two users are in the same place at the same time). Although it is highly unlikely that two trajectories are complete duplicate, it may be the case that a subtrajectory is compatible with multiple trajectories. That is to say, there is more than one trajectory that contains all spacio-temporal pairs from the subtrajectory. We can again formalise this notion:</p><ol start="2"><li>We compute <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>S</mi><mo stretchy="false">(</mo><msub><mi>I</mi><mi>p</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">{</mo><mi>T</mi><mtext> </mtext><mi mathvariant="normal">∣</mi><mtext> </mtext><mi>T</mi><mo>∈</mo><mi>D</mi><mo separator="true">,</mo><mtext> </mtext><msub><mi>I</mi><mi>p</mi></msub><mo>∈</mo><mi>T</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">S(I_p) = \{T\ |\ T \in D,\ I_p \in T\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-.286108em"></span><span class="mord mathdefault" style="margin-right:.05764em">S</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:.07847em">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.15139200000000003em"><span style="top:-2.5500000000000003em;margin-left:-.07847em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">{</span><span class="mord mathdefault" style="margin-right:.13889em">T</span><span class="mspace"> </span><span class="mord">∣</span><span class="mspace"> </span><span class="mord mathdefault" style="margin-right:.13889em">T</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.969438em;vertical-align:-.286108em"></span><span class="mord mathdefault" style="margin-right:.02778em">D</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mspace"> </span><span class="mord"><span class="mord mathdefault" style="margin-right:.07847em">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.15139200000000003em"><span style="top:-2.5500000000000003em;margin-left:-.07847em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathdefault" style="margin-right:.13889em">T</span><span class="mclose">}</span></span></span></span>, which is defined as the set of all trajectories in the dataset which are compatible with <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>I</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">I_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.969438em;vertical-align:-.286108em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:.07847em">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.15139200000000003em"><span style="top:-2.5500000000000003em;margin-left:-.07847em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span></span></span></span>.</li></ol><p>Finally, we are particularly interested in subtrajectories that have only one compatible trajectory. That way, if an adversary knew that a certain user followed that subtrajectory, they would have uniquely idenitified that user in the dataset and could take advantage of all corresponding trajectory data—the exact outcome we would like to avoid. Formally, we have:</p><ol start="3"><li>The subtrajectory <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>I</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">I_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.969438em;vertical-align:-.286108em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:.07847em">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.15139200000000003em"><span style="top:-2.5500000000000003em;margin-left:-.07847em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span></span></span></span> is uniquely characterised if <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">∣</mi><mi>S</mi><mo stretchy="false">(</mo><msub><mi>I</mi><mi>p</mi></msub><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">|S(I_p)| = 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-.286108em"></span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:.05764em">S</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:.07847em">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.15139200000000003em"><span style="top:-2.5500000000000003em;margin-left:-.07847em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">∣</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span></span></span></span>, meaning that <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathdefault" style="margin-right:.13889em">T</span></span></span></span> is uniquely identifiable if all points in <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>I</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">I_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.969438em;vertical-align:-.286108em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:.07847em">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.15139200000000003em"><span style="top:-2.5500000000000003em;margin-left:-.07847em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span></span></span></span> are revealed.</li></ol><div class="note success"><p>For more details on the formal definition of Unicity, please see the research paper in which it was first proposed: <a href="https://www.nature.com/articles/srep01376" target="_blank" rel="noopener">Unique in the Crowd: The privacy bounds of human mobility</a></p></div><p>Unicity (<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>ε</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">\varepsilon_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.716668em;vertical-align:-.286108em"></span><span class="mord"><span class="mord mathdefault">ε</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.15139200000000003em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span></span></span></span>) is calculated by finding the proportion of uniquely characterised subtrajectories out of all possible subtrajectories across all trajectories in a dataset, given a value of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathdefault">p</span></span></span></span> points to reveal. The unicity of a dataset therefore gives us the probability that a randomly chosen trajectory (or user) will be reidentified from a dataset given that <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathdefault">p</span></span></span></span> of their data points are known.</p><p>Consider this with respect to our analysis of the modified Guess Who ruleset:</p><ol><li>Given a face, we took a subset of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathdefault">p</span></span></span></span> features from the face to represent the features that would be disclosed by the revealer through our questions.</li><li>We then found the number of faces on the board with a matching subset of features, or alternatively, we removed all the faces on the board which didn’t match the subset of features.</li><li>If there was only one matching face, this represented a winning result as we were successfully able to reidentify the face given a subset of its features.</li></ol><p>We then computed the probability of winning for this face using all possible subsets of features of size <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathdefault">p</span></span></span></span> and subsequently averaged this value across all faces to get an overall probability of winning given <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathdefault">p</span></span></span></span> questions.</p><p>The resemblance should be fairly clear. Comparing unicity to the probability of winning our modified version of <em>Guess Who?</em> offers a clear visualisation of the privacy metric; but, to really see the utility of such a technique, we need to apply it to a real-world dataset.</p><h2 id="case-study-24-hours-in-shanghai"><a class="markdownIt-Anchor" href="#case-study-24-hours-in-shanghai"></a> Case Study: 24 Hours in Shanghai</h2><p>On June 1st 2014, 2979 ordinary people woke up to a warm Sunday morning in the Chinese city of Shanghai. These individuals, linked together only by their choice of mobile service provider, went about their daily lives in the heaving metropolis of over 24 million people, tapping, chatting and swiping on their phones as they went.</p><p>What these individuals may not have realised was that their mobile service provider, Shanghai Telecom, was following their movements across the city through their mobile phones, by recording the closest base station to the device each time a user accessed the internet through mobile data. This information was later published publicly for use in computer science research, and although personally identifiable information was removed from the dataset before publication, we can use unicity to explore how the data still carries a privacy risk to the 2979 individuals who used their Shanghai Telecom phones on that date.</p><h3 id="preparing-the-data"><a class="markdownIt-Anchor" href="#preparing-the-data"></a> Preparing the Data</h3><div class="note success"><p>You can find the source code for this project in <a href="https://github.com/AlistairRobinson/Unicity" target="_blank" rel="noopener">this repository</a>, which includes a Dockerfile for reproducing the environment.</p></div><p>The full Shanghai Telecom dataset consists of over 7.2 million records of mobile internet accesses across the city over a period of 6 months. Data was collected from 9481 mobile devices accessing 3233 different base stations around the city. Each record contains a pseudonym identifier for a particular user, the start and end times for which their mobile data was in use and the geographic location of the base station used to provide network access to the device.</p><table><thead><tr><th style="text-align:center">Start Time</th><th style="text-align:center">End Time</th><th style="text-align:center">Location</th><th style="text-align:center">User ID</th></tr></thead><tbody><tr><td style="text-align:center">01/06/2014 10:22</td><td style="text-align:center">01/06/2014 11:09</td><td style="text-align:center"><code>31.237872/121.470259</code></td><td style="text-align:center"><code>edbc54...</code></td></tr><tr><td style="text-align:center">01/06/2014 07:00</td><td style="text-align:center">01/06/2014 08:49</td><td style="text-align:center"><code>31.237872/121.470259</code></td><td style="text-align:center"><code>f8206a...</code></td></tr><tr><td style="text-align:center"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">⋮</mi><mpadded height="+0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mrow><annotation encoding="application/x-tex">\vdots</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.53em;vertical-align:-.03em"></span><span class="mord"><span class="mord">⋮</span><span class="mord rule" style="border-right-width:0;border-top-width:1.5em;bottom:0"></span></span></span></span></span></td><td style="text-align:center"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">⋮</mi><mpadded height="+0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mrow><annotation encoding="application/x-tex">\vdots</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.53em;vertical-align:-.03em"></span><span class="mord"><span class="mord">⋮</span><span class="mord rule" style="border-right-width:0;border-top-width:1.5em;bottom:0"></span></span></span></span></span></td><td style="text-align:center"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">⋮</mi><mpadded height="+0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mrow><annotation encoding="application/x-tex">\vdots</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.53em;vertical-align:-.03em"></span><span class="mord"><span class="mord">⋮</span><span class="mord rule" style="border-right-width:0;border-top-width:1.5em;bottom:0"></span></span></span></span></span></td><td style="text-align:center"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">⋮</mi><mpadded height="+0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mrow><annotation encoding="application/x-tex">\vdots</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.53em;vertical-align:-.03em"></span><span class="mord"><span class="mord">⋮</span><span class="mord rule" style="border-right-width:0;border-top-width:1.5em;bottom:0"></span></span></span></span></span></td></tr></tbody></table><p>Because the complete 6-month dataset is so large, it would be infeasible to calculate unicity across the full time interval. Instead, we can limit our dataset to a single day (01/06/2014) in the 6-month period and use this as a representative sample to estimate the unicity for the full time period.</p><p>Furthermore, we want to change the way that the dataset considers the time at each data point. Having time represented as a range between a start point and an end point is helpful if we want to investigate the duration of mobile data usage, but for our purposes, it’s more useful to have data recorded at a single point in time as opposed to a time interval, so that it becomes a trajectory of locations over time. To achieve this, we take the midpoint of the start and end time and round it to a suitable time interval of 60 minutes. This has the added positive effect of coalescing groups of high frequency, low duration records into single data points.</p><p>Following the data cleaning procedure, the dataset now contains 20851 records, representing the movements of 2979 users on 01/06/2014.</p><table><thead><tr><th style="text-align:center">Location</th><th style="text-align:center">User ID</th><th style="text-align:center">Time</th></tr></thead><tbody><tr><td style="text-align:center"><code>31.237872/121.470259</code></td><td style="text-align:center"><code>edbc54...</code></td><td style="text-align:center">2014-06-01 11:00:00</td></tr><tr><td style="text-align:center"><code>31.237872/121.470259</code></td><td style="text-align:center"><code>f8206a...</code></td><td style="text-align:center">2014-06-01 08:00:00</td></tr><tr><td style="text-align:center"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">⋮</mi><mpadded height="+0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mrow><annotation encoding="application/x-tex">\vdots</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.53em;vertical-align:-.03em"></span><span class="mord"><span class="mord">⋮</span><span class="mord rule" style="border-right-width:0;border-top-width:1.5em;bottom:0"></span></span></span></span></span></td><td style="text-align:center"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">⋮</mi><mpadded height="+0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mrow><annotation encoding="application/x-tex">\vdots</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.53em;vertical-align:-.03em"></span><span class="mord"><span class="mord">⋮</span><span class="mord rule" style="border-right-width:0;border-top-width:1.5em;bottom:0"></span></span></span></span></span></td><td style="text-align:center"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">⋮</mi><mpadded height="+0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mrow><annotation encoding="application/x-tex">\vdots</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.53em;vertical-align:-.03em"></span><span class="mord"><span class="mord">⋮</span><span class="mord rule" style="border-right-width:0;border-top-width:1.5em;bottom:0"></span></span></span></span></span></td></tr></tbody></table><h3 id="developing-the-experiment"><a class="markdownIt-Anchor" href="#developing-the-experiment"></a> Developing the Experiment</h3><p>To perform analysis on our cleaned dataset, I developed a Python module called <code>unicity</code> to facilitate the calculation of unicity in arbitrary datasets. Much of the module contains helper functions to facilitate the experiment interface and the data cleaning procedure; the core functionality of unicity calculations is provided by the <code>unicity</code> function in <code>unicity/stats.py</code>. This function facilitates the calculation of unicity for any arbitrary dataset with a user identifier column. I encourage you to experiment using it yourselves and test it out on your own datasets.</p><p>We can use the <code>unicity</code> module to compute the unicity of our cleaned dataset at different values of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathdefault">p</span></span></span></span>. From this, we can demonstrate how the number of data points revealed through background knowledge affects the risk of reidentification in a dataset.</p><h3 id="risk-of-reidentification"><a class="markdownIt-Anchor" href="#risk-of-reidentification"></a> Risk of Reidentification</h3><p>Calculating unicity values for the dataset with <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathdefault">p</span></span></span></span> values between 1 and 10 gave us the following results:</p><p><img src="/" class="lazyload" data-src="/images/quantifying-anonymity/quantifying-anonymity_16_0.png"  alt=""></p><div class="note info"><p>For values of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>≥</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">p \geq 5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8304100000000001em;vertical-align:-.19444em"></span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">≥</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">5</span></span></span></span>, a representative sample of 100,000 subtrajectories was taken to calculate unicity, as computing all possible subtrajectories would have taken weeks to complete.</p></div><p>As we would expect, the risk of reidentification increases with the amount of background knowledge released. However, you may be surprised how little information is required to reidentify individuals in the dataset with high accuracy: for <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">p = 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span></span></span></span>, unicity is 0.50024. Therefore, the probability of successful reidentification is just over 50%. However, just by adding one more piece of spatio-temporal data at <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">p = 2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">2</span></span></span></span>, we increase this risk to just over 90%. At <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">p = 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">3</span></span></span></span>, this increases to over 96%. For <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>≥</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">p \geq 5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8304100000000001em;vertical-align:-.19444em"></span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">≥</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">5</span></span></span></span>, the probability of successfully reidentifying an individual is over 99%.</p><p>In other words, if you knew the location of one of the 2979 users at any five points in time on 01/06/2014, you would be able to reidentify them in the dataset with a probability of over 99%. This would also reveal their pseudonym identifier to you, allowing you to track their movements across the city for the entire 6 month time period included in the dataset.</p><p>However, these results also highlight an important limitation of unicity. Shanghai is a city with over 24 million inhabitants, and yet, our findings appear to suggest that just 5 spatio-temporal points are sufficient to reidentify any inhabitant with over 99% accuracy. Is this really the case?</p><p>The answer is no, because our data only takes into consideration the movements of 2979 out of the 24 million inhabitants of Shanghai. Therefore, any probabilities relating to the reidentification of an individual which are derived from unicity are limited by the constraint that the individual is known to be in the dataset. Otherwise, we don’t have enough information to derive the probability of reidentification for the wider population.</p><p>This is the primary limitation of unicity and similar privacy metrics: because it only gives us the risk of reidentification for individuals within a dataset, we can’t extrapolate this information to find the risk of reidentification for a wider population without additional data. This limits the applicability of unicity to real-world contexts because it’s very rare to have enough data to represent an entire population.</p><p>However, this limitation doesn’t stop unicity from excelling at one of its best applications: comparing the efficacy of different methods of increasing privacy in a dataset.</p><h3 id="zoom-and-enhance"><a class="markdownIt-Anchor" href="#zoom-and-enhance"></a> Zoom and Enhance</h3><p>The granularity of information in a dataset has a substantial impact on the privacy of individuals in the dataset. For example, tracking your precise location every second provides almost no privacy, whereas tracking your location to the nearest postcode every hour provides offers much more. This is because the data becomes more generalised as its resolution decreases; in other words, because there are several people passing through the same postcode every hour, the individual risk to privacy is reduced.</p><p>Our dataset carries spatio-temporal information about users, so we can adjust the spatial and temporal resolution of our data and measure the effect this has on privacy using unicity. However, because the dataset already uses a discrete set of points to represent location data (the locations of base stations in Shanghai), it is more difficult to change our spatial resolution. We can, however, easily modify the temporal resolution of our data by changing the time interval used when rounding time midpoints during the data cleaning process.</p><p>Comparing unicity over <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathdefault">p</span></span></span></span> with temporal resolutions of 60 minutes, 120 minutes, 180 minutes and 240 minutes, we find the following results:</p><p><img src="/" class="lazyload" data-src="/images/quantifying-anonymity/quantifying-anonymity_19_0.png"  alt=""></p><div class="note info"><p>For temporal resolutions of 180 and 240 minutes, some unicity values are not defined. This is because the maximum trajectory lengths over the 24-hour period at these resolutions are 8 and 6 points respectively.</p></div><p>We can draw two important conclusions from these results:</p><ul><li>Unicity, and by extension, the risk of reidentification in the dataset decreases with temporal resolution.</li><li>However, for large <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathdefault">p</span></span></span></span>, it is possible to accurately (&gt;90%) reidentify users regardless of temporal resolution.</li></ul><p>This suggests that, although decreasing data granularity increases privacy and anonymity in a dataset, individuals are still at risk of reidentification when large amounts of background information are known.</p><h2 id="conclusion"><a class="markdownIt-Anchor" href="#conclusion"></a> Conclusion</h2><p>This article has given an insight into the world of data privacy and demonstrated just one method which data scientists might use to measure privacy risks in an anonymised dataset. We’ve used a metric for measuring the risk of reidentification in a dataset, unicity, to demonstrate how background knowledge can allow us to reidentify individuals based on their location data with high accuracy. We’ve also shown that, to increase the privacy of users in a dataset, you can lower the temporal resolution of your data to make it appear more generalised.</p><p>What this article has intentionally not addressed is why you should care about privacy; an area often seen as the more political or sociological aspect of data privacy. Regardless of your opinions towards the importance of individual privacy, this article has demonstrated how easy it is for the illusion of privacy to be shattered by seemingly insignificant pieces of background information. Furthermore, understanding the ways in which privacy can be compromised is an important step in the direction of learning how to responsibly collect, clean, store and process information from individuals who are protected by data protection law; a skill necessary for all data scientists. Hopefully, that’s an idea we can all get behind.</p><p><em>The <a href="http://sguangwang.com/TelecomDataset.html" target="_blank" rel="noopener">dataset</a> used in this article is publicly available and was provided by academics from the Beijing University of Posts and Telecommunications. A full appendix of experimental results collected from this project is available on <a href="https://github.com/AlistairRobinson/Unicity/blob/master/results/2014-06-01.csv" target="_blank" rel="noopener">GitHub</a></em>.</p>]]></content>
      
      
      <categories>
          
          <category> Computer Science </category>
          
          <category> Cyber Security </category>
          
      </categories>
      
      
        <tags>
            
            <tag> lesson </tag>
            
            <tag> ethics </tag>
            
            <tag> geospatial </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>The Trump Effect: Can Data Explain What We Never Saw Coming?</title>
      <link href="/trump-effect/"/>
      <url>/trump-effect/</url>
      
        <content type="html"><![CDATA[<div class="note info"><p><strong>Accessing Post Source</strong><br>We are still working on getting this site set up, so source code for this post is not yet available. Check back soon and you’ll be able to find it linked here.</p></div><p>Looking back four years ago to the United States’s 2016 Presidential election, it can be easy to forget how confident people were that Trump couldn’t possibly win the race. With the 2020 election fast approaching, it is critical that we can understand what led to his victory and how that might influence the upcoming result. We will do this not by relying on speculation or political opinion, but by showcasing a data-driven regression model that we can use to reliably intuit the primary demographic and socio-economic factors that led to Trump’s victory. In doing this, we seek to answer two questions:</p><ol><li><p>Despite his overall unpopularity, how did Trump pull off an election victory?</p></li><li><p>Why did this come as such a surprise?</p></li></ol><p><img src="/" class="lazyload" data-src="/images/trump-effect/newspaper_headlines.jpg"  alt="Newspaper headlines incorrectly predicting a loss for Trump"></p><h2 id="how-trump-won-and-lost"><a class="markdownIt-Anchor" href="#how-trump-won-and-lost"></a> How Trump Won (and Lost)</h2><div class="note info"><p><strong>A Brief Introduction to the Electoral College</strong><br>The Electoral College is the United State’s electoral system, which allocates each of its 50 states plus DC a set number of votes, loosely (though not exactly) based on their populations. The winner of the plurality vote in each state receives that state’s full number of college votes (with the exception of Maine and Nebraska).</p></div><p>To address our first question, despite losing the national popular vote, Trump was returned as president in 2016 by the electoral college. That is, despite having less votes overall, the states that he did win were such that he ended up with more college votes allocated to him than Clinton did, and so claimed his victory. This is intuitive when we see that Clinton won heavily in large states, such as California (by a wide 30% margin), giving her 55 college votes, whilst Trump won by less than a percent in Michigan, Pennsylvania, and Wisconsin, as well as taking Florida by 1.2%, winning him 75 college votes for these alone. These small wins count for far more than Clinton blowing Trump away in California, since a state victory leads to the same amount of college votes regardless of the margin.</p><p><img src="/" class="lazyload" data-src="/images/trump-effect/trump-effect_7_0.png"  alt=""></p><p>Since this system is so state-centric, we can pinpoint exactly where Trump won electoral votes that in turn won him the election. There were only six states that changed party, all switching from voting Obama to voting Trump. This can be shown on an electoral map of the US. All states which voted Republican in 2012 and 2016 are in red, whilst blue denotes the same for the Democrats. Greyed states are those which voted Obama, then Trump. Effectively, these 6 states decided the president.</p><p><img src="/" class="lazyload" data-src="/images/trump-effect/changing_states.png"  alt="State color changes from 2012 to 2016"><br>Incredibly, of these 6 states, 4 of them were won by a percent or less. In the others—Ohio and Iowa—Trump saw a huge swing his way, winning both by close to 10%.<br>Aside from these margins, the other crucial thing to note is the geographical proximity of 5 of these states. Broadly, these are all part of the midwest, and make up a region known as the “rust belt”. These used to be manufacturing powerhouses of the US, are largely white, and (owing to the US’s departure from a manufacturing economy) have rising unemployment. One of the key outcomes of this project is to demonstrate that Trump’s departure from typical Republican vote shares gave him the perfect coalition of states to win the White House without needing the national popular vote. To understand exactly how these states shot him to victory, we’ll need a statistical model to help us grapple with the complexities of election dynamics.</p><p><img src="/" class="lazyload" data-src="/images/trump-effect/trump-effect_11_0.png"  alt=""></p><div class="note success"><p>You will likely notice the outlying state of Utah, coloured purple. We explain the cause of this deviation in our <a href="#outliers">section on model outliers</a>.</p></div><h2 id="modelling-the-problem"><a class="markdownIt-Anchor" href="#modelling-the-problem"></a> Modelling the Problem</h2><p>In order to understand what led to Trump’s victory, we needed to develop a model that can capture the complex relationships between voter attributes and election outcomes. After trawling the web for relevant data sources, we ended up with enough fine-grained data to model the problem at a county level. This included demographic and socio-economic breakdowns for each county, as well as the presidential election vote counts for the years 2008, 2012, and 2016.</p><div class="note info"><p><strong>Data Sources &amp; Variable List</strong><br>A summary of the data sources used for this analysis can be found at <a href="https://github.com/warwickdatascience/wdss-research-website/tree/master/content/trump-effect/data/sources.yml" target="_blank" rel="noopener">this link</a> and a list of included variables can be found in <a href="#appendix">the post appendix</a>.</p></div><p>We took this data and formed a model in which we aimed to predict, for each county, the difference in Republican vote share from 2008/2012 to 2016 based on the descriptors we had available. By inspecting the coefficients of this model, we would then be able to intuit the impact that various demographic and socio-economic factors had on Trump’s victory.</p><p>This framework of modeling is highly complex, with a large number of correlated inputs and two correlated outputs (the 2008/16 and 2012/16 differences). A naïve approach would be to fit a multivariate <a href="https://statisticsbyjim.com/glossary/ordinary-least-squares/" target="_blank" rel="noopener">OLS model</a>, but this would almost certainly suffer from <a href="https://statisticsbyjim.com/regression/multicollinearity-in-regression-analysis/" target="_blank" rel="noopener">multicolinearity</a>. This is a phenomenon in linear modelling that occurs when combinations of predictions are highly correlated with others. This can have a severe impact on our regression coefficients, making it near impossible to determine which factors actually influenced the outcome.</p><p>A common workaround is to implement <a href="https://learnche.org/pid/latent-variable-modelling/principal-components-regression" target="_blank" rel="noopener">principal component regression (PCR)</a>, which prevents multicolinearity by first projecting the input predictors to a lower dimensional space in which all variables are orthogonal. This method still has its drawbacks, however; namely, the projection is chosen to maximise variance in the predictor space only, not taking into account which combinations of predictors are actually good for predicting. This could easily lead to scenarios in which valuable input variables are discarded in the projection because they don’t express much variance.</p><p>To combat this issue, we decided to use a slightly more complex model known as <a href="https://stats.idre.ucla.edu/wp-content/uploads/2016/02/pls.pdf" target="_blank" rel="noopener">partial least squares (PLS) regression</a>. This is a model used heavily in cheminformatics and signal processing for its ability to robustly handle highly-correlated predictors and responses without sacrificing predictive accuracy. The model is trained using a simple interative procedure, resulting in standardized coefficients, giving the predicted impact of each input factor on the response. In our case, we can use these coefficients to understand which demographic and socio-economic factors had the largest impact in Trump’s victory.</p><p>By using a statistical model such as PLS regression, we are able to capture relations that are unlikely to be detected with a simple model or by rudimentary data visualisation. This allows us to draw detailed political insight, in the knowledge that our results are backed by data and rigourous statistical methods. This is the power that data science brings to this analysis, letting us answer old questions in new, insightful ways.</p><div class="note info"><p><strong>Code Attributions</strong><br>Data scraping and initial model development was performed by <a href="https://www.linkedin.com/in/janique-krasnowska-94a1a0195/" target="_blank" rel="noopener">Janique Krasnowska</a>, with support, final tweaks and statistical write-up by <a href="https://www.linkedin.com/in/tim-hargreaves/" target="_blank" rel="noopener">Tim Hargreaves</a>. The intial modelling framework and data sources were suggested by <a href="https://www.linkedin.com/in/tom-palmer-61954a177/" target="_blank" rel="noopener">Tom Palmer</a>, the author of this post.</p></div><div class="note success"><p><strong>Further Reading</strong><br>If you wish to learn more about PLS regression and its implementation in R, the <a href="https://cran.r-project.org/web/packages/pls/vignettes/pls-manual.pdf" target="_blank" rel="noopener">following vignette</a> is a great starting point.</p></div><h2 id="explaining-trumps-victory"><a class="markdownIt-Anchor" href="#explaining-trumps-victory"></a> Explaining Trump’s Victory</h2><div class="note danger"><p><strong>Risk of Misinterpretation</strong><br>To avoid misuse of our models, it is important to highlight two key facts:</p><ul><li>All patterns shown in this post are correlations, not causations. By combining these trends with political theory and performing appropriate tests on our model, we can be more more confident in the causal nature of such patterns, but we can never be sure</li><li>The standardised coefficents from the model shown below do not directly relate to the impact of different factors on Trump’s swing, but rather give relative values more appropriate for comparison. Our PLS has the potential to give absolute coefficients, but we have omitted these in this post for simplicity and clarity.</li></ul></div><h3 id="age"><a class="markdownIt-Anchor" href="#age"></a> Age</h3><p>We began our exploration by testing the theory that support for Trump largely grows with increasing demographic age. Interestingly, our results support a more nuanced view, suggesting that his appeal is more generational, and that Trump’s main gain was from an older, yet still working age, demographic.</p><div class="note warning"><p><strong>Understanding Standarised Coefficient Plots</strong><br>This section contains several visualisations of the standardised coefficients from our model. These show the influence of a given predictor on the likelihood of Trump gaining a vote over a previous Republican candidate. In other words, a large positive coefficient means that a certain factor in a population led to Trump gaining votes, a large negative coefficent relates to Trump losing votes, and a small coefficient implies the factor had little impact.</p></div><p><img src="/" class="lazyload" data-src="/images/trump-effect/trump-effect_28_0.png"  alt=""></p><p>This is an unusual finding, and we take two conclusions from this. Firstly, the generational divide in politics does exist. In terms of Trump-populism, there is no linear adage we can apply, such as “the older you get, the more conservative you become”. Instead the trends are highly generational (think baby boomers, Generation Z, millennials, etc.). This cyclic nature of history is supported by <a href="https://www.theguardian.com/technology/2019/nov/12/history-as-a-giant-data-set-how-analysing-the-past-could-help-save-the-future" target="_blank" rel="noopener">Peter Turchin’s recent work</a> suggesting that many historical and political trends can be modelled surprisingly well using periodic systems. Secondly, a key base of Trump’s was the older working population; people in full-time work, but pre-retirement seemed to be particularly enamored with Trump’s message compared with previous Republicans’.</p><h3 id="industry"><a class="markdownIt-Anchor" href="#industry"></a> Industry</h3><p>With the knowledge that Trump swung five “rust belt” states his way, and that these are known for their waning manufacturing industries, it was critical that we included industry breakdowns in our model formulation. From this, we found a strong positive swing to Trump from production workers, corroborating the theory that this is how he swung these particular states. Clearly, this played a huge part of his campaign, aiming rhetoric at returning jobs to America and backing American manufacturing.</p><p><img src="/" class="lazyload" data-src="/images/trump-effect/trump-effect_32_0.png"  alt=""></p><p>This massive shift in voter base was essential for Trump’s win, as it allowed him to unexpectedly take the midwest, with it’s large proportion of production workers.</p><p><img src="/" class="lazyload" data-src="/images/trump-effect/trump-effect_34_0.png"  alt=""></p><h3 id="race"><a class="markdownIt-Anchor" href="#race"></a> Race</h3><p>The graph above also hints that race played a large role in Trump’s victory. Our model confirms this, highlighting in particular that white people swung heavily towards Trump, whereas black and hispanic people swung away. This is particularly relevant to the midwest, since it is one of the whiter areas of the US, meaning Trump’s success here, and therefore in the election at large, can be partially attributed to his success in winning over white people.</p><p><img src="/" class="lazyload" data-src="/images/trump-effect/trump-effect_36_0.png"  alt=""></p><p>In fact, our data had a higher fidelity than race alone, including the intersection of common races and sexes. This allows us to also note, perhaps unsurprisingly, that Trump gained more male votes than female, regardless of race.</p><p><img src="/" class="lazyload" data-src="/images/trump-effect/trump-effect_38_0.png"  alt=""></p><h3 id="religion"><a class="markdownIt-Anchor" href="#religion"></a> Religion</h3><p>Effectively coding religious demography proved challenging, as there are a huge range of denominations, many having relatively few adherents. Our goal was to balance out religous fidelity with introducing too many variables as to add noise to the model. In the end, we separated Evangelical groups into their own category, as there was (and still is) <a href="https://play.acast.com/s/intelligencesquared/whydoevangelicalsworshipatthealtarofdonaldtrump-withsarahposnerandbrianklaas" target="_blank" rel="noopener">a lot of speculation</a> that they played a crucial role in his victory. Despite this, the data strongly suggests that this variable is largely unimportant, as Trump seemed to lose Evangelical votes, but still performed well enough to win in Republican heartlands. The swing against Trump in the “bible belt” was minor, and insufficient to change the election outcome. This a fascinating example of how data science lets us see past our preconceptions and biases, to understand problems with greater clarity.</p><p><img src="/" class="lazyload" data-src="/images/trump-effect/trump-effect_41_0.png"  alt=""></p><p>For reference, the religious distributions for both the midwest and remaining US are shown below.</p><p><img src="/" class="lazyload" data-src="/images/trump-effect/trump-effect_43_0.png"  alt=""></p><h3 id="education-income"><a class="markdownIt-Anchor" href="#education-income"></a> Education &amp; Income</h3><p>The crucial findings in this area show us both how different Trump was as a candidate compared to previous Republicans, and also how largely unimportant certain swings were. In terms of education, Trump was incredibly unpopular, facing a large negative swing from bachelor’s degree holders. However, this did little to impact his success. One explanation for this is that educational attainment is concentrated in areas where Democrats are already successful, so in the “winner takes all” system of the US, a further loss was largely irrelevant.</p><p>Conversely, unemployment and poverty correlated positively with Trump voters, further backing our argument of him taking traditional Democrat voter groups.</p><p><img src="/" class="lazyload" data-src="/images/trump-effect/trump-effect_45_0.png"  alt=""></p><p>The impact of the change in Trump’s appeal towards bachelor’s degree holders is easily seen when we plot a heatmap of bachelor degree attainment; a dark patch is centred lower midwest where key swing states are located.</p><p><img src="/" class="lazyload" data-src="/images/trump-effect/trump-effect_47_0.png"  alt=""></p><h3 id="outliers"><a class="markdownIt-Anchor" href="#outliers"></a> Outliers</h3><p>Overall, we believe that our model is a strong success. Not only does it provide evidence for several previously speculative theories, it also casts doubt on some unproven beliefs. Our model captures a good proportion of the variability of the response with a desirable error distribution.</p><p><img src="/" class="lazyload" data-src="/images/trump-effect/trump-effect_50_0.png"  alt=""></p><p>As with all models, outliers exist, but these can be easily explained. On one end (marked blue), we overpredicted the swing from several counties in Utah and Idaho. That is, based on the trends we saw elsewhere, our model suggested he should have performed better than he did. This deviation from our predictions can be explained in one word—Mormons. Many counties in this region of the US were settled by the Church of Latter Day Saints, leading to a third Mormon candidate, Evan MacMullin, claiming a sizeable share of votes (20% in Utah, 6% in Idaho) largely from Trump, leading to this minor discrepancy in our model.</p><p>On the other end of the spectrum, we have areas such as Laporte County, Indiana (marked green) which we underpredicted. In other words, Trump did better in these areas than our model predicted. This is partially explained by Trump choosing Mike Pence to be his running mate. VP candidates are often hand-picked to help “deliver” a region. In this case, Pence was a longterm governor from a rust belt state. The model does not have a way to account for the running mate effect (though this could be introduced with manual feature engineering), hence underpredicted the swing Trump will have obtained from choosing Indiana-favourite Pence as running mate.</p><div class="note success"><p>Although we’ve only showed the 2008–2016 differences here, the outliers for 2012 are largely similar, though exacerbated even further by the fact that Utah was 2012 Republican candidate Mitt Romney’s home state.</p></div><h2 id="conclusion"><a class="markdownIt-Anchor" href="#conclusion"></a> Conclusion</h2><p>With that, we conclude our whirlwind tour of Trump’s 2016 presidential victory. Our model has cast light on Trump’s effect and dependence on a unique coalition of states in order to pull off his unexpected win. Upon analysing trends in historical patterns, we find that traditional left/right distributions (such as income and religion) were largely unimportant in this election, particularly compared to education levels and industry distributions.</p><p>Our work also highlights the groups that Trump must retain in order to win again in 2020. Much is made of his performance with evangelical groups—traditionally bastions of the Republican party—but our data-driven approach shows that this is largely a side factor, contrary to popular speculation. To win again, Trump doesn’t need more votes in evangelical areas—the &quot;bible belt” as it is usually known—but instead, he must retain workers in the production industry, and the older working generation. This enabled him to win the midwest states that were sufficient to edge him into the White House and could quite possibly keep him there.</p><p>This project showcases data science at its best, bringing together statistics, computer science, and domain expertise to solve an old problem in a new way. With this collaboration of disiplines, such conclusions would not be possible: political speculation and theory would lack supportive evidence that can only be derived through advanced statistical modelling; on the other hand, without the ability to tie model results to reality using domain knowledge and insight, the practicality and real-life validity of a statistical model can rarely be relied upon. This project was only facilitated by Warwick Data Science Society’s community of students and the infrastructure they provide for blogging. We hope that this post inspires fellow students to consider the insight that data science can bring to their field and to engage with WDSS to produce similar works. Thank you for reading.</p><h2 id="appendix"><a class="markdownIt-Anchor" href="#appendix"></a> Appendix</h2><div class="note success"><p>For a circle of correlations, please see the source notebook.</p></div><pre><code>Table of Standardized Coefficients==================================                 diff0816 diff12160to4                0.029    0.02510to14             -0.060   -0.08015to19             -0.057   -0.06620to24              0.000    0.00725to29              0.023    0.04030to34              0.002    0.01135to39             -0.023   -0.02840to44             -0.025   -0.03445to49              0.029    0.0315to9               -0.027   -0.04050to54              0.097    0.10755to59              0.072    0.07560to64              0.019    0.02065to69             -0.043   -0.04570to74             -0.064   -0.06575to79             -0.020   -0.02080to84              0.047    0.05285+                 0.080    0.084AsianF             -0.011   -0.001AsianM             -0.002    0.009BlackF             -0.115   -0.085BlackM             -0.098   -0.067HispanicF          -0.086   -0.081HispanicM          -0.077   -0.072WhiteF              0.099    0.067WhiteM              0.115    0.084UnemploymentRate    0.068    0.090IncomeLog          -0.080   -0.095Poverty             0.058    0.090Professional       -0.110   -0.122Service             0.028    0.051Office             -0.049   -0.045Construction       -0.001   -0.009Production          0.132    0.136popsqmile           0.024    0.036High School        -0.036   -0.057Bachelor           -0.228   -0.250Mainline            0.178    0.183Evangelical        -0.054   -0.050Catholic            0.124    0.137Black Prot         -0.067   -0.039</code></pre>]]></content>
      
      
      <categories>
          
          <category> Social Sciences </category>
          
          <category> Politics </category>
          
      </categories>
      
      
        <tags>
            
            <tag> visualization </tag>
            
            <tag> data-analysis </tag>
            
            <tag> regression </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A League of Its Own: Rethinking University League Tables</title>
      <link href="/league-of-its-own/"/>
      <url>/league-of-its-own/</url>
      
        <content type="html"><![CDATA[<div class="note info"><p>This post is the corresponding write-up for a WDSS project in which a pair of society members collaborated to produce a web app for visualizing university league table data. You can view the final product at <a href="https://shiny.warwickdatascience.com/league-table-explorer" target="_blank" rel="noopener">this link</a>.</p></div><h2 id="motivation"><a class="markdownIt-Anchor" href="#motivation"></a> Motivation</h2><p>The power of data manifests itself not only in accessiblity, but in usability. Without the ability to present trends and patterns in an understandable form, we cannot answer the questions we care about, and so data provides us less value than it is capable of.</p><p>This scenario presents itself in regard to university ranking data. There is a lot of year-on-year variation in university rankings; take as an example, the overall ranking of the University of Warwick in the last five years, spanning the range from 8th to 11th. This temporal evolution is often overlooked in making university choices, despite it being likely to have a significant impact on how one’s degree is viewed in the future.</p><p>The reason for this oversight is that all the most popular ranking websites display only the current year’s data by default. Any long-term comparison has to be done by manually accessing data for different years, requiring considerable time and effort. This led us to question whether we could collate this data and present it in a more usable form, capable of displaying these long-term trends.</p><p>This project demonstrates the usefulness of web scraping in obtaining larger datasets for comparative purposes than we would be able to otherwise. It also highlights the importance of effective visualizations in data science and how they make data more interpretable and accessible. Two members of WDSS collaborated on this project: <a href="https://www.linkedin.com/in/tim-hargreaves/" target="_blank" rel="noopener">Tim Hargreaves</a>, who developed the app and scraped the ranking data, and I (<a href="https://www.linkedin.com/in/janique-krasnowska-94a1a0195/" target="_blank" rel="noopener">Janique Krasnowska</a>), who performed the initial data exploration and communicated the findings.</p><h2 id="implementation"><a class="markdownIt-Anchor" href="#implementation"></a> Implementation</h2><div class="note info"><p>The source code for the web app and scraping scripts have been open-sourced in <a href="https://github.com/warwickdatascience/league-table-explorer" target="_blank" rel="noopener">this repository</a>.</p></div><p>The entire project was developed using the language R. We began by scraping data from the <a href="https://www.thecompleteuniversityguide.co.uk/league-tables/rankings" target="_blank" rel="noopener">Complete University Guide</a> using the <code>rvest</code> package. In doing this, we obtained data for 187 universities and 70 courses, spanning the past 13 years; 45,580 individual observations in total. Needless to say, it would be counterproductive to attempt to combine the insights from this data in just one graph. Instead, we opted for an interactive web app that allows the user to choose the universities and courses they may be interested in.</p><p>This app was implemented using <a href="https://shiny.rstudio.com/" target="_blank" rel="noopener">Shiny</a>, a framework developed by RStudio that makes it easy to build interactive web applications directly from R, with very limited knowledge of HTML, CSS and JavaScript. We used two graphics libraries for the visualisation; ggplot is responsible for the backbone of the graphs and Plotly adds an additional layer of interactivity. For example, Plotly makes it possible for the user to hover over a data point on a graph and have detailed information related to that observation displayed.</p><p>There are two types of comparisons a user can make: the same course at multiple universities or multiple courses at the same university. More complex comparisons would also be possible with more complicated code, but one might question the usefulness of comparing Archaeology at Oxford with Economics at Warwick, for example.</p><p>To make discovering trends easier, we included a LOESS smoothing feature in the app. LOESS is a non-parametric method based on moving averages that constructs a smooth curve of best fit for the data points. It helps us to filter out noise and focus instead on general trends. For example, we can see in recent years that Warwick’s mathematical prowess is decreasing whilst Durham overtakes. By choosing to show extra features when hovering, we can see that Durham now outperforms Warwick across all the Complete University Guide metrics (entry standards, student satisfaction, research intensity, and graduate prospects).</p><p><img src="/" class="lazyload" data-src="/images/league-of-its-own/durham_warwick_comp.png"  alt="Comparing Mathematics course at Warwick and Durham"></p><h2 id="takeaways"><a class="markdownIt-Anchor" href="#takeaways"></a> Takeaways</h2><p>This small coding project demonstrates some of the core values of data science—obtaining data of interest with the help of programming software and presenting the gained insights in a way that is accessible to a lay person. These skills could be applied to any problem where data is stored poorly or in multiple sources and has to be transformed to answer questions from people without any coding background.</p><p>The final product wouldn’t be possible without WDSS resources, including its Shiny server and blogging platform.</p><p>Thank you for reading. We hope you find other interesting trends in higher education rankings with our app.</p>]]></content>
      
      
      <categories>
          
          <category> Computer Science </category>
          
          <category> Web </category>
          
      </categories>
      
      
        <tags>
            
            <tag> shiny </tag>
            
            <tag> web-scraping </tag>
            
            <tag> visualization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Money for Nothing: An Arbitrage Paradox</title>
      <link href="/money-for-nothing/"/>
      <url>/money-for-nothing/</url>
      
        <content type="html"><![CDATA[<p>Utilising the concept of arbitrage can be a lucrative opportunity when done correctly. Yet somehow, when done incorrectly, the results can be even more propitious (at least for one of the parties involved). In this post, we will walk through a paradox related to arbitrage in which the seemingly obvious fair price for an asset results in a strategy for obtaining infinite wealth.</p><h2 id="the-financial-jargon"><a class="markdownIt-Anchor" href="#the-financial-jargon"></a> The Financial Jargon</h2><div class="note warning"><p>If you are already familiar with the notions of options and arbitrage, you may wish to skip to the <a href="#the-paradox">next section</a>.</p></div><p>To make this post inclusive to all, we will begin by offering a brief, requisite understanding of the financial vocabulary used in this post, before attempting to describe the paradox itself.</p><p>In the financial world, it is hard to avoid the words <em>stocks</em> and <em>shares</em>, so we will begin by defining these. A stock is a type of security that represents the ownership of a fraction of a company. This entitles the owner of the stock to a fraction of the corporation’s assets and profits proportional to how much stock they own. A share is simply the smallest denomination of a company’s stock and acts as a unit for measuring one’s stake in a company.</p><p>Another key financial concept is that of the <em>option</em>. These are financial instruments based on the value of underlying securities (such as stocks). Since the value of an option is dependent on the value of another financial asset (or assets), it is a type of derivative. An options contract offers the buyer the opportunity to buy or sell the underlying asset within some fixed timeframe for a predetermined price. The two most common types of options are <em>calls</em> and <em>puts</em>; a call allowing the holder to buy the asset whereas a put offers the sale. It is important to note, that an options contract puts the holder under no obligation to buy/sell the asset within the agreed upon timeframe (unlike a futures contract). Instead, they are free to exercise the option only when it benefits them financially.</p><p>There are a several components of an options contract, such as the premium, strike price, expiration date, and contract size, but we will focus only on the premium and strike price in this post. The premium is the down payment that the holder of the contract pays for having the right to exercise an options trade. This is paid regardless of whether the holder exercises the option. The strike price is the price at which the holder of the option can buy/sell the underlying security if they decide to exercise it. Note, that this is a fixed price and so does not change, even though the value of the underlying security might.</p><p>This brings us to <em>arbitrage</em>. Arbitrage is the act of simultaneously buying and selling assets or commodities in different markets or in derivative forms, to take advantage of different prices of the same asset. It exploits the price difference that arises as a result of market inefficiencies and is usually considered useful to markets, as it helps to re-establish efficiency. However, this opportunity is usually captured by automated trading software, making it nearly impossible for anyone to take advantage of directly.</p><h2 id="the-paradox"><a class="markdownIt-Anchor" href="#the-paradox"></a> The Paradox</h2><h3 id="setup"><a class="markdownIt-Anchor" href="#setup"></a> Setup</h3><p>Say one share of stock is currently worth <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>£</mi><mn>100</mn></mrow><annotation encoding="application/x-tex">£100</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">1</span><span class="mord">0</span><span class="mord">0</span></span></span></span>. On top of this, by some sort of divine wisdom, we know that the share price has a <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mn>2</mn><mn>3</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{2}{3}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-.345em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.845108em"><span style="top:-2.6550000000000002em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> chance of rising to <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>£</mi><mn>120</mn></mrow><annotation encoding="application/x-tex">£120</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">1</span><span class="mord">2</span><span class="mord">0</span></span></span></span> by the next day and a <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mn>1</mn><mn>3</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{3}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-.345em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.845108em"><span style="top:-2.6550000000000002em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> chance of dropping to <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>£</mi><mn>90</mn></mrow><annotation encoding="application/x-tex">£90</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">9</span><span class="mord">0</span></span></span></span>. In reality, the behaviour of the share price would be governed by a statistical distribution. In this case, this post would still be valid, but the numbers wouldn’t be quite as simple to handle.<br><img src="/" class="lazyload" data-src="/images/money-for-nothing/share_price.png"  alt="Illustration of the possible share price changes (Credit: Freepik.com)"><br>A options contract for the same asset has a strike price of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>£</mi><mn>105</mn></mrow><annotation encoding="application/x-tex">£105</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">1</span><span class="mord">0</span><span class="mord">5</span></span></span></span>. This implies that there is a <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mn>2</mn><mn>3</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{2}{3}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-.345em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.845108em"><span style="top:-2.6550000000000002em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> chance of making a profit of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>£</mi><mn>15</mn></mrow><annotation encoding="application/x-tex">£15</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">1</span><span class="mord">5</span></span></span></span> (the difference between the asset selling price and the strike price) and a <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mn>1</mn><mn>3</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{3}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-.345em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.845108em"><span style="top:-2.6550000000000002em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> of neither losing nor gaining money (since the holder wouldn’t exercise the option in this case, as this would lead to losing money).<br><img src="/" class="lazyload" data-src="/images/money-for-nothing/option_value.png"  alt="Illustration of the options contract value (Credit: Freepik.com)"><br>This leaves us to ask what the fair price for such an option would be. The answer to this seems intuitive; if we have a <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mn>2</mn><mn>3</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{2}{3}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-.345em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.845108em"><span style="top:-2.6550000000000002em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> chance of making <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>£</mi><mn>15</mn></mrow><annotation encoding="application/x-tex">£15</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">1</span><span class="mord">5</span></span></span></span>, and a <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mn>1</mn><mn>3</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{3}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-.345em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.845108em"><span style="top:-2.6550000000000002em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> chance of gaining nothing, then on average, we would expect to gain <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mn>2</mn><mn>3</mn></mfrac><mo>×</mo><mi>£</mi><mn>15</mn><mo>+</mo><mfrac><mn>1</mn><mn>3</mn></mfrac><mo>×</mo><mi>£</mi><mn>0</mn><mo>=</mo><mi>£</mi><mn>10</mn></mrow><annotation encoding="application/x-tex">\frac{2}{3} \times £15 + \frac{1}{3} \times £0 = £10</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-.345em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.845108em"><span style="top:-2.6550000000000002em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.77777em;vertical-align:-.08333em"></span><span class="mord mathit">£</span><span class="mord">1</span><span class="mord">5</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-.345em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.845108em"><span style="top:-2.6550000000000002em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">0</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">1</span><span class="mord">0</span></span></span></span>. Therefore, pricing the option at <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>£</mi><mn>10</mn></mrow><annotation encoding="application/x-tex">£10</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">1</span><span class="mord">0</span></span></span></span> seems to be the only logical option.</p><p>This seemed like the obvious answer when I first saw this problem, yet we will see that leads to a paradox. Furthermore, this isn’t some sort of trivial, technical paradox, but rather a way for a bank to exploit us to generate an endless stream of cash.</p><h3 id="breaking-down-the-balance-sheet"><a class="markdownIt-Anchor" href="#breaking-down-the-balance-sheet"></a> Breaking Down the Balance Sheet</h3><p>The process for such exploitation is devilishly simple. The bank starts by selling an option for the price of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>£</mi><mn>10</mn></mrow><annotation encoding="application/x-tex">£10</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">1</span><span class="mord">0</span></span></span></span> that we agreed on above. They follow this by buying a half-stock for <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>£</mi><mn>50</mn></mrow><annotation encoding="application/x-tex">£50</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">5</span><span class="mord">0</span></span></span></span>. We will insist that, by the end of the day, the bank must have a net zero balance sheet, so they also take out a loan. For simplicity, we will assume that the loan is interest-free but the paradox would hold regardless.<br><img src="/" class="lazyload" data-src="/images/money-for-nothing/day_zero.png"  alt="The bank's day zero strategy"></p><div class="note info"><p>Pedants will note that a half-share breaks our definition of a share as the smallest denomination of a stock. This is true, though <a href="https://www.investopedia.com/terms/f/fractionalshare.asp" target="_blank" rel="noopener">fractional share trading</a> lets us get around this. This technically doesn’t affect the logic of the paradox though, and we could simply double the quanity of all shares to avoid it.</p></div><br>By the following day, the share price will have either increased to <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>£</mi><mn>120</mn></mrow><annotation encoding="application/x-tex">£120</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">1</span><span class="mord">2</span><span class="mord">0</span></span></span></span> or decreased to <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>£</mi><mn>90</mn></mrow><annotation encoding="application/x-tex">£90</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">9</span><span class="mord">0</span></span></span></span>. Let’s first consider the former possibility.<p></p><p>In this case, the buyer will exercise the option, so the bank will need to pay out <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>£</mi><mn>15</mn></mrow><annotation encoding="application/x-tex">£15</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">1</span><span class="mord">5</span></span></span></span>. On top of this, the bank has to pay back the loan of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>£</mi><mn>40</mn></mrow><annotation encoding="application/x-tex">£40</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">4</span><span class="mord">0</span></span></span></span>. It’s not all bad news though, as the stock price has now increased, leaving their half-share worth <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>£</mi><mn>60</mn></mrow><annotation encoding="application/x-tex">£60</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">6</span><span class="mord">0</span></span></span></span>. When we tally up the wins and losses, we see that the bank has made a tidy profit of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>£</mi><mn>5</mn></mrow><annotation encoding="application/x-tex">£5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">5</span></span></span></span>.<br><img src="/" class="lazyload" data-src="/images/money-for-nothing/day_one_up.png"  alt="The bank's day one strategy for share price increase"><br>In the case where the price drops to <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>£</mi><mn>90</mn></mrow><annotation encoding="application/x-tex">£90</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">9</span><span class="mord">0</span></span></span></span>, the result is equally auspicious. The buyer won’t exercise the option, so no money is lost there, though the bank still has to pay back the <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>£</mi><mn>40</mn></mrow><annotation encoding="application/x-tex">£40</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">4</span><span class="mord">0</span></span></span></span> loan. The stock price may have dropped, but this still leaves the bank with a half-share worth <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>£</mi><mn>45</mn></mrow><annotation encoding="application/x-tex">£45</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">4</span><span class="mord">5</span></span></span></span>. After crunching the numbers, we should be amazed to see that the bank has again made a profit of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>£</mi><mn>5</mn></mrow><annotation encoding="application/x-tex">£5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">5</span></span></span></span>.<br><img src="/" class="lazyload" data-src="/images/money-for-nothing/day_one_down.png"  alt="The bank's day one strategy for share price increase"><br>There’s no funny-business or mathematical slight-of-hand here; by using this strategy, the bank is <strong>guaranteed</strong> to make a profit (at our expense, no less). Additionally, by repeating this process or scaling up the numbers involved, the bank is left with what is essentially a monetary printing press.</p><p>Thankfully (or sadly, if banking is your thing), this is not how this problem goes in the real world. The chink is this paradox’s armour was our initial assumption of what a fair price for the option would be. Now that we’ve seen the result of such a decision, it is clear that what may have seemed obviously true at first, was in fact incorrect.</p><p>What we failed to account for was the risk associated with the option. Rather than using the raw expected value of the option for our valuation, we should instead use a risk-adjusted version. When we follow this procedure, we see that the correct premium should have been <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>£</mi><mn>5</mn></mrow><annotation encoding="application/x-tex">£5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">5</span></span></span></span>, not <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>£</mi><mn>10</mn></mrow><annotation encoding="application/x-tex">£10</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathit">£</span><span class="mord">1</span><span class="mord">0</span></span></span></span>. We can confirm this by modifying the balance sheets to use this figure, at which point, the bank’s easy profit evaporates.<br><img src="/" class="lazyload" data-src="/images/money-for-nothing/day_zero_adj.png"  alt="Day zero after risk adjustment"><br>We see in this case, that the reduced option price results in the bank requiring a slightly larger loan to equalise losses.<br><img src="/" class="lazyload" data-src="/images/money-for-nothing/day_one_adj.png"  alt="Day one after risk adjustment"><br>This additional loan repayment balances the sheet on the next day so that no profit is made in either scenario.</p><h2 id="closing-remarks"><a class="markdownIt-Anchor" href="#closing-remarks"></a> Closing Remarks</h2><p>This paradox has been overly simplified, and in reality, the forces of supply and demand cut off most opportunity for arbitrage of this form. Nevertheless, the notion of arbitrage is used extensively in practice to price options. For example, in 1973, Fisher Black and Myron Scholes used the method of repeated portfolio replication alongside an arbitrage argument to determine the price of an option. They were awarded with a Nobel Prize for their work, but unfortunately Fischer Black passed away before the award was given.</p><p>I hope that this post has brought to light an interesting application of arbritage and inspired the reader to look further into the topic, especially with regard to its practical applications. Thank you for reading.</p>]]></content>
      
      
      <categories>
          
          <category> Economics </category>
          
          <category> Finance </category>
          
      </categories>
      
      
        <tags>
            
            <tag> lesson </tag>
            
            <tag> puzzle </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Word2Vec: Arithmetic with Words</title>
      <link href="/word2vec/"/>
      <url>/word2vec/</url>
      
        <content type="html"><![CDATA[<div class="note info"><p><strong>Accessing Post Source</strong><br>We are still working on getting this site set up, so source code for this post is not yet available. Check back soon and you’ll be able to find it linked here.</p></div><p>Unsurprisingly, human language seem intuitive to humans, but that is not the case for a computer. On the other hand, though we can easily determine whether two words share a similar meaning, if asked to give a quantitative reason as to why that is the case, we will may struggle to reach a satisfying answer. Thankfully, our shortcoming is where computers thrive, as they are remarkably good with numbers and tabular data. All this leaves us to do is to translate our problem into such a numeric system. Word embedding is a technique to take language and transform it into this computer-friendly format so we can take advantage of computing power for solving natural language processing tasks.</p><h2 id="background"><a class="markdownIt-Anchor" href="#background"></a> Background</h2><h3 id="the-need-for-word2vec"><a class="markdownIt-Anchor" href="#the-need-for-word2vec"></a> The Need for Word2vec</h3><p>Initial attempts at solving this problem before word embeddings included one-hot encoding. The essence of this process is to represent our text as a sequence of vectors in a space with number of dimensions equal to the number of words in our corpus. We assign each word its own dimension so that each word vector is orthogonal to the others. Technical details aside, this isn’t practical to model anything on the scale of human languages, as the vector dimensions would be the size of the language (and as a results, storage and computational requirements would be astronomical). More importantly, however, this doesn’t capture any sort of similarity between two words (since ever vector is mutually orthogonal to the others). As far as this method is concerned, the words ‘dolphin’ and ‘neoliberal’ are equally similar to ‘shark’. Word2vec aims to solve this problem by providing word embedding which take into account relations between words. In essence, word2vec provides a canvas (albeit a strange multi-dimensional one) where any possible word in the language could lie, and plots points on this canvas for each word in our corpus. How close any two points on this canvas lie (captured mathematically by the cosine distance) should therefore correspond to how likely humans are to describe the respresented words as “similar”.</p><p><img src="/" class="lazyload" data-src="/images/word2vec/word2vec_6_0.png"  alt=""></p><h3 id="deriving-the-word-embeddings"><a class="markdownIt-Anchor" href="#deriving-the-word-embeddings"></a> Deriving the Word Embeddings</h3><p>To derive each word embedding, the word2vec model is usually trained using a method called Skipgram with Negative Sampling (SGNS). Essentially, a large corpus (typically billions of words) is fed to the model, and an <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathdefault">n</span></span></span></span>-sized sliding window is used to capture the words that lie either side of each word in the corpus to determine each word’s context. The context for each word is then used to determine the word’s embedding vector, with the negative sampling process controlling the rate at which these weights are updated to reduce computation time and produce a more robust result. Because words with a similar context usually have closely-linked meanings, such words will end up having similar embedding vectors too.</p><p><img src="/" class="lazyload" data-src="/images/word2vec/sliding_window.png"  alt="The sliding window used in word2vec training"><br>Take the above diagram as an example. On iteration <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.85396em;vertical-align:-.19444em"></span><span class="mord mathdefault" style="margin-right:.05724em">j</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathdefault" style="margin-right:.03148em">k</span></span></span></span>, ‘fox’ and ‘bear’ have similar contexts, so will end up with relatively close embedding vectors. After many adjustments each time they are found in the corpus, their vectors will provide an increasingly accurate represention of the “fox” and “bear” relation—types of animals.</p><h2 id="application"><a class="markdownIt-Anchor" href="#application"></a> Application</h2><div class="note info"><p>The following examples are derived from a word2vec model trained on the <a href="https://code.google.com/archive/p/word2vec/" target="_blank" rel="noopener">Google News dataset</a>, featuring over 100 billion words taken from various news articles. The trained model is stored an object called <code>model</code> which we can query for results.</p></div><p>Once we have trained a word embedding using word2vec, we can apply it in many different ways to extract the relationships between words in the corpus.</p><p>For example, we can find the similarity between words based on their cosine distance in the vector space.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.similarity(<span class="string">'queen'</span>, <span class="string">'throne'</span>)</span><br></pre></td></tr></table></figure><pre><code>0.45448625</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.similarity(<span class="string">'queen'</span>, <span class="string">'forklift'</span>)</span><br></pre></td></tr></table></figure><pre><code>-0.030027825</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.similarity(<span class="string">'Queen'</span>, <span class="string">'Bowie'</span>)</span><br></pre></td></tr></table></figure><pre><code>0.20833209</code></pre><div class="note warning"><p>Since we are measuring similarity using the cosine distance, values will range from -1 to 1. Words with a similarity near 1 are likely to be extremely similar, words with a similarity of 0 have little in common, and words with similarity near -1 <em>should</em> be opposites (though we’ll later see that this doesn’t always work)</p></div><p>As expected, the word ‘forklift’ is relatively distinct from ‘queen’, especially when compared to ‘throne’. What’s fascinating, however, is that multiple facets of the word ‘queen’ are captured; we see that ‘Bowie’ is also relatively close to ‘Queen’ due to the word’s relation to the iconic rock band.</p><p>Naturally, with vectors come mathematical operations, and the real power of word2vec starts to emerge. Vector differences are the crux behind <em>analogies</em>, a concept best explained through examples…</p><h3 id="analogies"><a class="markdownIt-Anchor" href="#analogies"></a> Analogies</h3><h4 id="starting-with-a-classic"><a class="markdownIt-Anchor" href="#starting-with-a-classic"></a> Starting With a Classic</h4><p>The most infamous example of the use of word2vec is answering the question, “Man is to woman, as king is to…what?”. As we can see, word2vec takes this puzzle in it’s stride.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># king + (woman - man) = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'king'</span>, <span class="string">'woman'</span>], negative=[<span class="string">'man'</span>], topn=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>queen (0.712)</code></pre><p>This example is rather intuitive; the female version of the male title ‘king’ is ‘queen’ and so this is the natural choice to complete the analogy. To get word2vec to return this result, we have to phrase the question in the language of arthimetic; that is, <code>king + (woman - man)</code>. In other words, we are taking the word ‘king’ and asking what the corresponding word would be if added the difference between ‘woman’ and ‘man’. This may seem unituitive—why can’t we just add ‘woman’ to ‘king’? The reason for this is that the word ‘king’ already has ‘man’ as a component of its vector representation. Therefore, if we simply added ‘woman’ without first subtracting ‘man’ we end up with components of both ‘woman’ and ‘man’ which confuses the model, leaving us with nonsensical results.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Invalid approach: king + woman = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'king'</span>, <span class="string">'woman'</span>], topn=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>man (0.663)</code></pre><h3 id="plurals"><a class="markdownIt-Anchor" href="#plurals"></a> Plurals</h3><p>We can use this system of analogy solving for finding the singular and plural forms of words. With a rather mundane example such as <code>gloves + (bike - bikes)</code>, it’s not unsurprising the model returns ‘glove’; it could simply be obtained from deciphering that the pattern is removing the trailing ‘s’—hardly groundbreaking. But when talking about irregular plurals, the required task to output the derived word shifts from spotting a simple pattern to seemingly needing a human-like understanding of the structure and complexities of the English language. Never-the-less, word2vec is up for the challenge.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># foot + (cacti - cactus) = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'foot'</span>, <span class="string">'cacti'</span>], negative=[<span class="string">'cactus'</span>], topn=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>feet (0.568)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># child + (sheep - sheep) = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'child'</span>, <span class="string">'sheep'</span>], negative=[<span class="string">'sheep'</span>], topn=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>children (0.726)</code></pre><p>Here, ‘sheep’ is both the singular and the plural, meaning the result of the word arithmetic is still ‘child’. But since ‘child’ is such a similar word to ‘children’, word2vec still manages to come out with the correct answer.</p><h3 id="geographical-analogies"><a class="markdownIt-Anchor" href="#geographical-analogies"></a> Geographical analogies</h3><p>We can use analogies to find cities.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Portugal + (Moscow - Russia) = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'Portugal'</span>, <span class="string">'Moscow'</span>], negative=[<span class="string">'Russia'</span>], topn=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>Lisbon (0.655)</code></pre><p>Or we can flip things around to find what country a city resides in.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Delhi + (Spain - Barcelona) = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'Delhi'</span>, <span class="string">'Spain'</span>], negative=[<span class="string">'Barcelona'</span>], topn=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>India (0.626)</code></pre><p>Finally, we can go one step up and find the geographic regions of countries.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Cambodia + (Africa - Egypt) = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'Cambodia'</span>, <span class="string">'Africa'</span>], negative=[<span class="string">'Egypt'</span>], topn=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>Southeast_Asia (0.566)</code></pre><p>The geographic intelligence of word2vec isn’t limited to the form of analogy. Here we see an example in which we perform straight addition.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Iran + war = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'Iran'</span>, <span class="string">'war'</span>],topn=<span class="number">3</span>))</span><br></pre></td></tr></table></figure><pre><code>Iraq (0.683)Islamic_republic (0.671)Syria (0.653)</code></pre><p>This example shows how much geographic and political complexity is captured in the model. ‘Iraq’ and ‘Islamic_republic’ are most likely referencing the Iran-Iraq war. On the other hand Iraq and Syria, are both war-stricken countries near Iran, which could easily explain this relation.</p><h3 id="opposites"><a class="markdownIt-Anchor" href="#opposites"></a> Opposites</h3><p>When a word has a clear opposite, we can use analogy to find it.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># high + (big - small)</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'high'</span>, <span class="string">'big'</span>], negative=[<span class="string">'small'</span>], topn=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>low (0.448)</code></pre><p>Note however that we can’t just negate a word to find its opposite or we obtain gibberish in return.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -high</span></span><br><span class="line">print_similar(model.most_similar(negative=[<span class="string">'high'</span>], topn=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>----------_-----------------------------------------------_GS## (0.321)</code></pre><p>The reason for this is that an opposite word in a vector space has to be opposite in every way. Even though we would say that ‘high’ and ‘low’ are opposites, they do in fact have components in common, such as how they both represent heights. For that reason the model stubbornly ignores words that are opposites in the way we intend, and instead, tries to find the word that is most dissimilar to ‘high’, resulting in some strange garble of characters.</p><h3 id="vector-sums-and-differences"><a class="markdownIt-Anchor" href="#vector-sums-and-differences"></a> Vector Sums and Differences</h3><p>As hinted at before, word2vec can solve problems far more general than analogies. Here we look at some examples of generic vector sums and differences.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># death + water = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'death'</span>, <span class="string">'water'</span>], topn=<span class="number">3</span>))</span><br></pre></td></tr></table></figure><pre><code>drowning (0.556)drowing (0.545)scalding_bath (0.526)</code></pre><div class="note info"><p>It appears that the second most similar term has picked up on a common typo of ‘drowning’. The joys of real world data…</p></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># death + knife = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'death'</span>, <span class="string">'knife'</span>], topn=<span class="number">3</span>))</span><br></pre></td></tr></table></figure><pre><code>kitchen_knife (0.644)stabbing (0.637)murder (0.634)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># girlfriend - love = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'girlfriend'</span>], negative=[<span class="string">'love'</span>], topn=<span class="number">3</span>))</span><br></pre></td></tr></table></figure><pre><code>ex_girlfriend (0.517)fiancee (0.479)estranged_wife (0.476)</code></pre><p>The second result is rather strange. If you have a theory of where this relation might have come from, make sure to comment below.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># colleague + love = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'colleage'</span>, <span class="string">'love'</span>], topn=<span class="number">3</span>))</span><br></pre></td></tr></table></figure><pre><code>loved (0.580)friend (0.551)pal (0.542)</code></pre><p>With the last example we can see a shortcoming of the word2vec model. It appears that ‘love’ has a much stronger vector representation than ‘collegue’; that is, the term captures more complexity, which makes sense. For this reason, the term ‘love’ can overpower the sum so that a word similar to ‘love’, ‘loved’ can be returned as highly similar even though it doesn’t relate much to the word ‘collegue’. Despite this, the other two preditions are strong.</p><h3 id="miscellaneous"><a class="markdownIt-Anchor" href="#miscellaneous"></a> Miscellaneous</h3><p>To wrap up our examples, we will look at some miscellaneous analogies involving people and places.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Obama + (Russia - USA) = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'Obama'</span>, <span class="string">'Russia'</span>], negative=[<span class="string">'USA'</span>], topn=<span class="number">3</span>))</span><br></pre></td></tr></table></figure><pre><code>Medvedev (0.674)Putin (0.647)Kremlin (0.617)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UK + (Hitler - Germany) = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'UK'</span>, <span class="string">'Hitler'</span>], negative=[<span class="string">'Germany'</span>], topn=<span class="number">3</span>))</span><br></pre></td></tr></table></figure><pre><code>Tony_Blair (0.522)Oliver_Cromwell (0.509)Maggie_Thatcher (0.506)</code></pre><p>Make of the above what you will…</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Apple + (Gates - Microsoft) = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'Apple'</span>, <span class="string">'Gates'</span>], negative=[<span class="string">'Microsoft'</span>], topn=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>Steve_Jobs (0.523)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Victoria Beckham + (Barack Obama - Michelle) = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'Victoria_Beckham'</span>, <span class="string">'Barack_Obama'</span>], negative=[<span class="string">'Michelle'</span>], topn=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>David_Beckham (0.528)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Manchester + (Anfield - Liverpool) = ...</span></span><br><span class="line">print_similar(model.most_similar(positive=[<span class="string">'Manchester'</span>, <span class="string">'Anfield'</span>], negative=[<span class="string">'Liverpool'</span>], topn=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>Old_Trafford (0.765)</code></pre><h2 id="word2vec-in-the-wild"><a class="markdownIt-Anchor" href="#word2vec-in-the-wild"></a> Word2Vec in the Wild</h2><p>Above, we have seen some fairly isolated applications of the word2vec model, but that is not to say that there are not wider reaching use cases. For example, word2vec is often a key step in the production of sentiment analysis models (See: <a href="https://youtu.be/l40-JFn6F9M?t=1845" target="_blank" rel="noopener">a WDSS virtual talk on the use of sentiment analysis for predicting presidential approval</a>), recommender systems, and chat bots. Aside from these ecommerce-centric examples, word2vec has also flurished in scientific applications such as BioNLP, which have utilised word embeddings for advancements in knowledge.</p><p>Hopefully, through these examples, the potential power of Word2Vec has been made clear. Thank you reading.</p>]]></content>
      
      
      <categories>
          
          <category> Computer Science </category>
          
          <category> Natural Language Processing </category>
          
      </categories>
      
      
        <tags>
            
            <tag> lesson </tag>
            
            <tag> word-embedding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Higher or Lower: Reinventing a Classic Card Game</title>
      <link href="/higher-or-lower/"/>
      <url>/higher-or-lower/</url>
      
        <content type="html"><![CDATA[<div class="note info"><p>This post is the corresponding write-up for a WDSS project in which a small team of society members collaborated to produce a web-toy that plays a game of Higher or Lower using the Twitter follower counts of celebrities. You can play this game at <a href="https://shiny.warwickdatascience.com/higher-or-lower/" target="_blank" rel="noopener">this link</a>.</p></div><h2 id="motivation"><a class="markdownIt-Anchor" href="#motivation"></a> Motivation</h2><p>Sometimes, simplicity is beautiful. Higher or Lower is a game embodying this philosophy. Played solo with a standard deck of cards, play consists of revealing these one at a time after first guessing whether the next card will have a higher or lower value. In recent years, this game has been re-envisioned as a <a href="http://www.higherlowergame.com/" target="_blank" rel="noopener">popular web toy</a>, in which card values are replaced by the number of global monthly Google searches for various topics. Not wishing to limit ourselves to search results, we decided to implement our own online version of the classic game based on the follower counts of Twitter celebrities. The final app can be found at the link above, and we will spend the rest of this post looking into the techniques behind our approach as well as reviewing the lessons this project can teach us about collaborative data science at WDSS.</p><p>For our purposes, this project is an ideal medium to practise web scraping and creating sharable products for others to enjoy. Web scraping is a way of extracting data from websites, leveraging automation to gather information efficiently and without unnecessary repetition. In all, three members of WDSS worked on this project, combining their specific skills to develop the final product. <a href="https://www.linkedin.com/in/tim-hargreaves/" target="_blank" rel="noopener">Tim Hargreaves</a>, focused on the backbone of the app, <a href="https://www.linkedin.com/in/mhbardsley/" target="_blank" rel="noopener">Matthew Bardsley</a>, the visuals of the game, and I (<a href="https://www.linkedin.com/in/parthdevalia/" target="_blank" rel="noopener">Parth Devalia</a>) have responsibility for the communication of results.</p><h2 id="implementation"><a class="markdownIt-Anchor" href="#implementation"></a> Implementation</h2><div class="note info"><p>The source code for the web app and scraping scripts have been open-sourced in <a href="https://github.com/warwickdatascience/higher-or-lower" target="_blank" rel="noopener">this repository</a>.</p></div><p>With 330 million monthly users, Twitter has become an indispensable medium for instant news and opinions from politicians, brands, and of course, celebrities. Manually defining celebrityhood and iterating through matching accounts would be a difficult task, so we decided to look at what existing resources we could take advantage of. We eventually settled on a website called <a href="http://profilerehab.com/twitter-help/celebrity_twitter_list" target="_blank" rel="noopener">ProfileRehab</a>. On this site, links to celebrities’ Twitter accounts are sorted into categories. By scraping this information we were able to collect Twitter profile URLs for around five hundred celebrities, matched to their names. We then interfaced with the Twitter API to read their respective follower counts and download their profile pictures. This entire scraping process was performed using Python, to take advantage of the rich ecosystem of web scraping packages the language has.</p><p>You may well be asking, “What is an API?”, and so I will take a moment to introduce this term. Application Programming Interfaces (APIs) simply allow applications to communicate with each other and are responsible for much of the connectivity we rely upon. They act as messengers, taking your request, telling the target application what you want to do, and then returning the response. Rather than accessing the application server directly, APIs offer us a dedicated access point, improving security and reliability. A common analogy is that of a restaurant—the API is the waiter, the interface between your table and the kitchen, taking your request and returning the response (the food).</p><p>With regard to our project, we decided to access the data we wanted through an API as Twitter have made recent obfuscations to their website code to make direct scraping more difficult. Use of Twitter’s API, as is often the case, is subject to terms and conditions regarding the usage of the data obtained. Additionally, the company have implemented a rate limit; that is, a maximum number of requests they can handle in a given timespan (just like with a restaurant waiter). Careful examination of these limitations needs to be considered when using an API, but fortunately we found them to be adequate for our needs.</p><p>The application is made using Shiny, a package for the R programming language that allows you to build interactive web applications. The framework allows for the development of powerful and flexible web applications with no need for HTML, CSS or JavaScript knowledge. For this reason, Shiny stands out for its unrivalled speed of development.</p><p>Despite its benefits, the raw product of Shiny development is not always the prettiest and can lack strong mobile support. To overcome this, Matthew implemented additional styling using CSS to improve the aesthetics of the final application.</p><h2 id="takeaways"><a class="markdownIt-Anchor" href="#takeaways"></a> Takeaways</h2><p>This project allowed us three WDSS members to work together in creating something that we wouldn’t have done individually. Further, if not for the society, we would not have had the opportunity to work together. This highlights the role of WDSS, bringing people from different backgrounds together to solve challenging problems.</p><p>Projects are extremely important in growing your skills and are critical for developing a strong portfolio. Through collaboration, we can see problems from new perspectives, build our professional networks, and gain experience of working in a team. This is in comparison to university work, that is often done alone without using real world data, and usually without a solid final product.</p><p>This project leverages infrastructure offered by WDSS, such as our blogging platform and Shiny server. For this reason, alongside the support<br>offered by experienced students, working with WDSS to complete research makes it easier to get projects off the ground and showcase what you can do.</p><p>Thank you for reading and we hope you enjoy playing our implementation of Higher or Lower.</p>]]></content>
      
      
      <categories>
          
          <category> Computer Science </category>
          
          <category> Web </category>
          
      </categories>
      
      
        <tags>
            
            <tag> shiny </tag>
            
            <tag> web-scraping </tag>
            
            <tag> game </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Urban Cities: A History Told By Data</title>
      <link href="/urban-cities/"/>
      <url>/urban-cities/</url>
      
        <content type="html"><![CDATA[<div class="note info"><p><strong>Accessing Post Source</strong></p><p>We are still working on getting this site set up, so source code for this post is not yet available. Check back soon and you’ll be able to find it linked here.</p></div><h2 id="introduction"><a class="markdownIt-Anchor" href="#introduction"></a> Introduction</h2><p><img src="/" class="lazyload" data-src="/images/urban-cities/barcelona.jpg"  alt="The skyline of Barcelona"><br>An aerial observer would be forgiven for mistaking Barcelona’s octagonal blocks and diagonal streets as some red-brick reimagining of Legoland. Indeed, Spain’s 2000-year-old capital is a strict grid-like design, engineered to tackle overpopulation while maximizing airflow for its inhabitants. It’s an ancient city with all the efficiency of contemporary urban structures like New York.</p><p>Barcelona is a fascinating example, but its grid-like patterns are obvious to the human eye. I wondered about other cities with more complex features. What subtle quirks lie in the road/street structures of Bristol, Newcastle or Coventry?</p><p>For that question I wanted a scientific answer, so I set about applying data science to learn more about the intricacies of the UK’s densely populated cities.</p><h3 id="methodology"><a class="markdownIt-Anchor" href="#methodology"></a> Methodology</h3><p>This project was made possible by <a href="https://www.openstreetmap.org/" target="_blank" rel="noopener">OpenStreetMap</a> (OSM). OSM is a world-wide database of roads, trails and streets, verified by field maps and aerial imagery and maintained by an active community of engineers and GIS professionals. Its open API enables raw geodata to be sourced on any city in the world.</p><p>OSM describes geodata elements in a number of ways. Linear features, like roads or rivers, are modelled using a “way”: nodes (between 2 and 2000) connected into a simple chain of line segments, a polyline. Most whole cities with solid polygon geometries (Manchester and Birmingham to name a few) are bounded by a single “closed way”, with the same start and end nodes. These are generally those with a well-defined natural border in the real world. Others, that have no obvious boundary, are rather represented as a single point marker, denoting the center of the city. We will be focusing on the former type as the more complex definition of the geometry allows us to draw far more insight.</p><p>Of the UK cities with these ‘nice’ geometries, the twelve with the largest populations were considered and OSM API was used to fetch full network graphs, from which the bearings of streets were determined. A summary of this data is shown below.</p><table><thead><tr><th></th><th>geometry</th><th>place_name</th><th>bbox_north</th><th>bbox_south</th><th>bbox_east</th><th>bbox_west</th></tr></thead><tbody><tr><th>0</th><td>POLYGON ((-2.03365 52.40231, -2.03322 52.40217...</td><td>Birmingham, West Midlands Combined Authority, ...</td><td>52.608706</td><td>52.381053</td><td>-1.728858</td><td>-2.033649</td></tr><tr><th>1</th><td>POLYGON ((-1.80042 53.88595, -1.80041 53.88594...</td><td>Leeds, Yorkshire and the Humber, England, Unit...</td><td>53.945872</td><td>53.698968</td><td>-1.290352</td><td>-1.800421</td></tr><tr><th>2</th><td>POLYGON ((-1.80147 53.48098, -1.80049 53.48027...</td><td>Sheffield, Yorkshire and the Humber, England, ...</td><td>53.503104</td><td>53.304512</td><td>-1.324669</td><td>-1.801471</td></tr><tr><th>3</th><td>POLYGON ((-2.06125 53.82562, -2.06017 53.82504...</td><td>Bradford, Yorkshire and the Humber, England, U...</td><td>53.963151</td><td>53.724341</td><td>-1.640330</td><td>-2.061248</td></tr><tr><th>4</th><td>POLYGON ((-2.31992 53.41161, -2.31847 53.40999...</td><td>Manchester, Greater Manchester, North West Eng...</td><td>53.544592</td><td>53.340104</td><td>-2.146829</td><td>-2.319918</td></tr><tr><th>5</th><td>POLYGON ((-3.01917 53.43616, -3.01806 53.43323...</td><td>Liverpool, North West England, England, United...</td><td>53.474967</td><td>53.311543</td><td>-2.818000</td><td>-3.019173</td></tr><tr><th>6</th><td>POLYGON ((-2.71837 51.50617, -2.71837 51.50616...</td><td>Bristol, City of Bristol, South West England, ...</td><td>51.544432</td><td>51.397284</td><td>-2.510419</td><td>-2.718370</td></tr><tr><th>7</th><td>POLYGON ((-1.62490 53.65363, -1.62488 53.65358...</td><td>Wakefield, Yorkshire and the Humber, England, ...</td><td>53.741811</td><td>53.575349</td><td>-1.198814</td><td>-1.624898</td></tr><tr><th>8</th><td>POLYGON ((-1.61446 52.42795, -1.61412 52.42774...</td><td>Coventry, West Midlands Combined Authority, We...</td><td>52.464772</td><td>52.363885</td><td>-1.423957</td><td>-1.614459</td></tr><tr><th>9</th><td>POLYGON ((-1.24696 52.95344, -1.24689 52.95317...</td><td>City of Nottingham, East Midlands, England, Un...</td><td>53.018672</td><td>52.889008</td><td>-1.086119</td><td>-1.246956</td></tr><tr><th>10</th><td>POLYGON ((-1.77567 54.98962, -1.77566 54.98955...</td><td>Newcastle upon Tyne, Tyne and Wear, North East...</td><td>55.079382</td><td>54.959032</td><td>-1.529200</td><td>-1.775672</td></tr><tr><th>11</th><td>POLYGON ((-1.56888 54.92462, -1.56824 54.92409...</td><td>Sunderland, Tyne and Wear, North East England,...</td><td>54.944170</td><td>54.799042</td><td>-1.345665</td><td>-1.568879</td></tr></tbody></table><h2 id="visualizations"><a class="markdownIt-Anchor" href="#visualizations"></a> Visualizations</h2><p>For each of these 12 cities, the returned bearings were weighted by street length to produce the following visualisations using simple polar projection plots. It is important to note two key points about this approach:</p><ol><li>The bearings of streets were derived from only their start and finish nodes, ignoring paths in-between. This was to ensure reasonable computation times as an alternative to using the full polylines.</li><li>Naturally, bearings are rotationally symmetric as we do not account for the direction of one-way streets.</li></ol><h3 id="birmingham"><a class="markdownIt-Anchor" href="#birmingham"></a> Birmingham</h3><p><img src="/" class="lazyload" data-src="/images/urban-cities/urban-cities_11_0.png"  alt=""></p><p>Birmingham’s visualization is unmistakably circular. Many older cities lack a grid structure, with impromptu-built streets going off in many different directions. The place now called “Birmingham” has been around for more than 1,400 years. It was believed to have been established by a Saxon tribe, before expanding over the centuries into the city of 8500 streets we know today.</p><p><img src="/" class="lazyload" data-src="/images/urban-cities/prospect_of_birmingham.jpg"  alt="William Westley's 1732 Prospect of Birmingham"></p><h3 id="manchester-newcastle"><a class="markdownIt-Anchor" href="#manchester-newcastle"></a> Manchester &amp; Newcastle</h3><p><img src="/" class="lazyload" data-src="/images/urban-cities/urban-cities_15_0.png"  alt=""></p><p>Manchester displays quite a clear cross-like visualization. This city has an interesting grid structure with a strong emphasis on moving north-to-south. The east-to-west flow is perhaps due to Manchester sitting almost directly between Liverpool and Sheffield, and the reduced SE/NW activity might result from its position just in the upper left of the Peak District.</p><p>Newcastle is similar but for a more obvious reason; its central ring-road system, which is vaguely hexagonal, is presumably responsible for the high degree of symmetry we observe.</p><h3 id="coventry"><a class="markdownIt-Anchor" href="#coventry"></a> Coventry</h3><p><img src="/" class="lazyload" data-src="/images/urban-cities/urban-cities_18_0.png"  alt=""></p><p>Not unlike Manchester, Coventry’s visualization hints at a grid-like design. Famously, this ancient city—once a hotspot of trade for cloth and textiles—was obliterated in 1940 by a series of bombing raids, now called the Coventry Blitz. In the decades following, the city’s remains were rebuilt into a modern grid structure.</p><h3 id="bristol"><a class="markdownIt-Anchor" href="#bristol"></a> Bristol</h3><p><img src="/" class="lazyload" data-src="/images/urban-cities/urban-cities_21_0.png"  alt=""></p><p>For a city founded on the turn of the second-last millennia, we wouldn’t expect to see much beyond a uniformly distributed set of bearings. Yet Bristol’s otherwise almost-circular plot is cut along the NE/SW line. Modern Bristol is dominated by the M5, as well as the River Avon. It’s interesting that a 1960s motorway construction can have such an impact on the bearings of an ancient city.</p><h2 id="final-thoughts"><a class="markdownIt-Anchor" href="#final-thoughts"></a> Final Thoughts</h2><p>As well as analyzing the street orientations of individual cities, it is beneficial to to showcase their visualizations side-by-side to draw comparisons and appreciate relative differences. To extend, I have collated the visualizations for the twelve cities I considered into one final image.</p><p><img src="/" class="lazyload" data-src="/images/urban-cities/urban-cities_25_0.png"  alt=""></p><h3 id="the-importance-of-data-science"><a class="markdownIt-Anchor" href="#the-importance-of-data-science"></a> The Importance of Data Science</h3><p>I think it important to observe how data science, once a niche and theoretical discipline, can offer such rich insights into the history of the UK’s major cities. From Birmingham’s pre-industrial beginnings, to the devastating campaigns of the German Luftwaffe, these simple visualisations tell stories of our past. They compress a millennia-long archive of the achievements and failures that gave rise to modern Britain.</p><p>To that extent, if you are a data scientist wondering how you can apply your technical skills to real-world projects, or a student with domain expertise, keen to build up the technical skills to answer the questions you care about, make sure you follow Warwick Data Science Society <a href="https://www.facebook.com/warwickdatascience" target="_blank" rel="noopener">on social media</a> to keep up-to-date with relevant opportunties. These include academic talks, data science news, workshops, and beginners programming courses, so there is certainly something for everyone. Thank you for reading this piece.</p>]]></content>
      
      
      <categories>
          
          <category> Humanities </category>
          
          <category> Geography </category>
          
          <category> History </category>
          
      </categories>
      
      
        <tags>
            
            <tag> visualization </tag>
            
            <tag> data-analysis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>What Factors Actually Affect Your Grades?</title>
      <link href="/school-success/"/>
      <url>/school-success/</url>
      
        <content type="html"><![CDATA[<div class="note info"><p><strong>Accessing Post Source</strong><br>We are still working on getting this site set up, so source code for this post is not yet available. Check back soon and you’ll be able to find it linked here.</p></div><p>With exam period approaching fast, every student is wondering how to score the best possible grade. Some factors—like how much sleep you’re getting or how healthy you are—seem to have an obvious correlation with your final grade. What about your relationship status? How much should you be studying to achieve the grade you want? Does the subject you’re studying influence your final grade? In this article, we will use two datasets containing student math and Portuguese language performance in two different Portuguese schools and see which factors affected student performance the most.</p><h2 id="exploratory-data-analysis"><a class="markdownIt-Anchor" href="#exploratory-data-analysis"></a> Exploratory Data Analysis</h2><h3 id="dataset-overview"><a class="markdownIt-Anchor" href="#dataset-overview"></a> Dataset Overview</h3><p>The variables are the same for the two datasets:</p><table><thead><tr><th style="text-align:center">Variable</th><th style="text-align:center">Description</th><th style="text-align:center">Type</th><th style="text-align:center">Possible Values</th></tr></thead><tbody><tr><td style="text-align:center">school</td><td style="text-align:center">School</td><td style="text-align:center">binary</td><td style="text-align:center">GP—Gabriel Pereira; MS—Mousinho da Silveira</td></tr><tr><td style="text-align:center">sex</td><td style="text-align:center">Sex</td><td style="text-align:center">binary</td><td style="text-align:center">F—female; M—male</td></tr><tr><td style="text-align:center">age</td><td style="text-align:center">Age</td><td style="text-align:center">numeric</td><td style="text-align:center">15–22, inclusive</td></tr><tr><td style="text-align:center">address</td><td style="text-align:center">Address type</td><td style="text-align:center">binary</td><td style="text-align:center">U—Urban; R—Rural</td></tr><tr><td style="text-align:center">famsize</td><td style="text-align:center">Family Size</td><td style="text-align:center">binary</td><td style="text-align:center">LE3—less than or equal to 3; GE3—greater than 3</td></tr><tr><td style="text-align:center">Pstatus</td><td style="text-align:center">Parent’s cohabitation status</td><td style="text-align:center">binary</td><td style="text-align:center">T—living together; A—living apart</td></tr><tr><td style="text-align:center">Medu</td><td style="text-align:center">Mother’s Education</td><td style="text-align:center">ordinal</td><td style="text-align:center">0—none; 1—up to 4th grade; 2—5th–9th grade; 3—secondary; 4—higher</td></tr><tr><td style="text-align:center">Fedu</td><td style="text-align:center">Father’s Education</td><td style="text-align:center">ordinal</td><td style="text-align:center">0—none; 1—up to 4th grade; 2—5th–9th grade; 3—secondary; 4—higher</td></tr><tr><td style="text-align:center">Mjob</td><td style="text-align:center">Mother’s Job</td><td style="text-align:center">nominal</td><td style="text-align:center">teacher; health(-care related); (civil )services; at home; other</td></tr><tr><td style="text-align:center">Fjob</td><td style="text-align:center">Father’s Job</td><td style="text-align:center">nominal</td><td style="text-align:center">teacher; health(-care related); (civil )services; at home; other</td></tr><tr><td style="text-align:center">reason</td><td style="text-align:center">Reason for choosing school</td><td style="text-align:center">nominal</td><td style="text-align:center">(close to )home; (school )reputation; course(preference); other</td></tr><tr><td style="text-align:center">guardian</td><td style="text-align:center">Student’s guardian</td><td style="text-align:center">nominal</td><td style="text-align:center">mother; father; other</td></tr><tr><td style="text-align:center">traveltime</td><td style="text-align:center">Travel time to school</td><td style="text-align:center">ordinal</td><td style="text-align:center">1—&lt;15 min.; 2—15–30 min.; 3—30 min.–1 hour; 4—&gt;1 hour</td></tr><tr><td style="text-align:center">studytime</td><td style="text-align:center">Weekly study time</td><td style="text-align:center">ordinal</td><td style="text-align:center">1—&lt;2 hours; 2—2–5 hours; 3—5–10 hours; 4—&gt;10 hours</td></tr><tr><td style="text-align:center">failures</td><td style="text-align:center">Past class failures</td><td style="text-align:center">numeric</td><td style="text-align:center">0–3, else 4</td></tr><tr><td style="text-align:center">schoolsup</td><td style="text-align:center">Extra educational support</td><td style="text-align:center">binary</td><td style="text-align:center">yes; no</td></tr><tr><td style="text-align:center">famsup</td><td style="text-align:center">Family educational support</td><td style="text-align:center">binary</td><td style="text-align:center">yes; no</td></tr><tr><td style="text-align:center">paid</td><td style="text-align:center">Extra paid classes</td><td style="text-align:center">binary</td><td style="text-align:center">yes; no</td></tr><tr><td style="text-align:center">activities</td><td style="text-align:center">Extra-curricular activities</td><td style="text-align:center">binary</td><td style="text-align:center">yes; no</td></tr><tr><td style="text-align:center">nursery</td><td style="text-align:center">Attend nursery</td><td style="text-align:center">binary</td><td style="text-align:center">yes; no</td></tr><tr><td style="text-align:center">higher</td><td style="text-align:center">Wants to take higher education</td><td style="text-align:center">binary</td><td style="text-align:center">yes; no</td></tr><tr><td style="text-align:center">internet</td><td style="text-align:center">Home internet access</td><td style="text-align:center">binary</td><td style="text-align:center">yes; no</td></tr><tr><td style="text-align:center">romantic</td><td style="text-align:center">In a romantic relationship</td><td style="text-align:center">binary</td><td style="text-align:center">yes; no</td></tr><tr><td style="text-align:center">famrel</td><td style="text-align:center">Quality of family relationships</td><td style="text-align:center">ordinal</td><td style="text-align:center">1—very bad to 5—very good</td></tr><tr><td style="text-align:center">freetime</td><td style="text-align:center">Free time after school</td><td style="text-align:center">ordinal</td><td style="text-align:center">1—very low to 5—very high</td></tr><tr><td style="text-align:center">goout</td><td style="text-align:center">Going out with friends</td><td style="text-align:center">ordinal</td><td style="text-align:center">1—very low to 5—very high</td></tr><tr><td style="text-align:center">Dalc</td><td style="text-align:center">Workday alcohol consumption</td><td style="text-align:center">ordinal</td><td style="text-align:center">1—very low to 5—very high</td></tr><tr><td style="text-align:center">Walc</td><td style="text-align:center">Weekend alcohol consumption</td><td style="text-align:center">ordinal</td><td style="text-align:center">1—very low to 5—very high</td></tr><tr><td style="text-align:center">health</td><td style="text-align:center">Current health status</td><td style="text-align:center">ordinal</td><td style="text-align:center">1—very bad to 5—very good</td></tr><tr><td style="text-align:center">absences</td><td style="text-align:center">Number absences</td><td style="text-align:center">numeric</td><td style="text-align:center">0–93</td></tr><tr><td style="text-align:center">G1</td><td style="text-align:center">First Period Grade</td><td style="text-align:center">numeric</td><td style="text-align:center">0–20</td></tr><tr><td style="text-align:center">G2</td><td style="text-align:center">Second Period Grade</td><td style="text-align:center">numeric</td><td style="text-align:center">0–20</td></tr><tr><td style="text-align:center">G3</td><td style="text-align:center">Final Grade</td><td style="text-align:center">numeric</td><td style="text-align:center">0–20</td></tr></tbody></table><p>I will be conducting a basic analysis of the dataset followed by visualizations of the correlations between different factors. Finally, I will build a linear regression model for each subject to predict the students’ final grades.</p><p>We will start by importing all the necessary packages and load the datasets into a pandas dataframe.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import necessary packages</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd  </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np  </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> statistics <span class="keyword">as</span> stats</span><br><span class="line"><span class="keyword">import</span> statsmodels.api <span class="keyword">as</span> sm</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the dataset from the csv file using pandas</span></span><br><span class="line">data_m = pd.read_csv(<span class="string">r'data/student-mat.csv'</span>, sep=<span class="string">';'</span>)</span><br><span class="line">data_p = pd.read_csv(<span class="string">r'data/student-por.csv'</span>, sep=<span class="string">';'</span>)</span><br></pre></td></tr></table></figure><p>We can start by taking a look at the first few rows of each dataset.</p><pre><code>First 5 lines of the math performance dataset:</code></pre><table><thead><tr><th></th><th>school</th><th>sex</th><th>age</th><th>address</th><th>famsize</th><th>Pstatus</th><th>Medu</th><th>Fedu</th><th>Mjob</th><th>Fjob</th><th>reason</th><th>guardian</th><th>traveltime</th><th>studytime</th><th>failures</th><th>schoolsup</th><th>famsup</th><th>paid</th><th>activities</th><th>nursery</th><th>higher</th><th>internet</th><th>romantic</th><th>famrel</th><th>freetime</th><th>goout</th><th>Dalc</th><th>Walc</th><th>health</th><th>absences</th><th>G1</th><th>G2</th><th>G3</th></tr></thead><tbody><tr><th>0</th><td>GP</td><td>F</td><td>18</td><td>U</td><td>GT3</td><td>A</td><td>4</td><td>4</td><td>at_home</td><td>teacher</td><td>course</td><td>mother</td><td>2</td><td>2</td><td>0</td><td>yes</td><td>no</td><td>no</td><td>no</td><td>yes</td><td>yes</td><td>no</td><td>no</td><td>4</td><td>3</td><td>4</td><td>1</td><td>1</td><td>3</td><td>6</td><td>5</td><td>6</td><td>6</td></tr><tr><th>1</th><td>GP</td><td>F</td><td>17</td><td>U</td><td>GT3</td><td>T</td><td>1</td><td>1</td><td>at_home</td><td>other</td><td>course</td><td>father</td><td>1</td><td>2</td><td>0</td><td>no</td><td>yes</td><td>no</td><td>no</td><td>no</td><td>yes</td><td>yes</td><td>no</td><td>5</td><td>3</td><td>3</td><td>1</td><td>1</td><td>3</td><td>4</td><td>5</td><td>5</td><td>6</td></tr><tr><th>2</th><td>GP</td><td>F</td><td>15</td><td>U</td><td>LE3</td><td>T</td><td>1</td><td>1</td><td>at_home</td><td>other</td><td>other</td><td>mother</td><td>1</td><td>2</td><td>3</td><td>yes</td><td>no</td><td>yes</td><td>no</td><td>yes</td><td>yes</td><td>yes</td><td>no</td><td>4</td><td>3</td><td>2</td><td>2</td><td>3</td><td>3</td><td>10</td><td>7</td><td>8</td><td>10</td></tr><tr><th>3</th><td>GP</td><td>F</td><td>15</td><td>U</td><td>GT3</td><td>T</td><td>4</td><td>2</td><td>health</td><td>services</td><td>home</td><td>mother</td><td>1</td><td>3</td><td>0</td><td>no</td><td>yes</td><td>yes</td><td>yes</td><td>yes</td><td>yes</td><td>yes</td><td>yes</td><td>3</td><td>2</td><td>2</td><td>1</td><td>1</td><td>5</td><td>2</td><td>15</td><td>14</td><td>15</td></tr><tr><th>4</th><td>GP</td><td>F</td><td>16</td><td>U</td><td>GT3</td><td>T</td><td>3</td><td>3</td><td>other</td><td>other</td><td>home</td><td>father</td><td>1</td><td>2</td><td>0</td><td>no</td><td>yes</td><td>yes</td><td>no</td><td>yes</td><td>yes</td><td>no</td><td>no</td><td>4</td><td>3</td><td>2</td><td>1</td><td>2</td><td>5</td><td>4</td><td>6</td><td>10</td><td>10</td></tr></tbody></table><pre><code>First 5 lines of the Portuguese performance dataset:</code></pre><table><thead><tr><th></th><th>school</th><th>sex</th><th>age</th><th>address</th><th>famsize</th><th>Pstatus</th><th>Medu</th><th>Fedu</th><th>Mjob</th><th>Fjob</th><th>reason</th><th>guardian</th><th>traveltime</th><th>studytime</th><th>failures</th><th>schoolsup</th><th>famsup</th><th>paid</th><th>activities</th><th>nursery</th><th>higher</th><th>internet</th><th>romantic</th><th>famrel</th><th>freetime</th><th>goout</th><th>Dalc</th><th>Walc</th><th>health</th><th>absences</th><th>G1</th><th>G2</th><th>G3</th></tr></thead><tbody><tr><th>0</th><td>GP</td><td>F</td><td>18</td><td>U</td><td>GT3</td><td>A</td><td>4</td><td>4</td><td>at_home</td><td>teacher</td><td>course</td><td>mother</td><td>2</td><td>2</td><td>0</td><td>yes</td><td>no</td><td>no</td><td>no</td><td>yes</td><td>yes</td><td>no</td><td>no</td><td>4</td><td>3</td><td>4</td><td>1</td><td>1</td><td>3</td><td>4</td><td>0</td><td>11</td><td>11</td></tr><tr><th>1</th><td>GP</td><td>F</td><td>17</td><td>U</td><td>GT3</td><td>T</td><td>1</td><td>1</td><td>at_home</td><td>other</td><td>course</td><td>father</td><td>1</td><td>2</td><td>0</td><td>no</td><td>yes</td><td>no</td><td>no</td><td>no</td><td>yes</td><td>yes</td><td>no</td><td>5</td><td>3</td><td>3</td><td>1</td><td>1</td><td>3</td><td>2</td><td>9</td><td>11</td><td>11</td></tr><tr><th>2</th><td>GP</td><td>F</td><td>15</td><td>U</td><td>LE3</td><td>T</td><td>1</td><td>1</td><td>at_home</td><td>other</td><td>other</td><td>mother</td><td>1</td><td>2</td><td>0</td><td>yes</td><td>no</td><td>no</td><td>no</td><td>yes</td><td>yes</td><td>yes</td><td>no</td><td>4</td><td>3</td><td>2</td><td>2</td><td>3</td><td>3</td><td>6</td><td>12</td><td>13</td><td>12</td></tr><tr><th>3</th><td>GP</td><td>F</td><td>15</td><td>U</td><td>GT3</td><td>T</td><td>4</td><td>2</td><td>health</td><td>services</td><td>home</td><td>mother</td><td>1</td><td>3</td><td>0</td><td>no</td><td>yes</td><td>no</td><td>yes</td><td>yes</td><td>yes</td><td>yes</td><td>yes</td><td>3</td><td>2</td><td>2</td><td>1</td><td>1</td><td>5</td><td>0</td><td>14</td><td>14</td><td>14</td></tr><tr><th>4</th><td>GP</td><td>F</td><td>16</td><td>U</td><td>GT3</td><td>T</td><td>3</td><td>3</td><td>other</td><td>other</td><td>home</td><td>father</td><td>1</td><td>2</td><td>0</td><td>no</td><td>yes</td><td>no</td><td>no</td><td>yes</td><td>yes</td><td>no</td><td>no</td><td>4</td><td>3</td><td>2</td><td>1</td><td>2</td><td>5</td><td>0</td><td>11</td><td>13</td><td>13</td></tr></tbody></table><p>An important detail to note is that there are 395 high school students in the math dataset and 649 in the Portuguese dataset. The grades of the student are from 0 to 20. Furthermore, there are 16 numerical variables out of 33; the rest of the variables will need to be one-hot encoded when we will analyze correlations and build the regression model.</p><p>Now let’s visualize the final grades distributions for both subjects.</p><p><img src="/" class="lazyload" data-src="/images/school-success/school-success_13_0.png"  alt="Distribution of student grades for math and Portuguese"></p><p>We can also calculate that the average final grades for math and Portuguese students are 10.42 and 11.91, respectively. This suggests that Portuguese students score higher on average than math students although this comparison could easily have been skewed by the large number of math students scoring zero.</p><h3 id="finding-and-visualizing-correlations-for-numerical-variables"><a class="markdownIt-Anchor" href="#finding-and-visualizing-correlations-for-numerical-variables"></a> Finding and Visualizing Correlations for Numerical Variables</h3><p>We are now going to automatically find the variables with the strongest correlation to the final grades for both datasets. Finding correlations between non-numeric features and the outcome can get a bit messy, so we will focus on testing only the existing numerical values of the datasets at first. To better visualize the insights, we will also use correlation bar plots and heat maps for both datasets.</p><p><img src="/" class="lazyload" data-src="/images/school-success/school-success_17_0.png"  alt="Correlations between numeric predictors and the response for math"></p><p>To interpret correlation bar plots and heat map:</p><ul><li>The darker the bar/square, the stronger the correlation is.</li><li>Brown represents negative correlations, whereas purple represents positive correlations.</li></ul><p><img src="/" class="lazyload" data-src="/images/school-success/school-success_19_0.png"  alt="Correlations between numeric predictors and the response for Portuguese"></p><p>Insights:</p><ul><li>For both datasets, the number of past classes <code>failures</code> has a strong negative correlation with <code>G3</code>.</li><li>Other common variables with a negative correlation are <code>age</code>, frequency of going out with friends (<code>goout</code>), <code>traveltime</code>, <code>freetime</code> and <code>health</code>.</li><li><code>G1</code> and <code>G2</code> have very strong positive correlation coefficients for both datasets because student performance usually remains constant throughout the year; we will therefore ignore them.</li><li>Other common variables with a positive correlation are: <code>studytime</code>, education of parents (<code>Fedu</code> and <code>Medu</code>) and family relationship (<code>famrel</code>).</li></ul><h3 id="one-hot-encoding"><a class="markdownIt-Anchor" href="#one-hot-encoding"></a> One-Hot Encoding</h3><p>In order to get more insight from these datasets, we need to be able to use the categorical variables as well. An example of categorical variable is the <code>school</code> variable (the student is either at Gabriel Pereira or Mousinho da Silveira) as there are multiple possible values with no intrinsic ordering. We will use a technique called one-hot encoding, which assigns binary value to each category level indicating whether or not that level was the value of the original predictor. Here is an example of how it would look like for the variable father’s job (<code>Fjob</code>).</p><table><thead><tr><th>Father’s Job</th><th style="text-align:center">Occupation_teacher</th><th style="text-align:center">Occupation_health</th><th style="text-align:center">Occupation_services</th><th style="text-align:center">Occupation_at_home</th><th style="text-align:center">Occupation_other</th></tr></thead><tbody><tr><td>teacher</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td>health</td><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td>services</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td>at_home</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">0</td></tr><tr><td>other</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">1</td></tr></tbody></table><div class="note warning"><p><strong>Technical Detail</strong><br>One-hot encoding is actually slightly more subtle than the description given above. The missing detail is that we often drop the first of the resulting binary columns. The reason we do this is that knowing the value of the other columns is enough to be certain of the value of the first. Indeed, if all of the other columns are zero, then the first column must be one, and vice-versa. Removing the first column is important as the algebra behind linear regression fails we duplicate predictor information.</p></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">one_hot_encode</span><span class="params">(df)</span>:</span></span><br><span class="line">    <span class="comment"># Select only categorical variables</span></span><br><span class="line">    cat_df = df.select_dtypes(include=[<span class="string">'object'</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># One-hot encode variables</span></span><br><span class="line">    dummy_df = pd.get_dummies(cat_df, drop_first=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Add the response back and return</span></span><br><span class="line">    dummy_df[<span class="string">'G3'</span>] = df[<span class="string">'G3'</span>]</span><br><span class="line">    <span class="keyword">return</span> dummy_df</span><br><span class="line"></span><br><span class="line"><span class="comment"># One-hot encode both datasets</span></span><br><span class="line">dummy_dfm = one_hot_encode(data_m)</span><br><span class="line">dummy_dfp = one_hot_encode(data_p)</span><br></pre></td></tr></table></figure><h3 id="finding-and-visualizing-correlations-for-encoded-categorical-variables"><a class="markdownIt-Anchor" href="#finding-and-visualizing-correlations-for-encoded-categorical-variables"></a> Finding and Visualizing Correlations for Encoded Categorical Variables</h3><p>We can now analyze the correlation coefficients for the final grades of all the variables for both datasets.</p><p><img src="/" class="lazyload" data-src="/images/school-success/school-success_28_0.png"  alt="Correlations between categorical predictors and the response for math"></p><p><img src="/" class="lazyload" data-src="/images/school-success/school-success_29_0.png"  alt="Correlations between categorical predictors and the response for Portuguese"></p><p>Insights:</p><ul><li>Variables that impact negatively final grades in both datasets: in a romantic relationship (<code>romantic_yes</code>), does not want to go to higher education (<code>higher_no</code>), lives in a rural area (<code>address_R</code>) and has no access to internet (<code>internet_no</code>).</li><li>Variables that impact positively final grades in both datasets: not in a romantic relationship (<code>romantic_no</code>), wants to go to higher education (<code>higher_yes</code>), lives in a urban area (<code>address_U</code>), has access to internet (<code>internet_no</code>).</li><li>In Portuguese performance dataset, the <code>school</code> variable has a very high impact on the final grade (negatively impacted if goes to MS and positively impacted if goes to GP).</li><li>Males seem to score higher in math whereas females score higher in portuguese.</li></ul><h2 id="visualizing-key-trends"><a class="markdownIt-Anchor" href="#visualizing-key-trends"></a> Visualizing Key Trends</h2><p>Some of the results are quite unexpected so let’s visualize them.</p><h3 id="effect-of-address-type-on-grades"><a class="markdownIt-Anchor" href="#effect-of-address-type-on-grades"></a> Effect of Address Type on Grades</h3><p><img src="/" class="lazyload" data-src="/images/school-success/school-success_34_0.png"  alt="Impact of address type on student performance"></p><p>Insights:</p><ul><li>For math performance, there is not too much difference between urban and rural students. However, urban students tend to score slightly more.</li><li>For portuguese performance, we can see that urban students score higher more often than rural students.</li></ul><h3 id="effect-of-relationship-status-on-grades"><a class="markdownIt-Anchor" href="#effect-of-relationship-status-on-grades"></a> Effect of Relationship Status on Grades</h3><p><img src="/" class="lazyload" data-src="/images/school-success/school-success_37_0.png"  alt="Impact of relationship status on student performance"></p><p>Note, that of the 395 math students, 132 (33.4%) were in a relationship. Likewise 239 (36.8%) or of the 649 Portuguese students were in a relationship</p><p>Insights:</p><ul><li>In both datasets, there are more single students than in a relationship (only 33% in math dataset and 36% in portuguese dataset). This might skew results as there is less data to analyze for students in a relationship. We can see that in the Portuguese dataset where there are more values to analyze, the scatter plot shapes tend to look more similar.</li><li>Not enough data to say if relationship has true impact on math performance.</li></ul><h3 id="effect-of-sex-on-grades"><a class="markdownIt-Anchor" href="#effect-of-sex-on-grades"></a> Effect of Sex on Grades</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">fig, axs = plt.subplots(<span class="number">2</span>, <span class="number">1</span>, figsize=(<span class="number">12</span>,<span class="number">10</span>))</span><br><span class="line">plt.subplots_adjust(hspace=<span class="number">.25</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sex_plot</span><span class="params">(data, ax, subject)</span>:</span></span><br><span class="line">    ax.set_xlim(<span class="number">0</span>, <span class="number">20</span>)</span><br><span class="line">    sns.kdeplot(data.loc[data[<span class="string">'sex'</span>] == <span class="string">'F'</span>, <span class="string">'G3'</span>], label=<span class="string">'Female'</span>, shade=<span class="literal">True</span>, ax=ax)</span><br><span class="line">    sns.kdeplot(data.loc[data[<span class="string">'sex'</span>] == <span class="string">'M'</span>, <span class="string">'G3'</span>], label=<span class="string">'Male'</span>, shade=<span class="literal">True</span>, ax=ax)</span><br><span class="line">    ax.set_title(<span class="string">f'Female vs Male Students <span class="subst">&#123;subject&#125;</span> Performance'</span>)</span><br><span class="line">    ax.set_xlabel(<span class="string">'Grade'</span>)</span><br><span class="line">    ax.set_ylabel(<span class="string">'Density'</span>)</span><br><span class="line"></span><br><span class="line">sex_plot(data_m, axs[<span class="number">0</span>], subject=<span class="string">'Math'</span>)</span><br><span class="line">sex_plot(data_p, axs[<span class="number">1</span>], subject=<span class="string">'Portuguese'</span>)</span><br></pre></td></tr></table></figure><p><img src="/" class="lazyload" data-src="/images/school-success/school-success_41_0.png"  alt="Impact of sex on student performance"></p><h3 id="effect-of-school-choice-on-grades"><a class="markdownIt-Anchor" href="#effect-of-school-choice-on-grades"></a> Effect of School Choice on Grades</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Analyzing impact of choice of school on Portuguese performance</span></span><br><span class="line">plt.subplots(figsize=(<span class="number">12</span>,<span class="number">8</span>))</span><br><span class="line">b = sns.swarmplot(x=<span class="string">'school'</span>, y=<span class="string">'G3'</span>, data=data_p)</span><br><span class="line">b.axes.set_title(<span class="string">'School Choice vs Final Grade Portuguese'</span>)</span><br><span class="line">b.set_xlabel(<span class="string">'School'</span>)</span><br><span class="line">b.set_ylabel(<span class="string">'Final Grade'</span>);</span><br></pre></td></tr></table></figure><p><img src="/" class="lazyload" data-src="/images/school-success/school-success_43_0.png"  alt="Impact of school choice on student performance"></p><p>Insights:</p><ul><li>From the available data, MS students (<code>school_MS</code>) tend to score less than GP students (<code>school_GP</code>) in Portuguese. Maybe GP is specialized in Portuguese and students have access to higher-quality resources.</li><li>However as for the relationship analysis, there are less students going to MS so it might affect results.</li></ul><h2 id="model-fitting"><a class="markdownIt-Anchor" href="#model-fitting"></a> Model-fitting</h2><p>We are now going to build a multi-linear regression model for both datasets. To avoid the impact of correlated variables, we only use the top twelve most influential predictors. We start with the math scores.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit_regression_model</span><span class="params">(df, dummy_df)</span>:</span></span><br><span class="line">    num_df = df.select_dtypes(exclude=[<span class="string">'object'</span>])</span><br><span class="line">    full_df = pd.concat([num_df, dummy_df.drop(<span class="string">'G3'</span>, axis=<span class="number">1</span>)], axis=<span class="number">1</span>)</span><br><span class="line">    full_df.drop([<span class="string">'G1'</span>, <span class="string">'G2'</span>], axis=<span class="number">1</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">    most_inf = np.abs(full_df.corr()[<span class="string">'G3'</span>]).sort_values()[<span class="number">-13</span>:].index</span><br><span class="line">    red_df = full_df.loc[:, most_inf]</span><br><span class="line"></span><br><span class="line">    X = np.array(red_df.drop(<span class="string">'G3'</span>, axis=<span class="number">1</span>))</span><br><span class="line">    y = np.array(red_df[<span class="string">'G3'</span>])</span><br><span class="line"></span><br><span class="line">    Z = sm.add_constant(X)</span><br><span class="line">    mod = sm.OLS(y, Z).fit()</span><br><span class="line">    </span><br><span class="line">    results_as_html = mod.summary().tables[<span class="number">1</span>].as_html()</span><br><span class="line">    coeffs = pd.read_html(results_as_html, header=<span class="number">0</span>, index_col=<span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line">    coeffs = coeffs.set_index(pd.Index([<span class="string">'intercept'</span>]).append(red_df.drop(<span class="string">'G3'</span>, axis=<span class="number">1</span>).columns))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> mod.rsquared, coeffs</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">r2_m, coeffs_m = fit_regression_model(data_m, dummy_dfm)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"Model R^2: <span class="subst">&#123;r2_m:<span class="number">.02</span>f&#125;</span>"</span>)</span><br><span class="line">display(coeffs_m)</span><br></pre></td></tr></table></figure><pre><code>Model R^2: 0.20</code></pre><table><thead><tr><th></th><th>coef</th><th>std err</th><th>t</th><th>P&gt;|t|</th><th>[0.025</th><th>0.975]</th></tr></thead><tbody><tr><th>intercept</th><td>10.2932</td><td>3.454</td><td>2.981</td><td>0.003</td><td>3.503</td><td>17.083</td></tr><tr><th>paid_yes</th><td>0.2347</td><td>0.439</td><td>0.534</td><td>0.593</td><td>-0.629</td><td>1.099</td></tr><tr><th>sex_M</th><td>1.1617</td><td>0.436</td><td>2.665</td><td>0.008</td><td>0.305</td><td>2.019</td></tr><tr><th>address_U</th><td>0.5889</td><td>0.543</td><td>1.084</td><td>0.279</td><td>-0.479</td><td>1.657</td></tr><tr><th>Mjob_health</th><td>1.1831</td><td>0.779</td><td>1.519</td><td>0.130</td><td>-0.349</td><td>2.715</td></tr><tr><th>traveltime</th><td>-0.2877</td><td>0.324</td><td>-0.887</td><td>0.376</td><td>-0.926</td><td>0.350</td></tr><tr><th>romantic_yes</th><td>-0.8371</td><td>0.458</td><td>-1.829</td><td>0.068</td><td>-1.737</td><td>0.063</td></tr><tr><th>goout</th><td>-0.4753</td><td>0.194</td><td>-2.450</td><td>0.015</td><td>-0.857</td><td>-0.094</td></tr><tr><th>Fedu</th><td>-0.1004</td><td>0.251</td><td>-0.400</td><td>0.689</td><td>-0.594</td><td>0.393</td></tr><tr><th>age</th><td>-0.0467</td><td>0.178</td><td>-0.263</td><td>0.793</td><td>-0.396</td><td>0.302</td></tr><tr><th>higher_yes</th><td>1.4442</td><td>1.044</td><td>1.383</td><td>0.167</td><td>-0.608</td><td>3.497</td></tr><tr><th>Medu</th><td>0.4811</td><td>0.259</td><td>1.861</td><td>0.064</td><td>-0.027</td><td>0.989</td></tr><tr><th>failures</th><td>-1.7399</td><td>0.314</td><td>-5.547</td><td>0.000</td><td>-2.357</td><td>-1.123</td></tr></tbody></table><p>Insights for math data set linear regression model:</p><ul><li>Our model explains explains 20% of the inputs into the final grade (<code>G3</code>), however it could still be improve if the goal of this article would be pure accuracy.</li><li>We can see that the willingness of the student to go into higher education (<code>higher_yes</code>) is a variable with one of the largest absolute coefficients. If the student is willing to go into higher education, their score will increase, on average, by 1.44 points.</li><li>There are other statistically significant coefficients such as <code>failures</code>, <code>sex_M</code>, and <code>goout</code>.</li><li>For example, <code>failures</code> plays a decisive role in student performance: for each class the student has failed in the past, they can roughly except a decrease of 1.74 in their final score.</li></ul><p>And now for the Portuguese scores.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">r2_p, coeffs_p = fit_regression_model(data_p, dummy_dfp)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"Model R^2: <span class="subst">&#123;r2_p:<span class="number">.03</span>f&#125;</span>"</span>)</span><br><span class="line">display(coeffs_p)</span><br></pre></td></tr></table></figure><pre><code>Model R^2: 0.305</code></pre><table><thead><tr><th></th><th>coef</th><th>std err</th><th>t</th><th>P&gt;|t|</th><th>[0.025</th><th>0.975]</th></tr></thead><tbody><tr><th>intercept</th><td>9.8593</td><td>0.612</td><td>16.114</td><td>0.000</td><td>8.658</td><td>11.061</td></tr><tr><th>Mjob_teacher</th><td>0.2976</td><td>0.384</td><td>0.776</td><td>0.438</td><td>-0.456</td><td>1.051</td></tr><tr><th>internet_yes</th><td>0.3248</td><td>0.269</td><td>1.207</td><td>0.228</td><td>-0.204</td><td>0.853</td></tr><tr><th>address_U</th><td>0.3521</td><td>0.252</td><td>1.397</td><td>0.163</td><td>-0.143</td><td>0.847</td></tr><tr><th>reason_reputation</th><td>0.4602</td><td>0.270</td><td>1.704</td><td>0.089</td><td>-0.070</td><td>0.990</td></tr><tr><th>Walc</th><td>-0.1570</td><td>0.108</td><td>-1.455</td><td>0.146</td><td>-0.369</td><td>0.055</td></tr><tr><th>Dalc</th><td>-0.3119</td><td>0.149</td><td>-2.098</td><td>0.036</td><td>-0.604</td><td>-0.020</td></tr><tr><th>Fedu</th><td>0.1503</td><td>0.129</td><td>1.169</td><td>0.243</td><td>-0.102</td><td>0.403</td></tr><tr><th>Medu</th><td>0.0980</td><td>0.137</td><td>0.718</td><td>0.473</td><td>-0.170</td><td>0.366</td></tr><tr><th>studytime</th><td>0.4366</td><td>0.137</td><td>3.192</td><td>0.001</td><td>0.168</td><td>0.705</td></tr><tr><th>school_MS</th><td>-1.0299</td><td>0.252</td><td>-4.087</td><td>0.000</td><td>-1.525</td><td>-0.535</td></tr><tr><th>higher_yes</th><td>1.6627</td><td>0.376</td><td>4.421</td><td>0.000</td><td>0.924</td><td>2.401</td></tr><tr><th>failures</th><td>-1.4374</td><td>0.193</td><td>-7.449</td><td>0.000</td><td>-1.816</td><td>-1.058</td></tr></tbody></table><p>Insights for the portuguese data set linear regression model:</p><ul><li>Our model explains explains 30.5% of the inputs into the final grade (<code>G3</code>), better than the math model but still leaving room for improvement.</li><li>We again see that the desire to go into higher education and the number of previous failures are highly influential when determining a student’s final grade.</li><li>There are other statistically significant coefficients such as <code>school_MS</code>, <code>failures</code> and <code>studytime</code>.</li><li>In fact, the influence of going to Mousinho da Silveira is strong, with an expected decrease in one mark in a student’s Portuguese grade</li></ul><h2 id="conclusion"><a class="markdownIt-Anchor" href="#conclusion"></a> Conclusion</h2><p>We have seen that many factors can influence your final grades, the strongest of which typically being socio-economic characteristics (address, parent’s education, family relationship, etc.) that cannot be changed. Those factors can also depend on the potential biases of the dataset. For example, maybe the mother’s unemployment status has a bigger cultural impact on Portuguese student than on UK students. However, some variables that are controllable by the student such as <code>studytime</code>, going out (<code>goout</code>), consumption of alcohol (<code>Dalc</code> and <code>Walc</code>) and potentially relationship status (<code>romantic</code>) have been proved to have an impact on the final grade (<code>G3</code>) of students in these datasets.</p><p>Although valuable insights have been gleaned from this dataset it is clear from our poorly fitting regression model that linear interactions alone are insufficient for capturing a system as complicated as a student’s school performance. If a purely performative model is what we desired, then moving towards a tree-based model or including carefully chosen interaction terms would be advised.</p>]]></content>
      
      
      <categories>
          
          <category> Social Sciences </category>
          
          <category> Education </category>
          
      </categories>
      
      
        <tags>
            
            <tag> visualization </tag>
            
            <tag> data-analysis </tag>
            
            <tag> regression </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Gibrat&#39;s Law: The Central Limit Theorem&#39;s Forgotten Twin</title>
      <link href="/gibrats-law/"/>
      <url>/gibrats-law/</url>
      
        <content type="html"><![CDATA[<div class="note info"><p><strong>Accessing Post Source</strong><br>We are still working on getting this site set up, so source code for this post is not yet available. Check back soon and you’ll be able to find it linked here.</p></div><p>Testing, testing. One, two, three. Can everyone hear me alright?</p><p>Wonderful. Hello and welcome to the Warwick Data Science Society Research Blog. It’s so nice to see you.</p><h2 id="introduction"><a class="markdownIt-Anchor" href="#introduction"></a> Introduction</h2><h3 id="testing-and-tediosity"><a class="markdownIt-Anchor" href="#testing-and-tediosity"></a> Testing and Tediosity</h3><p>Testing is boring. Everyone knows that they <em>should</em> test the solutions they produce, yet reality often fails to live up to this ideal. We check a few obvious things, run through the logic in our head, and convince ourselves that nothing could ever go wrong…and then it does just that.</p><p>Sometimes this is okay. For small, individual projects, it’s not the end of the world if your code base comes collapsing down on you; a few hours of bodging and everything should be stable again (all be it with code now so messy that you’d want to consider cryongenics). For larger projects, and especially those of collaborative nature, this just won’t do.</p><p>That is a long way to say, this post is a test, but hopefully not a boring one. To ensure that the testing of this site is performed thoroughly, I decided to write this post to ensure that everything is behaving as expected. I hope it can also act as a template for other keen researchers—whether they are currently a member of WDSS or merely interested in getting involved—to contribute their own work.</p><p>The philosophy of this post is to write about a topic that is actually (well, hopefully) of some interest to people, and do so in a natural way. None of this testing through enumerating corner-cases; let’s use the site for what it’s made for in practice and share some interesting insights along the way.</p><p>With that in mind, I wish to introduce to you: Gibrat’s law. Don’t be disheartened if you haven’t come across this term before,—most haven’t. This is likely because the rule is overshadowed by it’s far more infamous twin, the central limit theorem. If you’ve not come across this second term either, don’t fret. You may want to watch <a href="https://www.youtube.com/watch?v=JNm3M9cqWyc" target="_blank" rel="noopener">this short video</a> by <a href="https://www.khanacademy.org/" target="_blank" rel="noopener">Khan Academy</a> as a preliminary, but we’ll introduce both of these ideas either way.</p><div class="note info"><p>As a matter of fact, the <a href="https://en.wikipedia.org/wiki/Gibrat%27s_law" target="_blank" rel="noopener">Wikipedia article for Gibrat’s Law</a> is only made up of a few paragraphs with the link to the law’s creator, Robert Gibrat, directing you to a yet non-existant page. We seem to be diving head first into the rabbit hole today.</p></div><h2 id="background"><a class="markdownIt-Anchor" href="#background"></a> Background</h2><div class="note warning"><p>If you are already familiar with central limit theorem, you may wish to skip to the <a href="#from-sums-to-products">next section</a>.</p></div><h3 id="the-central-limit-theorem"><a class="markdownIt-Anchor" href="#the-central-limit-theorem"></a> The Central Limit Theorem</h3><p>We’ll start with the central limit theorem. There are many ways of explaining or defining this phenomenon, each striking a subtle balance between statistical rigor and clarity of explanation. We will favor the latter, ensuring to remember that we should keep it too ourselves if we wish to avoid the disapproving gaze of the pure probability theorists.</p><p>This simplified description goes as follows:</p><ul><li>Consider a sequence of independent random variables</li><li>Take the sum of these values</li><li>Regardless of whether the original variables were normally distributed, the distribution of this sum will tend towards a <a href="https://www.mathsisfun.com/data/standard-normal-distribution.html" target="_blank" rel="noopener">normal distribution</a> as the number of variables increases</li></ul><p>To clarify this notion, let’s look an example. One of the simplest would be an experiment involving tossing multiple fair coins. We consider each coin flip to be an independent random variable taking value zero if the result is tails, and likewise one for heads. We can then say that the total number of heads is the sum of these random variables. It would follow from the central limit theorem that the total number of heads should therefore tend to a normal distribution as the number of total coin tosses gets large. Let’s simulate this experiment for different numbers of coins and see this process in action.</p><p><img src="/" class="lazyload" data-src="/images/gibrats-law/gibrats-law_18_0.png"  alt=""></p><p>Just as we expected, the more coins we toss, the closer the resulting distribution of heads resembles the classic bell shape of the normal distribution. Indeed, once we use around one hundred or more tosses, we can almost model the number of heads as a continuous variable.</p><h3 id="the-importance-of-the-clt"><a class="markdownIt-Anchor" href="#the-importance-of-the-clt"></a> The Importance of the CLT</h3><p>It’s all well and good that the theory works, but why should we care?</p><p>The real power of the central limit theorem presents itself when we want to perform statistical analysis or tests on unfriendly distributions. The normal distribution has some incredibly delightful properties and has been studied in great extent, so working with it is often straightforward. On the other hand, there are many distributions which are not so cooperative, and that’s even assuming we know the distribution of whatever we are trying to model. Thankfully, the central limit theorem allows us to circumvent these issues by assuring us that as long as our process can be modeled as the sum of independent random variables (which is a surprisingly common property), we can approximate its distribution as normal and apply all of the standard techniques we know and love.</p><p>For example, the number of visitors to a website in a given time period is unlikely to have an underlying normal distribution governing the process. This could make it difficult to analyze data related to this, however the central limit theorem offers a workaround. The rough conceptual notion behind this approach is to divide our time period up into smaller and smaller intervals and consider how many people visit the website in each of those. We can assume that these are reasonably independent and so our total visitor count becomes the sum of many independent random variables and so the central limit theorem holds. A small caveat of this specific result is that we require the number of visitors to the site in the time period to be reasonably large (&gt;20 visitors is typically fine), but as long as this holds, we can perform analysis just as if our data was normally distributed.</p><div class="note success"><p>We can even go one step further. Since translating or scaling a normal distribution does not change its normality, it is possible to approximate many situations using the standard normal distrubtion (mean 0, variance 1) by applying appropriate transformations. This makes our lives even simpler and offers some elegance in the process.</p></div><h2 id="from-sums-to-products"><a class="markdownIt-Anchor" href="#from-sums-to-products"></a> From Sums to Products</h2><p>This leads us nicely onto Gibrat’s law. In our rough definition of the central limit theorem above, we described taking numerous independent random variables and computing their sum. Gibrat’s law begins in a similar vein but goes on to consider the product of these values instead. Because of this alteration, we can no longer expect the aggregation to approach a normal distribution as the number of variables grows large. Instead we tend towards a related distribution—the log-normal distribution.</p><p>As the name eludes to, a random variable following log-normal distribution can be defined by its logarithm being normally distributed. The converse of this framing is that whenever we take a normal random variable and take its exponential, the result will follow a log-normal distribution. Because of this, a log-normal random variable only takes strictly positive values with a density curve along the lines of the following.</p><p><img src="/" class="lazyload" data-src="/images/gibrats-law/gibrats-law_28_0.png"  alt=""></p><p>Just as with our coin-tossing experiment for the central limit theorem, we can validate our belief in Gibrat’s law using another simulation. We’ll introduce an explicit example of Gibrat’s law in the final section of the post and for now look an a more esoteric example. In particular, we will consider numerous independent random variables, uniformly-distributed on the interval <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">(</mo><mn>0.5</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(0.5, 1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">(</span><span class="mord">0</span><span class="mord">.</span><span class="mord">5</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span>. We then take the product of the first <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathdefault">n</span></span></span></span> these variables and look at the probability distribution of this value. Using Gibrat’s law we would expect this product to approach a log-normal as <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathdefault">n</span></span></span></span> becomes large, with a density tending in shape towards the curves shown above. A few simulations shows exactly that.</p><p><img src="/" class="lazyload" data-src="/images/gibrats-law/gibrats-law_30_0.png"  alt=""></p><div class="note success"><p>The example shown here is, admittedly, rather abstract. Despite this, it is still simple to frame it in terms of a real world problem (though perhaps not one of upmost importance). For example, we could imagine the product of independent uniform random variables modeling the process of repeatedly cutting a piece of string at a randomly chosen point along its length and retaining the longer part.</p></div><p>It is worth noting that just as there is no one normal distribution, but rather many of various shapes and sizes, parameterized by their mean (<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>μ</mi></mrow><annotation encoding="application/x-tex">\mu</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathdefault">μ</span></span></span></span>) and variance (<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>σ</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\sigma^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8141079999999999em;vertical-align:0"></span><span class="mord"><span class="mord mathdefault" style="margin-right:.03588em">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8141079999999999em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>), the same is true for the log-normal distribution. In fact, we parameterize a log-normal distribution not by its own mean and variance, but by the mean and variance of the resulting normal distribution we obtain when taking logarithms.</p><p>This relationship between the two distributions offers us much of the same power the central limit theorem displays for Gibrat’s law too. Yes, taking the product of the independent random variables in its definition doesn’t give us the normal distribution we so desire directly, but with a simple logarithmic transformation we can get there. Further, because taking the logarithm of a random variable is a simple transformation with some desirable properties, we are able to utilize many of the tools we have developed for manipulating and analyzing the normal distribution with log-normal distribution. This in turn makes Gibrat’s law an incredibly powerful tool for simplifying statistical analysis. One question remains though: when does it hold? What sort of scenarios can be modeled as a product of independent random variables? This brings us neatly to the final section of this post in which we will look into just that, and confirm Gibrat’s law in action.</p><h2 id="verifying-the-law"><a class="markdownIt-Anchor" href="#verifying-the-law"></a> Verifying the Law</h2><p>So far we’ve discussed some interesting ideas, but I think it’s time to pull things back from the land of statistical theory into the real world. Where would we expect to find Gibrat’s law in our lives?</p><p>The key insight needed to answer this question is to understand the underlying mechanism of Gibrat’s law; it’s all above the multiplication of random variables. Specifically, we take the previous product and multiply it by some new random amount. Another way of looking at this is that these random variables represent the growth rate of some quantity. Now, this is not a fixed growth rate as you would have say with interest rates or nuclear decay, but rather a stochastic growth rate.</p><p>To simplify this notion, all we are looking for is systems that evolve by a growth rate proportional to their current size with some added variation captured by the random nature of the variables. An example of such a system (and incidentally the original inspiration for the law) is that of modeling the growth of a firm. In a simple model, we can ignore the impacts of overarching market changes and catastrophic economic events and instead suggest that the growth of a company is roughly proportional to its current size with some added noise. Obviously, this model is primitive and its assumptions do not hold exactly, but it is close enough to believe that the distribution of the size of firms would indeed be log-normally distributed.</p><p>Another example could be the distribution of the population sizes for various countries or cities. There are clear troubles with modeling such a system using Gibrat’s law, but overall it is not unreasonable to suggest that the population growth of a city is largely dependent on its current size, with some added stochasticity. Don’t take my word for it though! Instead, let’s quickly gather some data to verify this result for ourselves.</p><h3 id="sourcing-data"><a class="markdownIt-Anchor" href="#sourcing-data"></a> Sourcing Data</h3><p>After some searching, I decided that the best data source to investigate the validity of Gibrat’s law for use with population data is <a href="https://en.wikipedia.org/wiki/List_of_cities_in_the_United_Kingdom" target="_blank" rel="noopener">this Wikipedia article</a> containing the populations of all sovereign states and dependencies. I then scraped relevant columns from the main table of this page, resulting in a dataset for which ten randomly chosen rows are shown below.</p><table><thead><tr><th></th><th>City</th><th>Population</th></tr></thead><tbody><tr><th>5</th><td>Manchester</td><td>503127</td></tr><tr><th>24</th><td>Southampton</td><td>236882</td></tr><tr><th>28</th><td>York</td><td>198051</td></tr><tr><th>31</th><td>Chelmsford</td><td>168310</td></tr><tr><th>32</th><td>Dundee</td><td>153990</td></tr><tr><th>51</th><td>Bath</td><td>88859</td></tr><tr><th>53</th><td>Hereford</td><td>58896</td></tr><tr><th>56</th><td>Stirling</td><td>34790</td></tr><tr><th>57</th><td>Lichfield</td><td>32219</td></tr><tr><th>66</th><td>London</td><td>7375</td></tr></tbody></table><h3 id="visualizing-the-results"><a class="markdownIt-Anchor" href="#visualizing-the-results"></a> Visualizing the Results</h3><p>We can now plot a histogram of the populations for these cities. On top of this, we overlay a log-normal distribution with parameters chosen to best fit (using maximum likelihood estimation) to see how well the densities match.</p><p><img src="/" class="lazyload" data-src="/images/gibrats-law/gibrats-law_44_0.png"  alt=""></p><p>Now, before you say anything, I know. It’s not perfect. But, I hope we can agree that there is definitely <em>something</em> there. It’s also quite easy to explain why the match isn’t exact. For a start, we only had data for 69 cities so it’s no surprise that the histogram is so jagged. On top of this, city populations is one of the more difficult applications of Gibrat’s law due to the many factors that influence their size that aim to violate the assumptions of the rule.</p><p>Despite these shortcomings, it is clear that Gibrat’s law has value to it either as a conceptual or statistical tool. It should certainly be the case that the validity of the law in any particular scenario should be tested thoroughly before resting too heavy on the results, but as a tool to guide you in the right direction, it is is invaluable.</p><p>As eluded to above, the example of population data is more difficult than most applications of Gibrat’s law due to the numerous and influential externalities. I didn’t want to shy away from this case, especially when it is an example that has clear real-world implications for modeling. It therefore follows that in many more simplistic and controlled cases, Gibrat’s law shines even brighter. For example, it has been of great benefit in my work at AstraZeneca in forming suitable priors for energy distributions in statistical models. Without knowledge of this law it may have taken me more time to discover that the log-normal distribution was a natural and accurate model for these quanta.</p><p>If anything, this post does not offer anything of immediate practical use. That is not to say however that there isn’t an important message. That is, remember Gibrat’s law—it’s there more than you think, and your awareness of it is vital for optimum efficiency in your statistical work. I hope you found this post of some insight, and I look forward to sharing more ideas and research as this blog developments.</p>]]></content>
      
      
      <categories>
          
          <category> Meta </category>
          
          <category> Mathematics </category>
          
          <category> Statistics </category>
          
      </categories>
      
      
        <tags>
            
            <tag> lesson </tag>
            
            <tag> normal-distribution </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
